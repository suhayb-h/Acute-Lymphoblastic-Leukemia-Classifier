{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1W6P_KyJ0zJbVm8CaafuRHxRiuCyhvMUM",
      "authorship_tag": "ABX9TyOYSZfqhtu6MZPw5VZEgojB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suhayb-h/Acute-Lymphoblastic-Leukemia-Classifier/blob/main/OmniGlot_CNMC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Download\n",
        "import os\n",
        "import urllib.request\n",
        "import numpy as np\n",
        "import zipfile\n",
        "from imageio import imread #changed from scipy.ndimage -> imageio\n",
        "\n",
        "#Batcher\n",
        "from numpy.random import choice\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "#from scipy.misc import imresize as resize\n",
        "from PIL import Image\n",
        "#from image_augmenter import ImageAugmenter\n",
        "\n",
        "#Model\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "#Train\n",
        "import argparse\n",
        "from datetime import datetime, timedelta\n",
        "#import batcher\n",
        "#from batcher import Batcher\n",
        "#import models\n",
        "#from models import ArcBinaryClassifier\n",
        "\n",
        "#vizualize\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "#from models import ArcBinaryClassifier\n",
        "#from batcher import Batcher\n"
      ],
      "metadata": {
        "id": "pziLE6sG5yxK"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "xfevZwlVDb5v",
        "outputId": "a5bd4967-7d0f-41b8-d318-b3df9ff5b671"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-cae5bc24ddba>\"\u001b[0;36m, line \u001b[0;32m45\u001b[0m\n\u001b[0;31m    ))\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#omniglot_url = 'http://github.com/brendenlake/omniglot/archive/master.zip'\n",
        "data_dir = os.path.join(\"/content/drive/MyDrive/Omniglot_ARC\")\n",
        "zip_location = os.path.join(data_dir, \"omniglot.zip\")\n",
        "unzip_location = os.path.join(data_dir, \"extracted\")\n",
        "zipped_images_location = os.path.join(unzip_location, \"omniglot-master\", \"python\")\n",
        "extracted_images_location = os.path.join(data_dir, \"images\")\n",
        "\n",
        "def download() -> None:\n",
        "    if os.path.exists(zip_location) and os.path.isfile(zip_location):\n",
        "        return\n",
        "        omniglot_url, zip_location\n",
        "    ))\n",
        "    urllib.request.urlretrieve(omniglot_url, zip_location)\n",
        "\n",
        "def extract() -> None:\n",
        "    zip_ref = zipfile.ZipFile(zip_location, 'r')\n",
        "    zip_ref.extractall(unzip_location)\n",
        "    zip_ref.close()\n",
        "\n",
        "def extract_images() -> None:\n",
        "    image_sets = [\"images_background.zip\", \"images_evaluation.zip\"]\n",
        "    image_sets = [os.path.join(zipped_images_location, image_set) for image_set in image_sets]\n",
        "    for image_set in image_sets:\n",
        "        zip_ref = zipfile.ZipFile(image_set, 'r')\n",
        "        zip_ref.extractall(extracted_images_location)\n",
        "        zip_ref.close()\n",
        "\n",
        "def omniglot_folder_to_NDarray(path_im):\n",
        "    alphbts = os.listdir(path_im)\n",
        "    ALL_IMGS = []\n",
        "    for alphbt in alphbts:\n",
        "        chars = os.listdir(os.path.join(path_im, alphbt))\n",
        "        for char in chars:\n",
        "            img_filenames = os.listdir(os.path.join(path_im, alphbt, char))\n",
        "            char_imgs = []\n",
        "            for img_fn in img_filenames:\n",
        "                fn = os.path.join(path_im, alphbt, char, img_fn)\n",
        "                I = imread(fn)\n",
        "                I = np.invert(I)\n",
        "                char_imgs.append(I)\n",
        "            ALL_IMGS.append(char_imgs)\n",
        "    return np.array(ALL_IMGS)\n",
        "\n",
        "def save_to_numpy() -> None:\n",
        "    image_folders = [\"images_background\", \"images_evaluation\"]\n",
        "    all_np_array = []\n",
        "    for image_folder in image_folders:\n",
        "        np_array_loc = os.path.join(data_dir, image_folder + \".npy\")\n",
        "        np_array = omniglot_folder_to_NDarray(os.path.join(extracted_images_location, image_folder))\n",
        "        np.save(np_array_loc, np_array)\n",
        "        all_np_array.append(np_array)\n",
        "\n",
        "    all_np_array = np.concatenate(all_np_array, axis=0)\n",
        "    np.save(os.path.join(\"/content/drive/MyDrive/Omniglot_ARC\", \"omniglot.npy\"), all_np_array)\n",
        "\n",
        "def main():\n",
        "    download()\n",
        "    extract()\n",
        "    extract_images()\n",
        "    save_to_numpy()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Batcher: Original Source -> https://github.com/pranv/ARC\n",
        "use_cuda = False\n",
        "\n",
        "class Omniglot(object):\n",
        "    def __init__(self, path=os.path.join('/content/drive/MyDrive/Omniglot_ARC', 'omniglot.npy'), batch_size=128, image_size=32):\n",
        "        \"\"\"\n",
        "        batch_size: the output is (2 * batch size, 1, image_size, image_size)\n",
        "                    X[i] & X[i + batch_size] are the pair\n",
        "        image_size: size of the image\n",
        "        data_split: in number of alphabets, e.g. [30, 10] means out of 50 Omniglot characters,\n",
        "                    30 is for training, 10 for validation and the remaining(10) for testing\n",
        "        within_alphabet: for verfication task, when 2 characters are sampled to form a pair,\n",
        "                        this flag specifies if should they be from the same alphabet/language\n",
        "        ---------------------\n",
        "        Data Augmentation Parameters:\n",
        "            flip: here flipping both the images in a pair\n",
        "            scale: x would scale image by + or - x%\n",
        "            rotation_deg\n",
        "            shear_deg\n",
        "            translation_px: in both x and y directions\n",
        "        \"\"\"\n",
        "        chars = np.load(path)\n",
        "\n",
        "        # resize the images\n",
        "        resized_chars = np.zeros((1623, 20, image_size, image_size), dtype='uint8')\n",
        "        for i in range(1623):\n",
        "            for j in range(20):\n",
        "                resized_chars[i, j] = np.resize(chars[i, j], (image_size, image_size))\n",
        "        chars = resized_chars\n",
        "\n",
        "        self.mean_pixel = chars.mean() / 255.0  # used later for mean subtraction\n",
        "\n",
        "        # starting index of each alphabet in a list of chars\n",
        "        a_start = [0, 20, 49, 75, 116, 156, 180, 226, 240, 266, 300, 333, 355, 381,\n",
        "                   424, 448, 496, 518, 534, 586, 633, 673, 699, 739, 780, 813,\n",
        "                   827, 869, 892, 909, 964, 984, 1010, 1036, 1062, 1088, 1114,\n",
        "                   1159, 1204, 1245, 1271, 1318, 1358, 1388, 1433, 1479, 1507,\n",
        "                   1530, 1555, 1597]\n",
        "\n",
        "        # size of each alphabet (num of chars)\n",
        "        a_size = [20, 29, 26, 41, 40, 24, 46, 14, 26, 34, 33, 22, 26, 43, 24, 48, 22,\n",
        "                  16, 52, 47, 40, 26, 40, 41, 33, 14, 42, 23, 17, 55, 20, 26, 26, 26,\n",
        "                  26, 26, 45, 45, 41, 26, 47, 40, 30, 45, 46, 28, 23, 25, 42, 26]\n",
        "\n",
        "        # each alphabet/language has different number of characters.\n",
        "        # in order to uniformly sample all characters, we need weigh the probability\n",
        "        # of sampling a alphabet by its size. p is that probability\n",
        "        def size2p(size):\n",
        "            s = np.array(size).astype('float64')\n",
        "            return s / s.sum()\n",
        "\n",
        "        self.size2p = size2p\n",
        "        self.data = chars\n",
        "        self.a_start = a_start\n",
        "        self.a_size = a_size\n",
        "        self.image_size = image_size\n",
        "        self.batch_size = batch_size\n",
        "        flip = True\n",
        "        scale = 0.2\n",
        "        rotation_deg = 20\n",
        "        shear_deg = 10\n",
        "        translation_px = 5\n",
        "        #self.augmentor = ImageAugmenter(image_size, image_size,\n",
        "        #                                hflip=flip, vflip=flip,\n",
        "        #                                scale_to_percent=1.0 + scale, rotation_deg=rotation_deg, shear_deg=shear_deg,\n",
        "        #                                translation_x_px=translation_px, translation_y_px=translation_px)\n",
        "\n",
        "    def fetch_batch(self, part):\n",
        "        \"\"\"\n",
        "            This outputs batch_size number of pairs\n",
        "            Thus the actual number of images outputted is 2 * batch_size\n",
        "            Say A & B form the half of a pair\n",
        "            The Batch is divided into 4 parts:\n",
        "                Dissimilar A \t\tDissimilar B\n",
        "                Similar A \t\t\tSimilar B\n",
        "\n",
        "            Corresponding images in Similar A and Similar B form the similar pair\n",
        "            similarly, Dissimilar A and Dissimilar B form the dissimilar pair\n",
        "\n",
        "            When flattened, the batch has 4 parts with indices:\n",
        "                Dissimilar A \t\t0 - batch_size / 2\n",
        "                Similar A    \t\tbatch_size / 2  - batch_size\n",
        "                Dissimilar B \t\tbatch_size  - 3 * batch_size / 2\n",
        "                Similar B \t\t\t3 * batch_size / 2 - batch_size\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "class Batcher(Omniglot):\n",
        "    def __init__(self, path=os.path.join('/content/drive/MyDrive/Omniglot_ARC', 'omniglot.npy'), batch_size=128, image_size=32):\n",
        "        Omniglot.__init__(self, path, batch_size, image_size)\n",
        "\n",
        "        a_start = self.a_start\n",
        "        a_size = self.a_size\n",
        "\n",
        "        # slicing indices for splitting a_start & a_size\n",
        "        i = 20\n",
        "        j = 30\n",
        "        starts = {}\n",
        "        starts['train'], starts['val'], starts['test'] = a_start[:i], a_start[i:j], a_start[j:]\n",
        "        sizes = {}\n",
        "        sizes['train'], sizes['val'], sizes['test'] = a_size[:i], a_size[i:j], a_size[j:]\n",
        "        size2p = self.size2p\n",
        "        p = {}\n",
        "        p['train'], p['val'], p['test'] = size2p(sizes['train']), size2p(sizes['val']), size2p(sizes['test'])\n",
        "        self.starts = starts\n",
        "        self.sizes = sizes\n",
        "        self.p = p\n",
        "\n",
        "    def fetch_batch(self, part, batch_size: int = None):\n",
        "\n",
        "        if batch_size is None:\n",
        "            batch_size = self.batch_size\n",
        "\n",
        "        X, Y = self._fetch_batch(part, batch_size)\n",
        "        X = Variable(torch.from_numpy(X)).view(2*batch_size, self.image_size, self.image_size)\n",
        "        X1 = X[:batch_size]  # (B, h, w)\n",
        "        X2 = X[batch_size:]  # (B, h, w)\n",
        "        X = torch.stack([X1, X2], dim=1)  # (B, 2, h, w)\n",
        "        Y = Variable(torch.from_numpy(Y))\n",
        "\n",
        "        if use_cuda:\n",
        "            X, Y = X.cuda(), Y.cuda()\n",
        "\n",
        "        return X, Y\n",
        "\n",
        "    def _fetch_batch(self, part, batch_size: int = None):\n",
        "        if batch_size is None:\n",
        "            batch_size = self.batch_size\n",
        "\n",
        "        data = self.data\n",
        "        starts = self.starts[part]\n",
        "        sizes = self.sizes[part]\n",
        "        p = self.p[part]\n",
        "        image_size = self.image_size\n",
        "        num_alphbts = len(starts)\n",
        "        X = np.zeros((2 * batch_size, image_size, image_size), dtype='uint8')\n",
        "        for i in range(batch_size // 2):\n",
        "            # choose similar chars\n",
        "            same_idx = choice(range(starts[0], starts[-1] + sizes[-1]))\n",
        "\n",
        "            # choose dissimilar chars within alphabet\n",
        "            alphbt_idx = choice(num_alphbts, p=p)\n",
        "            char_offset = choice(sizes[alphbt_idx], 2, replace=False)\n",
        "            diff_idx = starts[alphbt_idx] + char_offset\n",
        "            X[i], X[i + batch_size] = data[diff_idx, choice(20, 2)]\n",
        "            X[i + batch_size // 2], X[i + 3 * batch_size // 2] = data[same_idx, choice(20, 2, replace=False)]\n",
        "\n",
        "        y = np.zeros((batch_size, 1), dtype='int32')\n",
        "        y[:batch_size // 2] = 0\n",
        "        y[batch_size // 2:] = 1\n",
        "\n",
        "        if part == 'train':\n",
        "            #X = self.augmentor.augment_batch(X)\n",
        "        #else:\n",
        "            X = X / 255.0\n",
        "\n",
        "        X = X - self.mean_pixel\n",
        "        X = X[:, np.newaxis]\n",
        "        X = X.astype(\"float32\")\n",
        "\n",
        "        return X, y\n"
      ],
      "metadata": {
        "id": "y4Rtd_2wEMzV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model\n",
        "use_cuda = False\n",
        "\n",
        "class GlimpseWindow:\n",
        "    \"\"\"\n",
        "    Generates glimpses from images using Cauchy kernels.\n",
        "    Args:\n",
        "        glimpse_h (int): The height of the glimpses to be generated.\n",
        "        glimpse_w (int): The width of the glimpses to be generated.\n",
        "    \"\"\"\n",
        "    def __init__(self, glimpse_h: int, glimpse_w: int):\n",
        "        self.glimpse_h = glimpse_h\n",
        "        self.glimpse_w = glimpse_w\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_filterbanks(delta_caps: Variable, center_caps: Variable, image_size: int, glimpse_size: int) -> Variable:\n",
        "        \"\"\"\n",
        "        Generates Cauchy Filter Banks along a dimension.\n",
        "        Args:\n",
        "            delta_caps (B,):  A batch of deltas [-1, 1]\n",
        "            center_caps (B,): A batch of [-1, 1] reals that dictate the location of center of cauchy kernel glimpse.\n",
        "            image_size (int): size of images along that dimension\n",
        "            glimpse_size (int): size of glimpses to be generated along that dimension\n",
        "        Returns:\n",
        "            (B, image_size, glimpse_size): A batch of filter banks\n",
        "        \"\"\"\n",
        "        # convert dimension sizes to float. lots of math ahead.\n",
        "        image_size = float(image_size)\n",
        "        glimpse_size = float(glimpse_size)\n",
        "\n",
        "        # scale the centers and the deltas to map to the actual size of given image.\n",
        "        centers = (image_size - 1) * (center_caps + 1) / 2.0  # (B)\n",
        "        deltas = (float(image_size) / glimpse_size) * (1.0 - torch.abs(delta_caps))\n",
        "\n",
        "        # calculate gamma for cauchy kernel\n",
        "        gammas = torch.exp(1.0 - 2 * torch.abs(delta_caps))  # (B)\n",
        "\n",
        "        # coordinate of pixels on the glimpse\n",
        "        glimpse_pixels = Variable(torch.arange(0, glimpse_size) - (glimpse_size - 1.0) / 2.0)  # (glimpse_size)\n",
        "        if use_cuda:\n",
        "            glimpse_pixels = glimpse_pixels.cuda()\n",
        "\n",
        "        # space out with delta\n",
        "        glimpse_pixels = deltas[:, None] * glimpse_pixels[None, :]  # (B, glimpse_size)\n",
        "        # center around the centers\n",
        "        glimpse_pixels = centers[:, None] + glimpse_pixels  # (B, glimpse_size)\n",
        "\n",
        "        # coordinates of pixels on the image\n",
        "        image_pixels = Variable(torch.arange(0, image_size))  # (image_size)\n",
        "        if use_cuda:\n",
        "            image_pixels = image_pixels.cuda()\n",
        "\n",
        "        fx = image_pixels - glimpse_pixels[:, :, None]  # (B, glimpse_size, image_size)\n",
        "        fx = fx / gammas[:, None, None]\n",
        "        fx = fx ** 2.0\n",
        "        fx = 1.0 + fx\n",
        "        fx = math.pi * gammas[:, None, None] * fx\n",
        "        fx = 1.0 / fx\n",
        "        fx = fx / (torch.sum(fx, dim=2) + 1e-4)[:, :, None]  # we add a small constant in the denominator division by 0.\n",
        "\n",
        "        return fx.transpose(1, 2)\n",
        "\n",
        "    def get_attention_mask(self, glimpse_params: Variable, mask_h: int, mask_w: int) -> Variable:\n",
        "        \"\"\"\n",
        "        For visualization, generate a heat map (or mask) of which pixels got the most \"attention\".\n",
        "        Args:\n",
        "            glimpse_params (B, hx):  A batch of glimpse parameters.\n",
        "            mask_h (int): The height of the image for which the mask is being generated.\n",
        "            mask_w (int): The width of the image for which the mask is being generated.\n",
        "        Returns:\n",
        "            (B, mask_h, mask_w): A batch of masks with attended pixels weighted more.\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size, _ = glimpse_params.size()\n",
        "\n",
        "        # (B, image_h, glimpse_h)\n",
        "        F_h = self._get_filterbanks(delta_caps=glimpse_params[:, 2], center_caps=glimpse_params[:, 0],\n",
        "                                    image_size=mask_h, glimpse_size=self.glimpse_h)\n",
        "\n",
        "        # (B, image_w, glimpse_w)\n",
        "        F_w = self._get_filterbanks(delta_caps=glimpse_params[:, 2], center_caps=glimpse_params[:, 1],\n",
        "                                    image_size=mask_w, glimpse_size=self.glimpse_w)\n",
        "\n",
        "        # (B, glimpse_h, glimpse_w)\n",
        "        glimpse_proxy = Variable(torch.ones(batch_size, self.glimpse_h, self.glimpse_w))\n",
        "\n",
        "        # find the attention mask that lead to the glimpse.\n",
        "        mask = glimpse_proxy\n",
        "        mask = torch.bmm(F_h, mask)\n",
        "        mask = torch.bmm(mask, F_w.transpose(1, 2))\n",
        "\n",
        "        # scale to between 0 and 1.0\n",
        "        mask = mask - mask.min()\n",
        "        mask = mask / mask.max()\n",
        "        mask = mask.float()\n",
        "\n",
        "        return mask\n",
        "\n",
        "    def get_glimpse(self, images: Variable, glimpse_params: Variable) -> Variable:\n",
        "        \"\"\"\n",
        "        Generate glimpses given images and glimpse parameters. This is the main method of this class.\n",
        "        The glimpse parameters are (h_center, w_center, delta). (h_center, w_center)\n",
        "        represents the relative position of the center of the glimpse on the image. delta determines\n",
        "        the zoom factor of the glimpse.\n",
        "        Args:\n",
        "            images (B, h, w):  A batch of images\n",
        "            glimpse_params (B, 3):  A batch of glimpse parameters (h_center, w_center, delta)\n",
        "        Returns:\n",
        "            (B, glimpse_h, glimpse_w): A batch of glimpses.\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size, image_h, image_w = images.size()\n",
        "\n",
        "        # (B, image_h, glimpse_h)\n",
        "        F_h = self._get_filterbanks(delta_caps=glimpse_params[:, 2], center_caps=glimpse_params[:, 0],\n",
        "                                    image_size=image_h, glimpse_size=self.glimpse_h)\n",
        "\n",
        "        # (B, image_w, glimpse_w)\n",
        "        F_w = self._get_filterbanks(delta_caps=glimpse_params[:, 2], center_caps=glimpse_params[:, 1],\n",
        "                                    image_size=image_w, glimpse_size=self.glimpse_w)\n",
        "\n",
        "        # F_h.T * images * F_w\n",
        "        glimpses = images\n",
        "        glimpses = torch.bmm(F_h.transpose(1, 2), glimpses)\n",
        "        glimpses = torch.bmm(glimpses, F_w)\n",
        "\n",
        "        return glimpses  # (B, glimpse_h, glimpse_w)\n",
        "\n",
        "class ARC(nn.Module):\n",
        "    \"\"\"\n",
        "    This class implements the Attentive Recurrent Comparators. This module has two main parts.\n",
        "    1.) controller: The RNN module that takes as input glimpses from a pair of images and emits a hidden state.\n",
        "    2.) glimpser: A Linear layer that takes the hidden state emitted by the controller and generates the glimpse\n",
        "                    parameters. These glimpse parameters are (h_center, w_center, delta). (h_center, w_center)\n",
        "                    represents the relative position of the center of the glimpse on the image. delta determines\n",
        "                    the zoom factor of the glimpse.\n",
        "    Args:\n",
        "        num_glimpses (int): How many glimpses must the ARC \"see\" before emitting the final hidden state.\n",
        "        glimpse_h (int): The height of the glimpse in pixels.\n",
        "        glimpse_w (int): The width of the glimpse in pixels.\n",
        "        controller_out (int): The size of the hidden state emitted by the controller.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_glimpses: int=8, glimpse_h: int=8, glimpse_w: int=8, controller_out: int=128) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_glimpses = num_glimpses\n",
        "        self.glimpse_h = glimpse_h\n",
        "        self.glimpse_w = glimpse_w\n",
        "        self.controller_out = controller_out\n",
        "\n",
        "        # main modules of ARC\n",
        "\n",
        "        self.controller = nn.LSTMCell(input_size=(glimpse_h * glimpse_w), hidden_size=self.controller_out)\n",
        "        self.glimpser = nn.Linear(in_features=self.controller_out, out_features=3)\n",
        "\n",
        "        # this will actually generate glimpses from images using the glimpse parameters.\n",
        "        self.glimpse_window = GlimpseWindow(glimpse_h=self.glimpse_h, glimpse_w=self.glimpse_w)\n",
        "\n",
        "    def forward(self, image_pairs: Variable) -> Variable:\n",
        "        \"\"\"\n",
        "        The method calls the internal _forward() method which returns hidden states for all time steps. This i\n",
        "        Args:\n",
        "            image_pairs (B, 2, h, w):  A batch of pairs of images\n",
        "        Returns:\n",
        "            (B, controller_out): A batch of final hidden states after each pair of image has been shown for num_glimpses\n",
        "            glimpses.\n",
        "        \"\"\"\n",
        "        # return only the last hidden state\n",
        "        all_hidden = self._forward(image_pairs)  # (2*num_glimpses, B, controller_out)\n",
        "        last_hidden = all_hidden[-1, :, :]  # (B, controller_out)\n",
        "\n",
        "        return last_hidden\n",
        "\n",
        "    def _forward(self, image_pairs: Variable) -> Variable:\n",
        "        \"\"\"\n",
        "        The main forward method of ARC. But it returns hidden state from all time steps (all glimpses) as opposed to\n",
        "        just the last one. See the exposed forward() method.\n",
        "        Args:\n",
        "            image_pairs: (B, 2, h, w) A batch of pairs of images\n",
        "        Returns:\n",
        "            (2*num_glimpses, B, controller_out) Hidden states from ALL time steps.\n",
        "        \"\"\"\n",
        "        # convert to images to float.\n",
        "        image_pairs = image_pairs.float()\n",
        "\n",
        "        # calculate the batch size\n",
        "        batch_size = image_pairs.size()[0]\n",
        "\n",
        "        # an array for collecting hidden states from each time step.\n",
        "        all_hidden = []\n",
        "\n",
        "        # initial hidden state of the LSTM.\n",
        "        Hx = Variable(torch.zeros(batch_size, self.controller_out))  # (B, controller_out)\n",
        "        Cx = Variable(torch.zeros(batch_size, self.controller_out))  # (B, controller_out)\n",
        "\n",
        "        if use_cuda:\n",
        "            Hx, Cx = Hx.cuda(), Cx.cuda()\n",
        "\n",
        "        # take `num_glimpses` glimpses for both images, alternatingly.\n",
        "        for turn in range(2*self.num_glimpses):\n",
        "            # select image to show, alternate between the first and second image in the pair\n",
        "            images_to_observe = image_pairs[:,  turn % 2]  # (B, h, w)\n",
        "\n",
        "            # choose a portion from image to glimpse using attention\n",
        "            glimpse_params = torch.tanh(self.glimpser(Hx))  # (B, 3)  a batch of glimpse params (x, y, delta)\n",
        "            glimpses = self.glimpse_window.get_glimpse(images_to_observe, glimpse_params)  # (B, glimpse_h, glimpse_w)\n",
        "            flattened_glimpses = glimpses.view(batch_size, -1)  # (B, glimpse_h * glimpse_w), one time-step\n",
        "\n",
        "            # feed the glimpses and the previous hidden state to the LSTM.\n",
        "            Hx, Cx = self.controller(flattened_glimpses, (Hx, Cx))  # (B, controller_out), (B, controller_out)\n",
        "\n",
        "            # append this hidden state to all states\n",
        "            all_hidden.append(Hx)\n",
        "\n",
        "        all_hidden = torch.stack(all_hidden)  # (2*num_glimpses, B, controller_out)\n",
        "\n",
        "        # return a batch of all hidden states.\n",
        "        return all_hidden\n",
        "\n",
        "\n",
        "class ArcBinaryClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    A binary classifier that uses ARC.\n",
        "    Given a pair of images, feeds them the ARC and uses the final hidden state of ARC to\n",
        "    classify the images as belonging to the same class or not.\n",
        "    Args:\n",
        "        num_glimpses (int): How many glimpses must the ARC \"see\" before emitting the final hidden state.\n",
        "        glimpse_h (int): The height of the glimpse in pixels.\n",
        "        glimpse_w (int): The width of the glimpse in pixels.\n",
        "        controller_out (int): The size of the hidden state emitted by the controller.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_glimpses: int=8, glimpse_h: int=8, glimpse_w: int=8, controller_out: int = 128):\n",
        "        super().__init__()\n",
        "        self.arc = ARC(\n",
        "            num_glimpses=num_glimpses,\n",
        "            glimpse_h=glimpse_h,\n",
        "            glimpse_w=glimpse_w,\n",
        "            controller_out=controller_out)\n",
        "\n",
        "        # two dense layers, which take the hidden state from the controller of ARC and\n",
        "        # classify the images as belonging to the same class or not.\n",
        "        self.dense1 = nn.Linear(controller_out, 64)\n",
        "        self.dense2 = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, image_pairs: Variable) -> Variable:\n",
        "        arc_out = self.arc(image_pairs)\n",
        "\n",
        "        d1 = F.elu(self.dense1(arc_out))\n",
        "        decision = torch.sigmoid(self.dense2(d1))\n",
        "\n",
        "        return decision\n",
        "\n",
        "    def save_to_file(self, file_path: str) -> None:\n",
        "        torch.save(self.state_dict(), file_path)"
      ],
      "metadata": {
        "id": "ESAiij2DD-ra"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('-f')\n",
        "parser.add_argument('--batchSize', type=int, default=128, help='input batch size')\n",
        "parser.add_argument('--imageSize', type=int, default=32, help='the height / width of the input image to ARC')\n",
        "parser.add_argument('--glimpseSize', type=int, default=8, help='the height / width of glimpse seen by ARC')\n",
        "parser.add_argument('--numStates', type=int, default=128, help='number of hidden states in ARC controller')\n",
        "parser.add_argument('--numGlimpses', type=int, default=6, help='the number glimpses of each image in pair seen by ARC')\n",
        "parser.add_argument('--lr', type=float, default=0.0002, help='learning rate, default=0.0002')\n",
        "parser.add_argument('--cuda', action='store_true', help='enables cuda')\n",
        "parser.add_argument('--name', default=None, help='Custom name for this configuration. Needed for saving'\n",
        "                                                 ' model checkpoints in a separate folder.')\n",
        "parser.add_argument('--load', default=None, help='the model to load from. Start fresh if not specified.')\n",
        "\n",
        "def get_pct_accuracy(pred: Variable, target) -> int:\n",
        "    hard_pred = (pred > 0.5).int()\n",
        "    correct = (hard_pred == target).sum().data#[0]\n",
        "    accuracy = float(correct) / target.size()[0]\n",
        "    accuracy = int(accuracy * 100)\n",
        "    return accuracy\n",
        "\n",
        "def train():\n",
        "    opt = parser.parse_args()\n",
        "\n",
        "    if opt.cuda:\n",
        "        batcher.use_cuda = True\n",
        "        models.use_cuda = True\n",
        "\n",
        "    if opt.name is None:\n",
        "        # if no name is given, we generate a name from the parameters.\n",
        "        # only those parameters are taken, which if changed break torch.load compatibility.\n",
        "        opt.name = \"{}_{}_{}_{}\".format(opt.numGlimpses, opt.glimpseSize, opt.numStates,\n",
        "                                        \"cuda\" if opt.cuda else \"cpu\")\n",
        "        \n",
        "    # make directory for storing models.\n",
        "    models_path = os.path.join(\"saved_models\", opt.name)\n",
        "    os.makedirs(models_path, exist_ok=True)\n",
        "\n",
        "    # initialise the model\n",
        "    discriminator = ArcBinaryClassifier(num_glimpses=opt.numGlimpses,\n",
        "                                        glimpse_h=opt.glimpseSize,\n",
        "                                        glimpse_w=opt.glimpseSize,\n",
        "                                        controller_out=opt.numStates)\n",
        "\n",
        "    if opt.cuda:\n",
        "        discriminator.cuda()\n",
        "\n",
        "    # load from a previous checkpoint, if specified.\n",
        "    if opt.load is not None:\n",
        "        discriminator.load_state_dict(torch.load(os.path.join(models_path, opt.load)))\n",
        "\n",
        "    # set up the optimizer.\n",
        "    bce = torch.nn.BCELoss()\n",
        "    if opt.cuda:\n",
        "        bce = bce.cuda()\n",
        "\n",
        "    optimizer = torch.optim.Adam(params=discriminator.parameters(), lr=opt.lr)\n",
        "\n",
        "    # load the dataset in memory.\n",
        "    loader = Batcher(batch_size=opt.batchSize, image_size=opt.imageSize)\n",
        "\n",
        "    # ready to train ...\n",
        "    best_validation_loss = None\n",
        "    saving_threshold = 1.02\n",
        "    last_saved = datetime.utcnow()\n",
        "    save_every = timedelta(minutes=10)\n",
        "\n",
        "    i = -1\n",
        "    while True:\n",
        "        i += 1\n",
        "\n",
        "        X, Y = loader.fetch_batch(\"train\")\n",
        "        pred = discriminator(X)\n",
        "        loss = bce(pred, Y.float())\n",
        "\n",
        "        if i % 10 == 0:\n",
        "\n",
        "            # validate your model\n",
        "            X_val, Y_val = loader.fetch_batch(\"val\")\n",
        "            pred_val = discriminator(X_val)\n",
        "            loss_val = bce(pred_val, Y_val.float())\n",
        "\n",
        "            training_loss = loss.data#[0]\n",
        "            validation_loss = loss_val.data#[0]\n",
        "\n",
        "            print(\"Iteration: {} \\t Train: Acc={}%, Loss={} \\t\\t Validation: Acc={}%, Loss={}\".format(\n",
        "                i, get_pct_accuracy(pred, Y), training_loss, get_pct_accuracy(pred_val, Y_val), validation_loss\n",
        "            ))\n",
        "\n",
        "            if best_validation_loss is None:\n",
        "                best_validation_loss = validation_loss\n",
        "\n",
        "            if best_validation_loss > (saving_threshold * validation_loss):\n",
        "                print(\"Significantly improved validation loss from {} --> {}. Saving...\".format(\n",
        "                    best_validation_loss, validation_loss\n",
        "                ))\n",
        "                discriminator.save_to_file(os.path.join(models_path, str(validation_loss)))\n",
        "                best_validation_loss = validation_loss\n",
        "                last_saved = datetime.utcnow()\n",
        "\n",
        "            if last_saved + save_every < datetime.utcnow():\n",
        "                print(\"It's been too long since we last saved the model. Saving...\")\n",
        "                discriminator.save_to_file(os.path.join(models_path, str(validation_loss)))\n",
        "                last_saved = datetime.utcnow()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "def main() -> None:\n",
        "    train()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPNmyUcoDodI",
        "outputId": "8ea97e57-6057-44eb-85b8-4cecb0f7fd95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 0 \t Train: Acc=50%, Loss=0.6943123936653137 \t\t Validation: Acc=52%, Loss=0.6931002736091614\n",
            "Iteration: 10 \t Train: Acc=50%, Loss=0.693226158618927 \t\t Validation: Acc=50%, Loss=0.6922935247421265\n",
            "Iteration: 20 \t Train: Acc=50%, Loss=0.6932269334793091 \t\t Validation: Acc=50%, Loss=0.6926368474960327\n",
            "Iteration: 30 \t Train: Acc=50%, Loss=0.6931658387184143 \t\t Validation: Acc=57%, Loss=0.6892373561859131\n",
            "Iteration: 40 \t Train: Acc=48%, Loss=0.6932494640350342 \t\t Validation: Acc=49%, Loss=0.695663571357727\n",
            "Iteration: 50 \t Train: Acc=49%, Loss=0.6931873559951782 \t\t Validation: Acc=48%, Loss=0.6941076517105103\n",
            "Iteration: 60 \t Train: Acc=51%, Loss=0.6931415796279907 \t\t Validation: Acc=50%, Loss=0.6924882531166077\n",
            "Iteration: 70 \t Train: Acc=50%, Loss=0.6930981874465942 \t\t Validation: Acc=48%, Loss=0.6946386098861694\n",
            "Iteration: 80 \t Train: Acc=50%, Loss=0.6931996941566467 \t\t Validation: Acc=52%, Loss=0.6911894083023071\n",
            "Iteration: 90 \t Train: Acc=50%, Loss=0.6931205987930298 \t\t Validation: Acc=49%, Loss=0.6949449181556702\n",
            "Iteration: 100 \t Train: Acc=40%, Loss=0.6934763789176941 \t\t Validation: Acc=44%, Loss=0.6962155699729919\n",
            "Iteration: 110 \t Train: Acc=49%, Loss=0.6930423974990845 \t\t Validation: Acc=51%, Loss=0.6932079792022705\n",
            "Iteration: 120 \t Train: Acc=56%, Loss=0.6929852962493896 \t\t Validation: Acc=49%, Loss=0.6943262219429016\n",
            "Iteration: 130 \t Train: Acc=43%, Loss=0.6932346224784851 \t\t Validation: Acc=50%, Loss=0.6937205195426941\n",
            "Iteration: 140 \t Train: Acc=53%, Loss=0.6930506825447083 \t\t Validation: Acc=51%, Loss=0.6933783888816833\n",
            "Iteration: 150 \t Train: Acc=50%, Loss=0.6929616332054138 \t\t Validation: Acc=48%, Loss=0.6945202946662903\n",
            "Iteration: 160 \t Train: Acc=49%, Loss=0.6930959820747375 \t\t Validation: Acc=54%, Loss=0.6914131045341492\n",
            "Iteration: 170 \t Train: Acc=43%, Loss=0.6933711767196655 \t\t Validation: Acc=50%, Loss=0.6909370422363281\n",
            "Iteration: 180 \t Train: Acc=44%, Loss=0.6933243274688721 \t\t Validation: Acc=48%, Loss=0.69594806432724\n",
            "Iteration: 190 \t Train: Acc=47%, Loss=0.6931999325752258 \t\t Validation: Acc=46%, Loss=0.6959769129753113\n",
            "Iteration: 200 \t Train: Acc=46%, Loss=0.6932007074356079 \t\t Validation: Acc=48%, Loss=0.6956290006637573\n",
            "Iteration: 210 \t Train: Acc=49%, Loss=0.6929253935813904 \t\t Validation: Acc=43%, Loss=0.7026873230934143\n",
            "Iteration: 220 \t Train: Acc=50%, Loss=0.6928064227104187 \t\t Validation: Acc=50%, Loss=0.693673312664032\n",
            "Iteration: 230 \t Train: Acc=49%, Loss=0.6931678056716919 \t\t Validation: Acc=47%, Loss=0.6956540942192078\n",
            "Iteration: 240 \t Train: Acc=45%, Loss=0.6935238242149353 \t\t Validation: Acc=49%, Loss=0.6966776251792908\n",
            "Iteration: 250 \t Train: Acc=48%, Loss=0.6931278705596924 \t\t Validation: Acc=52%, Loss=0.6924729943275452\n",
            "Iteration: 260 \t Train: Acc=48%, Loss=0.6932312250137329 \t\t Validation: Acc=47%, Loss=0.698131263256073\n",
            "Iteration: 270 \t Train: Acc=50%, Loss=0.6925061345100403 \t\t Validation: Acc=46%, Loss=0.7018572092056274\n",
            "Iteration: 280 \t Train: Acc=55%, Loss=0.6926355957984924 \t\t Validation: Acc=48%, Loss=0.7013267278671265\n",
            "Iteration: 290 \t Train: Acc=51%, Loss=0.6928079128265381 \t\t Validation: Acc=47%, Loss=0.7009181380271912\n",
            "Iteration: 300 \t Train: Acc=47%, Loss=0.6935577988624573 \t\t Validation: Acc=53%, Loss=0.6904274225234985\n",
            "Iteration: 310 \t Train: Acc=53%, Loss=0.6915309429168701 \t\t Validation: Acc=53%, Loss=0.684931218624115\n",
            "Iteration: 320 \t Train: Acc=53%, Loss=0.692743718624115 \t\t Validation: Acc=46%, Loss=0.7057651281356812\n",
            "Iteration: 330 \t Train: Acc=44%, Loss=0.6938425898551941 \t\t Validation: Acc=48%, Loss=0.7014104127883911\n",
            "Iteration: 340 \t Train: Acc=53%, Loss=0.6913792490959167 \t\t Validation: Acc=42%, Loss=0.7080984711647034\n",
            "Iteration: 350 \t Train: Acc=50%, Loss=0.6929715275764465 \t\t Validation: Acc=44%, Loss=0.707761287689209\n",
            "Iteration: 360 \t Train: Acc=56%, Loss=0.692514955997467 \t\t Validation: Acc=48%, Loss=0.6987641453742981\n",
            "Iteration: 370 \t Train: Acc=53%, Loss=0.6933568716049194 \t\t Validation: Acc=43%, Loss=0.7075821757316589\n",
            "Iteration: 380 \t Train: Acc=49%, Loss=0.6932131052017212 \t\t Validation: Acc=50%, Loss=0.6951264142990112\n",
            "Iteration: 390 \t Train: Acc=49%, Loss=0.6919777989387512 \t\t Validation: Acc=47%, Loss=0.6971460580825806\n",
            "Iteration: 400 \t Train: Acc=49%, Loss=0.6927703619003296 \t\t Validation: Acc=52%, Loss=0.6926615834236145\n",
            "Iteration: 410 \t Train: Acc=42%, Loss=0.6949937343597412 \t\t Validation: Acc=46%, Loss=0.6997747421264648\n",
            "Iteration: 420 \t Train: Acc=49%, Loss=0.6924590468406677 \t\t Validation: Acc=46%, Loss=0.7038117051124573\n",
            "Iteration: 430 \t Train: Acc=51%, Loss=0.6927043199539185 \t\t Validation: Acc=49%, Loss=0.6955715417861938\n",
            "Iteration: 440 \t Train: Acc=55%, Loss=0.6924226880073547 \t\t Validation: Acc=48%, Loss=0.6983641982078552\n",
            "Iteration: 450 \t Train: Acc=46%, Loss=0.6934472918510437 \t\t Validation: Acc=50%, Loss=0.6997649669647217\n",
            "Iteration: 460 \t Train: Acc=52%, Loss=0.6927874088287354 \t\t Validation: Acc=48%, Loss=0.7041342258453369\n",
            "Iteration: 470 \t Train: Acc=51%, Loss=0.6932830214500427 \t\t Validation: Acc=52%, Loss=0.6851136088371277\n",
            "Iteration: 480 \t Train: Acc=49%, Loss=0.6925602555274963 \t\t Validation: Acc=51%, Loss=0.690464973449707\n",
            "Iteration: 490 \t Train: Acc=50%, Loss=0.6931638717651367 \t\t Validation: Acc=53%, Loss=0.6895914077758789\n",
            "Iteration: 500 \t Train: Acc=49%, Loss=0.6933318376541138 \t\t Validation: Acc=46%, Loss=0.698977530002594\n",
            "Iteration: 510 \t Train: Acc=45%, Loss=0.6939278841018677 \t\t Validation: Acc=42%, Loss=0.7053835988044739\n",
            "Iteration: 520 \t Train: Acc=54%, Loss=0.6920208930969238 \t\t Validation: Acc=53%, Loss=0.6850504875183105\n",
            "Iteration: 530 \t Train: Acc=48%, Loss=0.6930628418922424 \t\t Validation: Acc=45%, Loss=0.7055441737174988\n",
            "Iteration: 540 \t Train: Acc=48%, Loss=0.6933896541595459 \t\t Validation: Acc=51%, Loss=0.6947493553161621\n",
            "Iteration: 550 \t Train: Acc=51%, Loss=0.69306480884552 \t\t Validation: Acc=52%, Loss=0.6935782432556152\n",
            "Iteration: 560 \t Train: Acc=51%, Loss=0.693129301071167 \t\t Validation: Acc=51%, Loss=0.6967633962631226\n",
            "Iteration: 570 \t Train: Acc=50%, Loss=0.6930325627326965 \t\t Validation: Acc=46%, Loss=0.6991967558860779\n",
            "Iteration: 580 \t Train: Acc=52%, Loss=0.6930002570152283 \t\t Validation: Acc=53%, Loss=0.6870156526565552\n",
            "Iteration: 590 \t Train: Acc=50%, Loss=0.693121075630188 \t\t Validation: Acc=50%, Loss=0.6999694108963013\n",
            "Iteration: 600 \t Train: Acc=53%, Loss=0.6928847432136536 \t\t Validation: Acc=53%, Loss=0.6891512274742126\n",
            "Iteration: 610 \t Train: Acc=45%, Loss=0.6929603219032288 \t\t Validation: Acc=50%, Loss=0.6939566731452942\n",
            "Iteration: 620 \t Train: Acc=49%, Loss=0.6932413578033447 \t\t Validation: Acc=53%, Loss=0.6882842183113098\n",
            "Iteration: 630 \t Train: Acc=50%, Loss=0.6932570934295654 \t\t Validation: Acc=55%, Loss=0.6911488771438599\n",
            "Iteration: 640 \t Train: Acc=54%, Loss=0.6929935216903687 \t\t Validation: Acc=47%, Loss=0.696060061454773\n",
            "Iteration: 650 \t Train: Acc=50%, Loss=0.6931155323982239 \t\t Validation: Acc=55%, Loss=0.691564679145813\n",
            "Iteration: 660 \t Train: Acc=55%, Loss=0.6927939057350159 \t\t Validation: Acc=50%, Loss=0.6898155212402344\n",
            "Iteration: 670 \t Train: Acc=54%, Loss=0.6927376389503479 \t\t Validation: Acc=48%, Loss=0.6965768933296204\n",
            "Iteration: 680 \t Train: Acc=50%, Loss=0.6931225061416626 \t\t Validation: Acc=46%, Loss=0.7051652073860168\n",
            "Iteration: 690 \t Train: Acc=53%, Loss=0.6931341886520386 \t\t Validation: Acc=53%, Loss=0.6851359009742737\n",
            "Iteration: 700 \t Train: Acc=54%, Loss=0.6926168203353882 \t\t Validation: Acc=45%, Loss=0.7100656032562256\n",
            "Iteration: 710 \t Train: Acc=47%, Loss=0.6932111978530884 \t\t Validation: Acc=47%, Loss=0.6985810399055481\n",
            "Iteration: 720 \t Train: Acc=47%, Loss=0.6928297281265259 \t\t Validation: Acc=51%, Loss=0.6965023279190063\n",
            "Iteration: 730 \t Train: Acc=46%, Loss=0.6935234069824219 \t\t Validation: Acc=46%, Loss=0.7083325982093811\n",
            "Iteration: 740 \t Train: Acc=53%, Loss=0.6917550563812256 \t\t Validation: Acc=50%, Loss=0.6917088627815247\n",
            "Iteration: 750 \t Train: Acc=52%, Loss=0.6929088234901428 \t\t Validation: Acc=51%, Loss=0.6959453225135803\n",
            "Iteration: 760 \t Train: Acc=54%, Loss=0.6935691833496094 \t\t Validation: Acc=47%, Loss=0.7056548595428467\n",
            "Iteration: 770 \t Train: Acc=51%, Loss=0.6924772262573242 \t\t Validation: Acc=57%, Loss=0.6814563870429993\n",
            "Iteration: 780 \t Train: Acc=47%, Loss=0.6932139992713928 \t\t Validation: Acc=49%, Loss=0.699316143989563\n",
            "Iteration: 790 \t Train: Acc=53%, Loss=0.6928897500038147 \t\t Validation: Acc=53%, Loss=0.6885148286819458\n",
            "Iteration: 800 \t Train: Acc=49%, Loss=0.6930408477783203 \t\t Validation: Acc=49%, Loss=0.69815993309021\n",
            "Iteration: 810 \t Train: Acc=53%, Loss=0.693067729473114 \t\t Validation: Acc=51%, Loss=0.7054967284202576\n",
            "Iteration: 820 \t Train: Acc=53%, Loss=0.6928502917289734 \t\t Validation: Acc=47%, Loss=0.7072198390960693\n",
            "Iteration: 830 \t Train: Acc=53%, Loss=0.6928771138191223 \t\t Validation: Acc=44%, Loss=0.7090449929237366\n",
            "Iteration: 840 \t Train: Acc=53%, Loss=0.6928837895393372 \t\t Validation: Acc=42%, Loss=0.7056577205657959\n",
            "Iteration: 850 \t Train: Acc=44%, Loss=0.6935089230537415 \t\t Validation: Acc=51%, Loss=0.6916553974151611\n",
            "Iteration: 860 \t Train: Acc=44%, Loss=0.6931318640708923 \t\t Validation: Acc=50%, Loss=0.6934876441955566\n",
            "Iteration: 870 \t Train: Acc=47%, Loss=0.6931629180908203 \t\t Validation: Acc=43%, Loss=0.7147733569145203\n",
            "Iteration: 880 \t Train: Acc=50%, Loss=0.6933157444000244 \t\t Validation: Acc=58%, Loss=0.6806215047836304\n",
            "Iteration: 890 \t Train: Acc=48%, Loss=0.6931978464126587 \t\t Validation: Acc=50%, Loss=0.6979318261146545\n",
            "Iteration: 900 \t Train: Acc=48%, Loss=0.693764328956604 \t\t Validation: Acc=46%, Loss=0.699532151222229\n",
            "Iteration: 910 \t Train: Acc=47%, Loss=0.6935041546821594 \t\t Validation: Acc=48%, Loss=0.6984809637069702\n",
            "Iteration: 920 \t Train: Acc=49%, Loss=0.6933220624923706 \t\t Validation: Acc=50%, Loss=0.6945335865020752\n",
            "Iteration: 930 \t Train: Acc=51%, Loss=0.692813515663147 \t\t Validation: Acc=50%, Loss=0.6928698420524597\n",
            "Iteration: 940 \t Train: Acc=49%, Loss=0.6937971115112305 \t\t Validation: Acc=50%, Loss=0.6980632543563843\n",
            "Iteration: 950 \t Train: Acc=48%, Loss=0.693198025226593 \t\t Validation: Acc=46%, Loss=0.7016491889953613\n",
            "Iteration: 960 \t Train: Acc=50%, Loss=0.6932834386825562 \t\t Validation: Acc=50%, Loss=0.6920098662376404\n",
            "Iteration: 970 \t Train: Acc=50%, Loss=0.692954421043396 \t\t Validation: Acc=50%, Loss=0.6927575469017029\n",
            "Iteration: 980 \t Train: Acc=53%, Loss=0.6929867267608643 \t\t Validation: Acc=53%, Loss=0.6906251907348633\n",
            "Iteration: 990 \t Train: Acc=51%, Loss=0.6931504011154175 \t\t Validation: Acc=50%, Loss=0.6914693117141724\n",
            "Iteration: 1000 \t Train: Acc=46%, Loss=0.6933600306510925 \t\t Validation: Acc=53%, Loss=0.6899164319038391\n",
            "Iteration: 1010 \t Train: Acc=48%, Loss=0.6930567026138306 \t\t Validation: Acc=45%, Loss=0.6949991583824158\n",
            "Iteration: 1020 \t Train: Acc=50%, Loss=0.6932827830314636 \t\t Validation: Acc=48%, Loss=0.6941245198249817\n",
            "Iteration: 1030 \t Train: Acc=54%, Loss=0.6930152773857117 \t\t Validation: Acc=47%, Loss=0.6955355405807495\n",
            "Iteration: 1040 \t Train: Acc=50%, Loss=0.6930755972862244 \t\t Validation: Acc=52%, Loss=0.69246506690979\n",
            "Iteration: 1050 \t Train: Acc=56%, Loss=0.6930277943611145 \t\t Validation: Acc=52%, Loss=0.6927433609962463\n",
            "Iteration: 1060 \t Train: Acc=47%, Loss=0.6930915713310242 \t\t Validation: Acc=50%, Loss=0.6930877566337585\n",
            "Iteration: 1070 \t Train: Acc=43%, Loss=0.6932278871536255 \t\t Validation: Acc=51%, Loss=0.6925919651985168\n",
            "Iteration: 1080 \t Train: Acc=50%, Loss=0.6934140920639038 \t\t Validation: Acc=47%, Loss=0.6955205202102661\n",
            "Iteration: 1090 \t Train: Acc=56%, Loss=0.6929885745048523 \t\t Validation: Acc=52%, Loss=0.6929396390914917\n",
            "Iteration: 1100 \t Train: Acc=53%, Loss=0.6930102109909058 \t\t Validation: Acc=50%, Loss=0.6957194805145264\n",
            "Iteration: 1110 \t Train: Acc=50%, Loss=0.6932278275489807 \t\t Validation: Acc=46%, Loss=0.6954330205917358\n",
            "Iteration: 1120 \t Train: Acc=53%, Loss=0.6930654048919678 \t\t Validation: Acc=53%, Loss=0.6915144920349121\n",
            "Iteration: 1130 \t Train: Acc=53%, Loss=0.6929868459701538 \t\t Validation: Acc=50%, Loss=0.6936666369438171\n",
            "Iteration: 1140 \t Train: Acc=49%, Loss=0.6932082176208496 \t\t Validation: Acc=46%, Loss=0.696954071521759\n",
            "Iteration: 1150 \t Train: Acc=46%, Loss=0.6934219002723694 \t\t Validation: Acc=52%, Loss=0.6922121047973633\n",
            "Iteration: 1160 \t Train: Acc=52%, Loss=0.6930681467056274 \t\t Validation: Acc=47%, Loss=0.6930803656578064\n",
            "Iteration: 1170 \t Train: Acc=50%, Loss=0.6931514739990234 \t\t Validation: Acc=51%, Loss=0.6932221055030823\n",
            "Iteration: 1180 \t Train: Acc=54%, Loss=0.6929259300231934 \t\t Validation: Acc=50%, Loss=0.6931825280189514\n",
            "Iteration: 1190 \t Train: Acc=52%, Loss=0.6932371258735657 \t\t Validation: Acc=48%, Loss=0.6958661675453186\n",
            "Iteration: 1200 \t Train: Acc=50%, Loss=0.6930169463157654 \t\t Validation: Acc=53%, Loss=0.6936644315719604\n",
            "Iteration: 1210 \t Train: Acc=51%, Loss=0.6930468082427979 \t\t Validation: Acc=55%, Loss=0.6916646957397461\n",
            "Iteration: 1220 \t Train: Acc=51%, Loss=0.6931158304214478 \t\t Validation: Acc=47%, Loss=0.6943648457527161\n",
            "Iteration: 1230 \t Train: Acc=49%, Loss=0.6931191682815552 \t\t Validation: Acc=50%, Loss=0.6963927745819092\n",
            "Iteration: 1240 \t Train: Acc=51%, Loss=0.6930063366889954 \t\t Validation: Acc=49%, Loss=0.6931244730949402\n",
            "Iteration: 1250 \t Train: Acc=53%, Loss=0.6930275559425354 \t\t Validation: Acc=47%, Loss=0.6923710703849792\n",
            "Iteration: 1260 \t Train: Acc=60%, Loss=0.6928996443748474 \t\t Validation: Acc=50%, Loss=0.6882095336914062\n",
            "Iteration: 1270 \t Train: Acc=53%, Loss=0.6931341290473938 \t\t Validation: Acc=51%, Loss=0.6943963766098022\n",
            "Iteration: 1280 \t Train: Acc=49%, Loss=0.6928938031196594 \t\t Validation: Acc=53%, Loss=0.6921191811561584\n",
            "Iteration: 1290 \t Train: Acc=54%, Loss=0.692976713180542 \t\t Validation: Acc=50%, Loss=0.6928113102912903\n",
            "Iteration: 1300 \t Train: Acc=57%, Loss=0.6927198171615601 \t\t Validation: Acc=51%, Loss=0.6927333474159241\n",
            "Iteration: 1310 \t Train: Acc=53%, Loss=0.6927588582038879 \t\t Validation: Acc=49%, Loss=0.6911953091621399\n",
            "Iteration: 1320 \t Train: Acc=50%, Loss=0.6932862997055054 \t\t Validation: Acc=45%, Loss=0.7005103826522827\n",
            "Iteration: 1330 \t Train: Acc=51%, Loss=0.6931397914886475 \t\t Validation: Acc=49%, Loss=0.6989477276802063\n",
            "Iteration: 1340 \t Train: Acc=50%, Loss=0.6928803324699402 \t\t Validation: Acc=50%, Loss=0.6979252099990845\n",
            "Iteration: 1350 \t Train: Acc=47%, Loss=0.6931977868080139 \t\t Validation: Acc=49%, Loss=0.6940448880195618\n",
            "Iteration: 1360 \t Train: Acc=49%, Loss=0.6933474540710449 \t\t Validation: Acc=49%, Loss=0.696325421333313\n",
            "Iteration: 1370 \t Train: Acc=50%, Loss=0.6930972337722778 \t\t Validation: Acc=46%, Loss=0.6906903386116028\n",
            "Iteration: 1380 \t Train: Acc=46%, Loss=0.6932995915412903 \t\t Validation: Acc=52%, Loss=0.6945573091506958\n",
            "Iteration: 1390 \t Train: Acc=50%, Loss=0.6930395364761353 \t\t Validation: Acc=50%, Loss=0.6904155015945435\n",
            "Iteration: 1400 \t Train: Acc=50%, Loss=0.6927036643028259 \t\t Validation: Acc=50%, Loss=0.6952483057975769\n",
            "Iteration: 1410 \t Train: Acc=53%, Loss=0.6926790475845337 \t\t Validation: Acc=52%, Loss=0.6942641139030457\n",
            "Iteration: 1420 \t Train: Acc=48%, Loss=0.6931369304656982 \t\t Validation: Acc=48%, Loss=0.6955318450927734\n",
            "Iteration: 1430 \t Train: Acc=50%, Loss=0.693222165107727 \t\t Validation: Acc=50%, Loss=0.6924060583114624\n",
            "Iteration: 1440 \t Train: Acc=50%, Loss=0.6932739615440369 \t\t Validation: Acc=58%, Loss=0.6838400363922119\n",
            "Iteration: 1450 \t Train: Acc=48%, Loss=0.6932708621025085 \t\t Validation: Acc=47%, Loss=0.6928200125694275\n",
            "Iteration: 1460 \t Train: Acc=51%, Loss=0.6930569410324097 \t\t Validation: Acc=47%, Loss=0.694756269454956\n",
            "Iteration: 1470 \t Train: Acc=50%, Loss=0.693291425704956 \t\t Validation: Acc=50%, Loss=0.6914170384407043\n",
            "Iteration: 1480 \t Train: Acc=50%, Loss=0.6930745244026184 \t\t Validation: Acc=52%, Loss=0.6905036568641663\n",
            "Iteration: 1490 \t Train: Acc=48%, Loss=0.6934231519699097 \t\t Validation: Acc=46%, Loss=0.6955441236495972\n",
            "Iteration: 1500 \t Train: Acc=53%, Loss=0.6929345726966858 \t\t Validation: Acc=47%, Loss=0.6982589960098267\n",
            "Iteration: 1510 \t Train: Acc=50%, Loss=0.6931434869766235 \t\t Validation: Acc=51%, Loss=0.6930286884307861\n",
            "Iteration: 1520 \t Train: Acc=46%, Loss=0.6932271718978882 \t\t Validation: Acc=49%, Loss=0.6934276819229126\n",
            "Iteration: 1530 \t Train: Acc=46%, Loss=0.6938796043395996 \t\t Validation: Acc=49%, Loss=0.6956262588500977\n",
            "Iteration: 1540 \t Train: Acc=50%, Loss=0.6932327151298523 \t\t Validation: Acc=51%, Loss=0.6934680342674255\n",
            "Iteration: 1550 \t Train: Acc=53%, Loss=0.6930907964706421 \t\t Validation: Acc=52%, Loss=0.6913202404975891\n",
            "Iteration: 1560 \t Train: Acc=51%, Loss=0.6930860280990601 \t\t Validation: Acc=49%, Loss=0.6984780430793762\n",
            "Iteration: 1570 \t Train: Acc=51%, Loss=0.692846417427063 \t\t Validation: Acc=43%, Loss=0.7025755643844604\n",
            "Iteration: 1580 \t Train: Acc=48%, Loss=0.6933274269104004 \t\t Validation: Acc=48%, Loss=0.6915870308876038\n",
            "Iteration: 1590 \t Train: Acc=50%, Loss=0.6931396126747131 \t\t Validation: Acc=47%, Loss=0.6962807774543762\n",
            "Iteration: 1600 \t Train: Acc=51%, Loss=0.6929459571838379 \t\t Validation: Acc=46%, Loss=0.6991621255874634\n",
            "Iteration: 1610 \t Train: Acc=44%, Loss=0.6934279203414917 \t\t Validation: Acc=51%, Loss=0.6909723281860352\n",
            "Iteration: 1620 \t Train: Acc=48%, Loss=0.6931974291801453 \t\t Validation: Acc=51%, Loss=0.6902257800102234\n",
            "Iteration: 1630 \t Train: Acc=47%, Loss=0.6932363510131836 \t\t Validation: Acc=53%, Loss=0.6901458501815796\n",
            "Iteration: 1640 \t Train: Acc=47%, Loss=0.6934311985969543 \t\t Validation: Acc=57%, Loss=0.6897376179695129\n",
            "Iteration: 1650 \t Train: Acc=50%, Loss=0.6928043365478516 \t\t Validation: Acc=43%, Loss=0.7057108283042908\n",
            "Iteration: 1660 \t Train: Acc=54%, Loss=0.6921370029449463 \t\t Validation: Acc=46%, Loss=0.7021865844726562\n",
            "Iteration: 1670 \t Train: Acc=46%, Loss=0.6936373114585876 \t\t Validation: Acc=49%, Loss=0.7002134919166565\n",
            "Iteration: 1680 \t Train: Acc=49%, Loss=0.692730188369751 \t\t Validation: Acc=56%, Loss=0.6833391189575195\n",
            "Iteration: 1690 \t Train: Acc=45%, Loss=0.6935605406761169 \t\t Validation: Acc=51%, Loss=0.6918211579322815\n",
            "Iteration: 1700 \t Train: Acc=48%, Loss=0.6929959058761597 \t\t Validation: Acc=49%, Loss=0.69126296043396\n",
            "Iteration: 1710 \t Train: Acc=50%, Loss=0.6931798458099365 \t\t Validation: Acc=43%, Loss=0.701907753944397\n",
            "Iteration: 1720 \t Train: Acc=51%, Loss=0.6930028200149536 \t\t Validation: Acc=50%, Loss=0.6904706954956055\n",
            "Iteration: 1730 \t Train: Acc=49%, Loss=0.6935769319534302 \t\t Validation: Acc=48%, Loss=0.6983786225318909\n",
            "Iteration: 1740 \t Train: Acc=53%, Loss=0.6926750540733337 \t\t Validation: Acc=49%, Loss=0.6935835480690002\n",
            "Iteration: 1750 \t Train: Acc=44%, Loss=0.6933527588844299 \t\t Validation: Acc=50%, Loss=0.6955903768539429\n",
            "Iteration: 1760 \t Train: Acc=47%, Loss=0.6937737464904785 \t\t Validation: Acc=51%, Loss=0.6910010576248169\n",
            "Iteration: 1770 \t Train: Acc=49%, Loss=0.6933011412620544 \t\t Validation: Acc=48%, Loss=0.6914222836494446\n",
            "Iteration: 1780 \t Train: Acc=49%, Loss=0.693656325340271 \t\t Validation: Acc=43%, Loss=0.7007840871810913\n",
            "Iteration: 1790 \t Train: Acc=48%, Loss=0.6930667757987976 \t\t Validation: Acc=50%, Loss=0.6941590309143066\n",
            "Iteration: 1800 \t Train: Acc=51%, Loss=0.6930432319641113 \t\t Validation: Acc=48%, Loss=0.6956443786621094\n",
            "Iteration: 1810 \t Train: Acc=50%, Loss=0.693147599697113 \t\t Validation: Acc=52%, Loss=0.6972424983978271\n",
            "Iteration: 1820 \t Train: Acc=52%, Loss=0.6931058168411255 \t\t Validation: Acc=50%, Loss=0.6909689903259277\n",
            "Iteration: 1830 \t Train: Acc=50%, Loss=0.6931759119033813 \t\t Validation: Acc=51%, Loss=0.6886439919471741\n",
            "Iteration: 1840 \t Train: Acc=48%, Loss=0.693407416343689 \t\t Validation: Acc=49%, Loss=0.6912527680397034\n",
            "Iteration: 1850 \t Train: Acc=50%, Loss=0.6930186748504639 \t\t Validation: Acc=50%, Loss=0.6890368461608887\n",
            "Iteration: 1860 \t Train: Acc=48%, Loss=0.6931944489479065 \t\t Validation: Acc=49%, Loss=0.6957017183303833\n",
            "Iteration: 1870 \t Train: Acc=46%, Loss=0.69345623254776 \t\t Validation: Acc=46%, Loss=0.6926722526550293\n",
            "Iteration: 1880 \t Train: Acc=47%, Loss=0.6930375695228577 \t\t Validation: Acc=50%, Loss=0.6940484046936035\n",
            "Iteration: 1890 \t Train: Acc=50%, Loss=0.6930415034294128 \t\t Validation: Acc=50%, Loss=0.6925293207168579\n",
            "Iteration: 1900 \t Train: Acc=51%, Loss=0.6935738921165466 \t\t Validation: Acc=43%, Loss=0.6930491328239441\n",
            "Iteration: 1910 \t Train: Acc=46%, Loss=0.692529559135437 \t\t Validation: Acc=49%, Loss=0.6964660882949829\n",
            "Iteration: 1920 \t Train: Acc=47%, Loss=0.6931666731834412 \t\t Validation: Acc=50%, Loss=0.6991170048713684\n",
            "Iteration: 1930 \t Train: Acc=52%, Loss=0.6930962204933167 \t\t Validation: Acc=46%, Loss=0.7107110619544983\n",
            "Iteration: 1940 \t Train: Acc=51%, Loss=0.6940268874168396 \t\t Validation: Acc=42%, Loss=0.712202787399292\n",
            "Iteration: 1950 \t Train: Acc=53%, Loss=0.6930450201034546 \t\t Validation: Acc=49%, Loss=0.6980152130126953\n",
            "Iteration: 1960 \t Train: Acc=53%, Loss=0.6928252577781677 \t\t Validation: Acc=50%, Loss=0.6954655647277832\n",
            "Iteration: 1970 \t Train: Acc=46%, Loss=0.6928476691246033 \t\t Validation: Acc=53%, Loss=0.6866527199745178\n",
            "Iteration: 1980 \t Train: Acc=53%, Loss=0.6922584176063538 \t\t Validation: Acc=48%, Loss=0.6988498568534851\n",
            "Iteration: 1990 \t Train: Acc=52%, Loss=0.6918359994888306 \t\t Validation: Acc=51%, Loss=0.7023212909698486\n",
            "Iteration: 2000 \t Train: Acc=49%, Loss=0.6936154365539551 \t\t Validation: Acc=49%, Loss=0.7141045928001404\n",
            "Iteration: 2010 \t Train: Acc=50%, Loss=0.6927807927131653 \t\t Validation: Acc=45%, Loss=0.7353296279907227\n",
            "Iteration: 2020 \t Train: Acc=50%, Loss=0.6928442716598511 \t\t Validation: Acc=46%, Loss=0.7147557735443115\n",
            "Iteration: 2030 \t Train: Acc=50%, Loss=0.6926408410072327 \t\t Validation: Acc=50%, Loss=0.6944864392280579\n",
            "Iteration: 2040 \t Train: Acc=53%, Loss=0.6929919123649597 \t\t Validation: Acc=50%, Loss=0.694383442401886\n",
            "Iteration: 2050 \t Train: Acc=49%, Loss=0.6931145191192627 \t\t Validation: Acc=49%, Loss=0.6922729015350342\n",
            "Iteration: 2060 \t Train: Acc=48%, Loss=0.6931731700897217 \t\t Validation: Acc=52%, Loss=0.68780916929245\n",
            "Iteration: 2070 \t Train: Acc=49%, Loss=0.6930932998657227 \t\t Validation: Acc=53%, Loss=0.6905313730239868\n",
            "Iteration: 2080 \t Train: Acc=47%, Loss=0.6931831240653992 \t\t Validation: Acc=45%, Loss=0.7039997577667236\n",
            "Iteration: 2090 \t Train: Acc=48%, Loss=0.6933684349060059 \t\t Validation: Acc=46%, Loss=0.7063510417938232\n",
            "Iteration: 2100 \t Train: Acc=54%, Loss=0.6928036212921143 \t\t Validation: Acc=46%, Loss=0.7005942463874817\n",
            "Iteration: 2110 \t Train: Acc=52%, Loss=0.6927788257598877 \t\t Validation: Acc=44%, Loss=0.7058351039886475\n",
            "Iteration: 2120 \t Train: Acc=50%, Loss=0.6930214762687683 \t\t Validation: Acc=50%, Loss=0.6921836733818054\n",
            "Iteration: 2130 \t Train: Acc=53%, Loss=0.6927925944328308 \t\t Validation: Acc=49%, Loss=0.6995479464530945\n",
            "Iteration: 2140 \t Train: Acc=46%, Loss=0.6952410936355591 \t\t Validation: Acc=50%, Loss=0.6971006393432617\n",
            "Iteration: 2150 \t Train: Acc=52%, Loss=0.6923426985740662 \t\t Validation: Acc=48%, Loss=0.7006551027297974\n",
            "Iteration: 2160 \t Train: Acc=51%, Loss=0.6925650238990784 \t\t Validation: Acc=47%, Loss=0.701959490776062\n",
            "Iteration: 2170 \t Train: Acc=50%, Loss=0.6935935616493225 \t\t Validation: Acc=51%, Loss=0.6973305940628052\n",
            "Iteration: 2180 \t Train: Acc=49%, Loss=0.6924954652786255 \t\t Validation: Acc=50%, Loss=0.7233949303627014\n",
            "Iteration: 2190 \t Train: Acc=50%, Loss=0.6932353973388672 \t\t Validation: Acc=51%, Loss=0.6891148090362549\n",
            "Iteration: 2200 \t Train: Acc=47%, Loss=0.6930832266807556 \t\t Validation: Acc=46%, Loss=0.723437488079071\n",
            "Iteration: 2210 \t Train: Acc=50%, Loss=0.6929056644439697 \t\t Validation: Acc=53%, Loss=0.6882309317588806\n",
            "Iteration: 2220 \t Train: Acc=47%, Loss=0.6935102939605713 \t\t Validation: Acc=54%, Loss=0.6967023611068726\n",
            "Iteration: 2230 \t Train: Acc=51%, Loss=0.6909658908843994 \t\t Validation: Acc=49%, Loss=0.7173311710357666\n",
            "Iteration: 2240 \t Train: Acc=43%, Loss=0.6932139992713928 \t\t Validation: Acc=53%, Loss=0.6927007436752319\n",
            "Iteration: 2250 \t Train: Acc=57%, Loss=0.6917474269866943 \t\t Validation: Acc=51%, Loss=0.7006375789642334\n",
            "Iteration: 2260 \t Train: Acc=46%, Loss=0.6942039728164673 \t\t Validation: Acc=49%, Loss=0.7030141949653625\n",
            "Iteration: 2270 \t Train: Acc=57%, Loss=0.6929162740707397 \t\t Validation: Acc=48%, Loss=0.7018769383430481\n",
            "Iteration: 2280 \t Train: Acc=50%, Loss=0.693558394908905 \t\t Validation: Acc=51%, Loss=0.6926592588424683\n",
            "Iteration: 2290 \t Train: Acc=44%, Loss=0.6940261721611023 \t\t Validation: Acc=47%, Loss=0.7136028409004211\n",
            "Iteration: 2300 \t Train: Acc=51%, Loss=0.6931116580963135 \t\t Validation: Acc=47%, Loss=0.7085689902305603\n",
            "Iteration: 2310 \t Train: Acc=48%, Loss=0.6934590339660645 \t\t Validation: Acc=50%, Loss=0.7035344839096069\n",
            "Iteration: 2320 \t Train: Acc=50%, Loss=0.6932427287101746 \t\t Validation: Acc=53%, Loss=0.6903056502342224\n",
            "Iteration: 2330 \t Train: Acc=45%, Loss=0.6937741041183472 \t\t Validation: Acc=47%, Loss=0.7035896182060242\n",
            "Iteration: 2340 \t Train: Acc=49%, Loss=0.6931617856025696 \t\t Validation: Acc=46%, Loss=0.7115911245346069\n",
            "Iteration: 2350 \t Train: Acc=53%, Loss=0.6924852132797241 \t\t Validation: Acc=46%, Loss=0.7098474502563477\n",
            "Iteration: 2360 \t Train: Acc=46%, Loss=0.6932264566421509 \t\t Validation: Acc=42%, Loss=0.7196019291877747\n",
            "Iteration: 2370 \t Train: Acc=49%, Loss=0.6934908628463745 \t\t Validation: Acc=46%, Loss=0.7078050374984741\n",
            "Iteration: 2380 \t Train: Acc=46%, Loss=0.6935790181159973 \t\t Validation: Acc=47%, Loss=0.7056859731674194\n",
            "Iteration: 2390 \t Train: Acc=56%, Loss=0.6923924684524536 \t\t Validation: Acc=44%, Loss=0.71889728307724\n",
            "Iteration: 2400 \t Train: Acc=51%, Loss=0.6934759020805359 \t\t Validation: Acc=49%, Loss=0.7064933180809021\n",
            "Iteration: 2410 \t Train: Acc=49%, Loss=0.692737877368927 \t\t Validation: Acc=47%, Loss=0.7033735513687134\n",
            "Iteration: 2420 \t Train: Acc=48%, Loss=0.6932590007781982 \t\t Validation: Acc=50%, Loss=0.6996499300003052\n",
            "Iteration: 2430 \t Train: Acc=48%, Loss=0.6932400465011597 \t\t Validation: Acc=50%, Loss=0.6807423233985901\n",
            "Iteration: 2440 \t Train: Acc=48%, Loss=0.6931042671203613 \t\t Validation: Acc=50%, Loss=0.6938368678092957\n",
            "Iteration: 2450 \t Train: Acc=50%, Loss=0.6932176351547241 \t\t Validation: Acc=51%, Loss=0.6943863034248352\n",
            "Iteration: 2460 \t Train: Acc=49%, Loss=0.6933077573776245 \t\t Validation: Acc=50%, Loss=0.693403422832489\n",
            "Iteration: 2470 \t Train: Acc=46%, Loss=0.6937435269355774 \t\t Validation: Acc=50%, Loss=0.6917083263397217\n",
            "Iteration: 2480 \t Train: Acc=50%, Loss=0.692740797996521 \t\t Validation: Acc=50%, Loss=0.6924539804458618\n",
            "Iteration: 2490 \t Train: Acc=50%, Loss=0.6921716928482056 \t\t Validation: Acc=50%, Loss=0.6987341642379761\n",
            "Iteration: 2500 \t Train: Acc=50%, Loss=0.693425178527832 \t\t Validation: Acc=50%, Loss=0.6909923553466797\n",
            "Iteration: 2510 \t Train: Acc=50%, Loss=0.6929958462715149 \t\t Validation: Acc=49%, Loss=0.6833941340446472\n",
            "Iteration: 2520 \t Train: Acc=46%, Loss=0.6933820843696594 \t\t Validation: Acc=50%, Loss=0.6978513598442078\n",
            "Iteration: 2530 \t Train: Acc=51%, Loss=0.6930487155914307 \t\t Validation: Acc=50%, Loss=0.7043461203575134\n",
            "Iteration: 2540 \t Train: Acc=50%, Loss=0.6934369802474976 \t\t Validation: Acc=49%, Loss=0.697635293006897\n",
            "Iteration: 2550 \t Train: Acc=57%, Loss=0.6928995847702026 \t\t Validation: Acc=49%, Loss=0.7044243812561035\n",
            "Iteration: 2560 \t Train: Acc=50%, Loss=0.6927951574325562 \t\t Validation: Acc=43%, Loss=0.7093029618263245\n",
            "Iteration: 2570 \t Train: Acc=52%, Loss=0.6929284930229187 \t\t Validation: Acc=49%, Loss=0.6928038597106934\n",
            "Iteration: 2580 \t Train: Acc=49%, Loss=0.6916790008544922 \t\t Validation: Acc=50%, Loss=0.7091928720474243\n",
            "Iteration: 2590 \t Train: Acc=51%, Loss=0.6947818994522095 \t\t Validation: Acc=47%, Loss=0.7305774688720703\n",
            "Iteration: 2600 \t Train: Acc=50%, Loss=0.69364333152771 \t\t Validation: Acc=52%, Loss=0.6991939544677734\n",
            "Iteration: 2610 \t Train: Acc=50%, Loss=0.6935254335403442 \t\t Validation: Acc=45%, Loss=0.7296078205108643\n",
            "Iteration: 2620 \t Train: Acc=47%, Loss=0.6933000683784485 \t\t Validation: Acc=49%, Loss=0.7086566686630249\n",
            "Iteration: 2630 \t Train: Acc=55%, Loss=0.6926232576370239 \t\t Validation: Acc=50%, Loss=0.7005953788757324\n",
            "Iteration: 2640 \t Train: Acc=49%, Loss=0.6935345530509949 \t\t Validation: Acc=46%, Loss=0.7104769945144653\n",
            "Iteration: 2650 \t Train: Acc=52%, Loss=0.6922919154167175 \t\t Validation: Acc=50%, Loss=0.6970675587654114\n",
            "Iteration: 2660 \t Train: Acc=51%, Loss=0.6929064989089966 \t\t Validation: Acc=51%, Loss=0.6936984658241272\n",
            "Iteration: 2670 \t Train: Acc=48%, Loss=0.6953803300857544 \t\t Validation: Acc=50%, Loss=0.7094612717628479\n",
            "Iteration: 2680 \t Train: Acc=56%, Loss=0.68843674659729 \t\t Validation: Acc=44%, Loss=0.7429525852203369\n",
            "Iteration: 2690 \t Train: Acc=50%, Loss=0.6923913955688477 \t\t Validation: Acc=53%, Loss=0.7012432813644409\n",
            "Iteration: 2700 \t Train: Acc=48%, Loss=0.6945618987083435 \t\t Validation: Acc=45%, Loss=0.7506062984466553\n",
            "Iteration: 2710 \t Train: Acc=47%, Loss=0.6921271085739136 \t\t Validation: Acc=46%, Loss=0.716881275177002\n",
            "Iteration: 2720 \t Train: Acc=50%, Loss=0.691755473613739 \t\t Validation: Acc=51%, Loss=0.6988245248794556\n",
            "Iteration: 2730 \t Train: Acc=50%, Loss=0.6931315660476685 \t\t Validation: Acc=53%, Loss=0.6933236718177795\n",
            "Iteration: 2740 \t Train: Acc=49%, Loss=0.6945464611053467 \t\t Validation: Acc=48%, Loss=0.7065639495849609\n",
            "Iteration: 2750 \t Train: Acc=45%, Loss=0.694097101688385 \t\t Validation: Acc=47%, Loss=0.7062826156616211\n",
            "Iteration: 2760 \t Train: Acc=48%, Loss=0.6934742331504822 \t\t Validation: Acc=50%, Loss=0.7004215121269226\n",
            "Iteration: 2770 \t Train: Acc=51%, Loss=0.6924453377723694 \t\t Validation: Acc=50%, Loss=0.7032167911529541\n",
            "Iteration: 2780 \t Train: Acc=51%, Loss=0.6932836771011353 \t\t Validation: Acc=48%, Loss=0.6944060921669006\n",
            "Iteration: 2790 \t Train: Acc=49%, Loss=0.6922770738601685 \t\t Validation: Acc=50%, Loss=0.6999859809875488\n",
            "Iteration: 2800 \t Train: Acc=51%, Loss=0.6923707723617554 \t\t Validation: Acc=45%, Loss=0.7095861434936523\n",
            "Iteration: 2810 \t Train: Acc=48%, Loss=0.6927450895309448 \t\t Validation: Acc=50%, Loss=0.6962748765945435\n",
            "Iteration: 2820 \t Train: Acc=50%, Loss=0.6926296353340149 \t\t Validation: Acc=53%, Loss=0.6808514595031738\n",
            "Iteration: 2830 \t Train: Acc=59%, Loss=0.6926429271697998 \t\t Validation: Acc=50%, Loss=0.6922229528427124\n",
            "Iteration: 2840 \t Train: Acc=53%, Loss=0.6932010054588318 \t\t Validation: Acc=50%, Loss=0.6960605382919312\n",
            "Iteration: 2850 \t Train: Acc=53%, Loss=0.6927940845489502 \t\t Validation: Acc=49%, Loss=0.7049837112426758\n",
            "Iteration: 2860 \t Train: Acc=53%, Loss=0.6930001378059387 \t\t Validation: Acc=50%, Loss=0.6930255889892578\n",
            "Iteration: 2870 \t Train: Acc=51%, Loss=0.691999077796936 \t\t Validation: Acc=49%, Loss=0.6982603073120117\n",
            "Iteration: 2880 \t Train: Acc=51%, Loss=0.6925009489059448 \t\t Validation: Acc=51%, Loss=0.6932176351547241\n",
            "Iteration: 2890 \t Train: Acc=50%, Loss=0.6926146149635315 \t\t Validation: Acc=49%, Loss=0.68621826171875\n",
            "Iteration: 2900 \t Train: Acc=50%, Loss=0.6929099559783936 \t\t Validation: Acc=49%, Loss=0.6933779120445251\n",
            "Iteration: 2910 \t Train: Acc=47%, Loss=0.6929141283035278 \t\t Validation: Acc=50%, Loss=0.6997922658920288\n",
            "Iteration: 2920 \t Train: Acc=50%, Loss=0.6929277777671814 \t\t Validation: Acc=52%, Loss=0.6890361905097961\n",
            "Iteration: 2930 \t Train: Acc=51%, Loss=0.6934662461280823 \t\t Validation: Acc=50%, Loss=0.7102444171905518\n",
            "Iteration: 2940 \t Train: Acc=49%, Loss=0.6928299069404602 \t\t Validation: Acc=51%, Loss=0.699295699596405\n",
            "Iteration: 2950 \t Train: Acc=49%, Loss=0.6936817169189453 \t\t Validation: Acc=50%, Loss=0.694711446762085\n",
            "Iteration: 2960 \t Train: Acc=54%, Loss=0.6921038627624512 \t\t Validation: Acc=48%, Loss=0.6929244995117188\n",
            "Iteration: 2970 \t Train: Acc=55%, Loss=0.6923121809959412 \t\t Validation: Acc=47%, Loss=0.6891937851905823\n",
            "Iteration: 2980 \t Train: Acc=56%, Loss=0.6919979453086853 \t\t Validation: Acc=48%, Loss=0.6918419599533081\n",
            "Iteration: 2990 \t Train: Acc=49%, Loss=0.6937786340713501 \t\t Validation: Acc=51%, Loss=0.6911414861679077\n",
            "Iteration: 3000 \t Train: Acc=50%, Loss=0.6924150586128235 \t\t Validation: Acc=51%, Loss=0.6929064989089966\n",
            "Iteration: 3010 \t Train: Acc=46%, Loss=0.6950122117996216 \t\t Validation: Acc=49%, Loss=0.7069240808486938\n",
            "Iteration: 3020 \t Train: Acc=50%, Loss=0.6931552290916443 \t\t Validation: Acc=50%, Loss=0.6991719007492065\n",
            "Iteration: 3030 \t Train: Acc=51%, Loss=0.6934593915939331 \t\t Validation: Acc=52%, Loss=0.698030948638916\n",
            "Iteration: 3040 \t Train: Acc=50%, Loss=0.6924901008605957 \t\t Validation: Acc=49%, Loss=0.6881236433982849\n",
            "Iteration: 3050 \t Train: Acc=53%, Loss=0.6929920315742493 \t\t Validation: Acc=50%, Loss=0.6986921429634094\n",
            "Iteration: 3060 \t Train: Acc=48%, Loss=0.693311870098114 \t\t Validation: Acc=51%, Loss=0.6948836445808411\n",
            "Iteration: 3070 \t Train: Acc=54%, Loss=0.6924684643745422 \t\t Validation: Acc=48%, Loss=0.6972943544387817\n",
            "Iteration: 3080 \t Train: Acc=48%, Loss=0.693572461605072 \t\t Validation: Acc=48%, Loss=0.6917833089828491\n",
            "Iteration: 3090 \t Train: Acc=53%, Loss=0.6938840746879578 \t\t Validation: Acc=50%, Loss=0.6881692409515381\n",
            "Iteration: 3100 \t Train: Acc=50%, Loss=0.6926984190940857 \t\t Validation: Acc=50%, Loss=0.6924762725830078\n",
            "Iteration: 3110 \t Train: Acc=55%, Loss=0.6930525302886963 \t\t Validation: Acc=50%, Loss=0.7023155689239502\n",
            "Iteration: 3120 \t Train: Acc=53%, Loss=0.6924141049385071 \t\t Validation: Acc=49%, Loss=0.6978767514228821\n",
            "Iteration: 3130 \t Train: Acc=50%, Loss=0.6929701566696167 \t\t Validation: Acc=51%, Loss=0.707738995552063\n",
            "Iteration: 3140 \t Train: Acc=47%, Loss=0.6945469379425049 \t\t Validation: Acc=49%, Loss=0.7031941413879395\n",
            "Iteration: 3150 \t Train: Acc=54%, Loss=0.6919273138046265 \t\t Validation: Acc=50%, Loss=0.6954416036605835\n",
            "Iteration: 3160 \t Train: Acc=57%, Loss=0.6888108849525452 \t\t Validation: Acc=50%, Loss=0.7014078497886658\n",
            "Iteration: 3170 \t Train: Acc=57%, Loss=0.6924927234649658 \t\t Validation: Acc=50%, Loss=0.689004123210907\n",
            "Iteration: 3180 \t Train: Acc=50%, Loss=0.6933492422103882 \t\t Validation: Acc=50%, Loss=0.7151100039482117\n",
            "Iteration: 3190 \t Train: Acc=48%, Loss=0.6940904855728149 \t\t Validation: Acc=51%, Loss=0.6998369693756104\n",
            "Iteration: 3200 \t Train: Acc=49%, Loss=0.6915017366409302 \t\t Validation: Acc=51%, Loss=0.7037416696548462\n",
            "Iteration: 3210 \t Train: Acc=49%, Loss=0.693440318107605 \t\t Validation: Acc=49%, Loss=0.7113910913467407\n",
            "Iteration: 3220 \t Train: Acc=50%, Loss=0.6903512477874756 \t\t Validation: Acc=50%, Loss=0.7067104578018188\n",
            "Iteration: 3230 \t Train: Acc=52%, Loss=0.6920639276504517 \t\t Validation: Acc=50%, Loss=0.6916436553001404\n",
            "Iteration: 3240 \t Train: Acc=48%, Loss=0.6916806697845459 \t\t Validation: Acc=47%, Loss=0.7162240743637085\n",
            "Iteration: 3250 \t Train: Acc=52%, Loss=0.6962640881538391 \t\t Validation: Acc=50%, Loss=0.6820396184921265\n",
            "Iteration: 3260 \t Train: Acc=54%, Loss=0.6911874413490295 \t\t Validation: Acc=50%, Loss=0.6923503875732422\n",
            "Iteration: 3270 \t Train: Acc=49%, Loss=0.6935545802116394 \t\t Validation: Acc=50%, Loss=0.6999660730361938\n",
            "Iteration: 3280 \t Train: Acc=50%, Loss=0.6940082311630249 \t\t Validation: Acc=49%, Loss=0.7051594853401184\n",
            "Iteration: 3290 \t Train: Acc=47%, Loss=0.6949900388717651 \t\t Validation: Acc=50%, Loss=0.6997696161270142\n",
            "Iteration: 3300 \t Train: Acc=47%, Loss=0.6918960213661194 \t\t Validation: Acc=50%, Loss=0.6927621364593506\n",
            "Iteration: 3310 \t Train: Acc=48%, Loss=0.6944679021835327 \t\t Validation: Acc=48%, Loss=0.684594452381134\n",
            "Iteration: 3320 \t Train: Acc=48%, Loss=0.6918801069259644 \t\t Validation: Acc=46%, Loss=0.7100200653076172\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#vizualize\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--batchSize', type=int, default=128, help='input batch size')\n",
        "parser.add_argument('--imageSize', type=int, default=32, help='the height / width of the input image to ARC')\n",
        "parser.add_argument('--glimpseSize', type=int, default=8, help='the height / width of glimpse seen by ARC')\n",
        "parser.add_argument('--numStates', type=int, default=128, help='number of hidden states in ARC controller')\n",
        "parser.add_argument('--numGlimpses', type=int, default=6, help='the number glimpses of each image in pair seen by ARC')\n",
        "parser.add_argument('--lr', type=float, default=0.0002, help='learning rate, default=0.0002')\n",
        "parser.add_argument('--cuda', action='store_true', help='enables cuda')\n",
        "parser.add_argument('--name', default=None, help='Custom name for this configuration. Needed for loading model'\n",
        "                                                 'and saving images')\n",
        "parser.add_argument('--load', required=True, help='the model to load from.')\n",
        "parser.add_argument('--same', action='store_true', help='whether to generate same character pairs or not')\n",
        "\n",
        "opt = parser.parse_args()\n",
        "\n",
        "if opt.name is None:\n",
        "    # if no name is given, we generate a name from the parameters.\n",
        "    # only those parameters are taken, which if changed break torch.load compatibility.\n",
        "    opt.name = \"{}_{}_{}_{}\".format(opt.numGlimpses, opt.glimpseSize, opt.numStates,\n",
        "                                    \"cuda\" if opt.cuda else \"cpu\")\n",
        "\n",
        "# make directory for storing images.\n",
        "images_path = os.path.join(\"visualization\", opt.name)\n",
        "os.makedirs(images_path, exist_ok=True)\n",
        "\n",
        "\n",
        "# initialise the batcher\n",
        "batcher = Batcher(batch_size=opt.batchSize)\n",
        "\n",
        "\n",
        "def display(image1, mask1, image2, mask2, name=\"hola.png\"):\n",
        "    _, ax = plt.subplots(1, 2)\n",
        "\n",
        "    # a heuristic for deciding cutoff\n",
        "    masking_cutoff = 2.4 / (opt.glimpseSize)**2\n",
        "\n",
        "    mask1 = (mask1 > masking_cutoff).data.numpy()\n",
        "    mask1 = np.ma.masked_where(mask1 == 0, mask1)\n",
        "\n",
        "    mask2 = (mask2 > masking_cutoff).data.numpy()\n",
        "    mask2 = np.ma.masked_where(mask2 == 0, mask2)\n",
        "\n",
        "    ax[0].imshow(image1.data.numpy(), cmap=mpl.cm.bone)\n",
        "    ax[0].imshow(mask1, interpolation=\"nearest\", cmap=mpl.cm.jet_r, alpha=0.7)\n",
        "\n",
        "    ax[1].imshow(image2.data.numpy(), cmap=mpl.cm.bone)\n",
        "    ax[1].imshow(mask2, interpolation=\"nearest\", cmap=mpl.cm.ocean, alpha=0.7)\n",
        "\n",
        "    plt.savefig(os.path.join(images_path, name))\n",
        "\n",
        "\n",
        "def get_sample(discriminator):\n",
        "\n",
        "    # size of the set to choose sample from from\n",
        "    sample_size = 30\n",
        "    X, Y = batcher.fetch_batch(\"train\", batch_size=sample_size)\n",
        "    pred = discriminator(X)\n",
        "\n",
        "    if opt.same:\n",
        "        same_pred = pred[sample_size // 2:].data.numpy()[:, 0]\n",
        "        mx = same_pred.argsort()[len(same_pred) // 2]  # choose the sample with median confidence\n",
        "        index = mx + sample_size // 2\n",
        "    else:\n",
        "        diff_pred = pred[:sample_size // 2].data.numpy()[:, 0]\n",
        "        mx = diff_pred.argsort()[len(diff_pred) // 2]  # choose the sample with median confidence\n",
        "        index = mx\n",
        "\n",
        "    return X[index]\n",
        "\n",
        "\n",
        "def visualize():\n",
        "    # initialise the model\n",
        "    discriminator = ArcBinaryClassifier(num_glimpses=opt.numGlimpses,\n",
        "                                        glimpse_h=opt.glimpseSize,\n",
        "                                        glimpse_w=opt.glimpseSize,\n",
        "                                        controller_out=opt.numStates)\n",
        "    discriminator.load_state_dict(torch.load(os.path.join(\"saved_models\", opt.name, opt.load)))\n",
        "\n",
        "    arc = discriminator.arc\n",
        "    sample = get_sample(discriminator)\n",
        "    all_hidden = arc._forward(sample[None, :, :])[:, 0, :]  # (2*numGlimpses, controller_out)\n",
        "    glimpse_params = torch.tanh(arc.glimpser(all_hidden))\n",
        "    masks = arc.glimpse_window.get_attention_mask(glimpse_params, mask_h=opt.imageSize, mask_w=opt.imageSize)\n",
        "\n",
        "    # separate the masks of each image.\n",
        "    masks1 = []\n",
        "    masks2 = []\n",
        "    for i, mask in enumerate(masks):\n",
        "        if i % 2 == 1:  # the first image outputs the hidden state for the next image\n",
        "            masks1.append(mask)\n",
        "        else:\n",
        "            masks2.append(mask)\n",
        "\n",
        "    for i, (mask1, mask2) in enumerate(zip(masks1, masks2)):\n",
        "        display(sample[0], mask1, sample[1], mask2, \"img_{}\".format(i))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    visualize()"
      ],
      "metadata": {
        "id": "5bp1E0BRD04c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}