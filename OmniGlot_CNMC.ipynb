{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1W6P_KyJ0zJbVm8CaafuRHxRiuCyhvMUM",
      "authorship_tag": "ABX9TyPLkiMom+b0AjLqIpHQ6h07",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suhayb-h/Acute-Lymphoblastic-Leukemia-Classifier/blob/main/OmniGlot_CNMC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Download\n",
        "import os\n",
        "import urllib.request\n",
        "import numpy as np\n",
        "import zipfile\n",
        "from imageio import imread #changed from scipy.ndimage -> imageio\n",
        "\n",
        "#Batcher\n",
        "from numpy.random import choice\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from PIL import Image\n",
        "\n",
        "#Model\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "#Train\n",
        "import argparse\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "#vizualize\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl"
      ],
      "metadata": {
        "id": "pziLE6sG5yxK"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "xfevZwlVDb5v",
        "outputId": "a5bd4967-7d0f-41b8-d318-b3df9ff5b671"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-cae5bc24ddba>\"\u001b[0;36m, line \u001b[0;32m45\u001b[0m\n\u001b[0;31m    ))\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "#omniglot_url = 'http://github.com/brendenlake/omniglot/archive/master.zip'\n",
        "data_dir = os.path.join(\"/content/drive/MyDrive/Omniglot_ARC\")\n",
        "zip_location = os.path.join(data_dir, \"omniglot.zip\")\n",
        "unzip_location = os.path.join(data_dir, \"extracted\")\n",
        "zipped_images_location = os.path.join(unzip_location, \"omniglot-master\", \"python\")\n",
        "extracted_images_location = os.path.join(data_dir, \"images\")\n",
        "\n",
        "def download() -> None:\n",
        "    if os.path.exists(zip_location) and os.path.isfile(zip_location):\n",
        "        return\n",
        "        omniglot_url, zip_location\n",
        "    ))\n",
        "    urllib.request.urlretrieve(omniglot_url, zip_location)\n",
        "\n",
        "def extract() -> None:\n",
        "    zip_ref = zipfile.ZipFile(zip_location, 'r')\n",
        "    zip_ref.extractall(unzip_location)\n",
        "    zip_ref.close()\n",
        "\n",
        "def extract_images() -> None:\n",
        "    image_sets = [\"images_background.zip\", \"images_evaluation.zip\"]\n",
        "    image_sets = [os.path.join(zipped_images_location, image_set) for image_set in image_sets]\n",
        "    for image_set in image_sets:\n",
        "        zip_ref = zipfile.ZipFile(image_set, 'r')\n",
        "        zip_ref.extractall(extracted_images_location)\n",
        "        zip_ref.close()\n",
        "\n",
        "def omniglot_folder_to_NDarray(path_im):\n",
        "    alphbts = os.listdir(path_im)\n",
        "    ALL_IMGS = []\n",
        "    for alphbt in alphbts:\n",
        "        chars = os.listdir(os.path.join(path_im, alphbt))\n",
        "        for char in chars:\n",
        "            img_filenames = os.listdir(os.path.join(path_im, alphbt, char))\n",
        "            char_imgs = []\n",
        "            for img_fn in img_filenames:\n",
        "                fn = os.path.join(path_im, alphbt, char, img_fn)\n",
        "                I = imread(fn)\n",
        "                I = np.invert(I)\n",
        "                char_imgs.append(I)\n",
        "            ALL_IMGS.append(char_imgs)\n",
        "    return np.array(ALL_IMGS)\n",
        "\n",
        "def save_to_numpy() -> None:\n",
        "    image_folders = [\"images_background\", \"images_evaluation\"]\n",
        "    all_np_array = []\n",
        "    for image_folder in image_folders:\n",
        "        np_array_loc = os.path.join(data_dir, image_folder + \".npy\")\n",
        "        np_array = omniglot_folder_to_NDarray(os.path.join(extracted_images_location, image_folder))\n",
        "        np.save(np_array_loc, np_array)\n",
        "        all_np_array.append(np_array)\n",
        "\n",
        "    all_np_array = np.concatenate(all_np_array, axis=0)\n",
        "    np.save(os.path.join(\"/content/drive/MyDrive/Omniglot_ARC\", \"omniglot.npy\"), all_np_array)\n",
        "\n",
        "def main():\n",
        "    download()\n",
        "    extract()\n",
        "    extract_images()\n",
        "    save_to_numpy()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Batcher: Original Source -> https://github.com/pranv/ARC\n",
        "use_cuda = False\n",
        "\n",
        "class Omniglot(object):\n",
        "    def __init__(self, path=os.path.join('/content/drive/MyDrive/Omniglot_ARC', 'omniglot.npy'), batch_size=128, image_size=32):\n",
        "        \"\"\"\n",
        "        batch_size: the output is (2 * batch size, 1, image_size, image_size)\n",
        "                    X[i] & X[i + batch_size] are the pair\n",
        "        image_size: size of the image\n",
        "        data_split: in number of alphabets, e.g. [30, 10] means out of 50 Omniglot characters,\n",
        "                    30 is for training, 10 for validation and the remaining(10) for testing\n",
        "        within_alphabet: for verfication task, when 2 characters are sampled to form a pair,\n",
        "                        this flag specifies if should they be from the same alphabet/language\n",
        "        ---------------------\n",
        "        Data Augmentation Parameters:\n",
        "            flip: here flipping both the images in a pair\n",
        "            scale: x would scale image by + or - x%\n",
        "            rotation_deg\n",
        "            shear_deg\n",
        "            translation_px: in both x and y directions\n",
        "        \"\"\"\n",
        "        chars = np.load(path)\n",
        "\n",
        "        # resize the images\n",
        "        resized_chars = np.zeros((1623, 20, image_size, image_size), dtype='uint8')\n",
        "        for i in range(1623):\n",
        "            for j in range(20):\n",
        "                resized_chars[i, j] = np.resize(chars[i, j], (image_size, image_size)) #np added for compatability\n",
        "        chars = resized_chars\n",
        "\n",
        "        self.mean_pixel = chars.mean() / 255.0  # used later for mean subtraction\n",
        "\n",
        "        # starting index of each alphabet in a list of chars\n",
        "        a_start = [0, 20, 49, 75, 116, 156, 180, 226, 240, 266, 300, 333, 355, 381,\n",
        "                   424, 448, 496, 518, 534, 586, 633, 673, 699, 739, 780, 813,\n",
        "                   827, 869, 892, 909, 964, 984, 1010, 1036, 1062, 1088, 1114,\n",
        "                   1159, 1204, 1245, 1271, 1318, 1358, 1388, 1433, 1479, 1507,\n",
        "                   1530, 1555, 1597]\n",
        "\n",
        "        # size of each alphabet (num of chars)\n",
        "        a_size = [20, 29, 26, 41, 40, 24, 46, 14, 26, 34, 33, 22, 26, 43, 24, 48, 22,\n",
        "                  16, 52, 47, 40, 26, 40, 41, 33, 14, 42, 23, 17, 55, 20, 26, 26, 26,\n",
        "                  26, 26, 45, 45, 41, 26, 47, 40, 30, 45, 46, 28, 23, 25, 42, 26]\n",
        "\n",
        "        # each alphabet/language has different number of characters.\n",
        "        # in order to uniformly sample all characters, we need weigh the probability\n",
        "        # of sampling a alphabet by its size. p is that probability\n",
        "        def size2p(size):\n",
        "            s = np.array(size).astype('float64')\n",
        "            return s / s.sum()\n",
        "\n",
        "        self.size2p = size2p\n",
        "        self.data = chars\n",
        "        self.a_start = a_start\n",
        "        self.a_size = a_size\n",
        "        self.image_size = image_size\n",
        "        self.batch_size = batch_size\n",
        "        flip = True\n",
        "        scale = 0.2\n",
        "        rotation_deg = 20\n",
        "        shear_deg = 10\n",
        "        translation_px = 5\n",
        "        #self.augmentor = ImageAugmenter(image_size, image_size,\n",
        "        #                                hflip=flip, vflip=flip,\n",
        "        #                                scale_to_percent=1.0 + scale, rotation_deg=rotation_deg, shear_deg=shear_deg,\n",
        "        #                                translation_x_px=translation_px, translation_y_px=translation_px)\n",
        "\n",
        "    def fetch_batch(self, part):\n",
        "        \"\"\"\n",
        "            This outputs batch_size number of pairs\n",
        "            Thus the actual number of images outputted is 2 * batch_size\n",
        "            Say A & B form the half of a pair\n",
        "            The Batch is divided into 4 parts:\n",
        "                Dissimilar A \t\tDissimilar B\n",
        "                Similar A \t\t\tSimilar B\n",
        "\n",
        "            Corresponding images in Similar A and Similar B form the similar pair\n",
        "            similarly, Dissimilar A and Dissimilar B form the dissimilar pair\n",
        "\n",
        "            When flattened, the batch has 4 parts with indices:\n",
        "                Dissimilar A \t\t0 - batch_size / 2\n",
        "                Similar A    \t\tbatch_size / 2  - batch_size\n",
        "                Dissimilar B \t\tbatch_size  - 3 * batch_size / 2\n",
        "                Similar B \t\t\t3 * batch_size / 2 - batch_size\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "class Batcher(Omniglot):\n",
        "    def __init__(self, path=os.path.join('/content/drive/MyDrive/Omniglot_ARC', 'omniglot.npy'), batch_size=128, image_size=32):\n",
        "        Omniglot.__init__(self, path, batch_size, image_size)\n",
        "\n",
        "        a_start = self.a_start\n",
        "        a_size = self.a_size\n",
        "\n",
        "        # slicing indices for splitting a_start & a_size\n",
        "        i = 20\n",
        "        j = 30\n",
        "        starts = {}\n",
        "        starts['train'], starts['val'], starts['test'] = a_start[:i], a_start[i:j], a_start[j:]\n",
        "        sizes = {}\n",
        "        sizes['train'], sizes['val'], sizes['test'] = a_size[:i], a_size[i:j], a_size[j:]\n",
        "        size2p = self.size2p\n",
        "        p = {}\n",
        "        p['train'], p['val'], p['test'] = size2p(sizes['train']), size2p(sizes['val']), size2p(sizes['test'])\n",
        "        self.starts = starts\n",
        "        self.sizes = sizes\n",
        "        self.p = p\n",
        "\n",
        "    def fetch_batch(self, part, batch_size: int = None):\n",
        "\n",
        "        if batch_size is None:\n",
        "            batch_size = self.batch_size\n",
        "\n",
        "        X, Y = self._fetch_batch(part, batch_size)\n",
        "        X = Variable(torch.from_numpy(X)).view(2*batch_size, self.image_size, self.image_size)\n",
        "        X1 = X[:batch_size]  # (B, h, w)\n",
        "        X2 = X[batch_size:]  # (B, h, w)\n",
        "        X = torch.stack([X1, X2], dim=1)  # (B, 2, h, w)\n",
        "        Y = Variable(torch.from_numpy(Y))\n",
        "\n",
        "        if use_cuda:\n",
        "            X, Y = X.cuda(), Y.cuda()\n",
        "\n",
        "        return X, Y\n",
        "\n",
        "    def _fetch_batch(self, part, batch_size: int = None):\n",
        "        if batch_size is None:\n",
        "            batch_size = self.batch_size\n",
        "\n",
        "        data = self.data\n",
        "        starts = self.starts[part]\n",
        "        sizes = self.sizes[part]\n",
        "        p = self.p[part]\n",
        "        image_size = self.image_size\n",
        "        num_alphbts = len(starts)\n",
        "        X = np.zeros((2 * batch_size, image_size, image_size), dtype='uint8')\n",
        "        for i in range(batch_size // 2):\n",
        "            # choose similar chars\n",
        "            same_idx = choice(range(starts[0], starts[-1] + sizes[-1]))\n",
        "\n",
        "            # choose dissimilar chars within alphabet\n",
        "            alphbt_idx = choice(num_alphbts, p=p)\n",
        "            char_offset = choice(sizes[alphbt_idx], 2, replace=False)\n",
        "            diff_idx = starts[alphbt_idx] + char_offset\n",
        "            X[i], X[i + batch_size] = data[diff_idx, choice(20, 2)]\n",
        "            X[i + batch_size // 2], X[i + 3 * batch_size // 2] = data[same_idx, choice(20, 2, replace=False)]\n",
        "\n",
        "        y = np.zeros((batch_size, 1), dtype='int32')\n",
        "        y[:batch_size // 2] = 0\n",
        "        y[batch_size // 2:] = 1\n",
        "\n",
        "        if part == 'train':\n",
        "            #X = self.augmentor.augment_batch(X) -> These two lines need to be removed for compatibility to work without ImageAugmenter\n",
        "        #else:\n",
        "            X = X / 255.0\n",
        "\n",
        "        X = X - self.mean_pixel\n",
        "        X = X[:, np.newaxis]\n",
        "        X = X.astype(\"float32\")\n",
        "\n",
        "        return X, y\n"
      ],
      "metadata": {
        "id": "y4Rtd_2wEMzV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model\n",
        "use_cuda = False\n",
        "\n",
        "class GlimpseWindow:\n",
        "    \"\"\"\n",
        "    Generates glimpses from images using Cauchy kernels.\n",
        "    Args:\n",
        "        glimpse_h (int): The height of the glimpses to be generated.\n",
        "        glimpse_w (int): The width of the glimpses to be generated.\n",
        "    \"\"\"\n",
        "    def __init__(self, glimpse_h: int, glimpse_w: int):\n",
        "        self.glimpse_h = glimpse_h\n",
        "        self.glimpse_w = glimpse_w\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_filterbanks(delta_caps: Variable, center_caps: Variable, image_size: int, glimpse_size: int) -> Variable:\n",
        "        \"\"\"\n",
        "        Generates Cauchy Filter Banks along a dimension.\n",
        "        Args:\n",
        "            delta_caps (B,):  A batch of deltas [-1, 1]\n",
        "            center_caps (B,): A batch of [-1, 1] reals that dictate the location of center of cauchy kernel glimpse.\n",
        "            image_size (int): size of images along that dimension\n",
        "            glimpse_size (int): size of glimpses to be generated along that dimension\n",
        "        Returns:\n",
        "            (B, image_size, glimpse_size): A batch of filter banks\n",
        "        \"\"\"\n",
        "        # convert dimension sizes to float. lots of math ahead.\n",
        "        image_size = float(image_size)\n",
        "        glimpse_size = float(glimpse_size)\n",
        "\n",
        "        # scale the centers and the deltas to map to the actual size of given image.\n",
        "        centers = (image_size - 1) * (center_caps + 1) / 2.0  # (B)\n",
        "        deltas = (float(image_size) / glimpse_size) * (1.0 - torch.abs(delta_caps))\n",
        "\n",
        "        # calculate gamma for cauchy kernel\n",
        "        gammas = torch.exp(1.0 - 2 * torch.abs(delta_caps))  # (B)\n",
        "\n",
        "        # coordinate of pixels on the glimpse\n",
        "        glimpse_pixels = Variable(torch.arange(0, glimpse_size) - (glimpse_size - 1.0) / 2.0)  # (glimpse_size)\n",
        "        if use_cuda:\n",
        "            glimpse_pixels = glimpse_pixels.cuda()\n",
        "\n",
        "        # space out with delta\n",
        "        glimpse_pixels = deltas[:, None] * glimpse_pixels[None, :]  # (B, glimpse_size)\n",
        "        # center around the centers\n",
        "        glimpse_pixels = centers[:, None] + glimpse_pixels  # (B, glimpse_size)\n",
        "\n",
        "        # coordinates of pixels on the image\n",
        "        image_pixels = Variable(torch.arange(0, image_size))  # (image_size)\n",
        "        if use_cuda:\n",
        "            image_pixels = image_pixels.cuda()\n",
        "\n",
        "        fx = image_pixels - glimpse_pixels[:, :, None]  # (B, glimpse_size, image_size)\n",
        "        fx = fx / gammas[:, None, None]\n",
        "        fx = fx ** 2.0\n",
        "        fx = 1.0 + fx\n",
        "        fx = math.pi * gammas[:, None, None] * fx\n",
        "        fx = 1.0 / fx\n",
        "        fx = fx / (torch.sum(fx, dim=2) + 1e-4)[:, :, None]  # we add a small constant in the denominator division by 0.\n",
        "\n",
        "        return fx.transpose(1, 2)\n",
        "\n",
        "    def get_attention_mask(self, glimpse_params: Variable, mask_h: int, mask_w: int) -> Variable:\n",
        "        \"\"\"\n",
        "        For visualization, generate a heat map (or mask) of which pixels got the most \"attention\".\n",
        "        Args:\n",
        "            glimpse_params (B, hx):  A batch of glimpse parameters.\n",
        "            mask_h (int): The height of the image for which the mask is being generated.\n",
        "            mask_w (int): The width of the image for which the mask is being generated.\n",
        "        Returns:\n",
        "            (B, mask_h, mask_w): A batch of masks with attended pixels weighted more.\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size, _ = glimpse_params.size()\n",
        "\n",
        "        # (B, image_h, glimpse_h)\n",
        "        F_h = self._get_filterbanks(delta_caps=glimpse_params[:, 2], center_caps=glimpse_params[:, 0],\n",
        "                                    image_size=mask_h, glimpse_size=self.glimpse_h)\n",
        "\n",
        "        # (B, image_w, glimpse_w)\n",
        "        F_w = self._get_filterbanks(delta_caps=glimpse_params[:, 2], center_caps=glimpse_params[:, 1],\n",
        "                                    image_size=mask_w, glimpse_size=self.glimpse_w)\n",
        "\n",
        "        # (B, glimpse_h, glimpse_w)\n",
        "        glimpse_proxy = Variable(torch.ones(batch_size, self.glimpse_h, self.glimpse_w))\n",
        "\n",
        "        # find the attention mask that lead to the glimpse.\n",
        "        mask = glimpse_proxy\n",
        "        mask = torch.bmm(F_h, mask)\n",
        "        mask = torch.bmm(mask, F_w.transpose(1, 2))\n",
        "\n",
        "        # scale to between 0 and 1.0\n",
        "        mask = mask - mask.min()\n",
        "        mask = mask / mask.max()\n",
        "        mask = mask.float()\n",
        "\n",
        "        return mask\n",
        "\n",
        "    def get_glimpse(self, images: Variable, glimpse_params: Variable) -> Variable:\n",
        "        \"\"\"\n",
        "        Generate glimpses given images and glimpse parameters. This is the main method of this class.\n",
        "        The glimpse parameters are (h_center, w_center, delta). (h_center, w_center)\n",
        "        represents the relative position of the center of the glimpse on the image. delta determines\n",
        "        the zoom factor of the glimpse.\n",
        "        Args:\n",
        "            images (B, h, w):  A batch of images\n",
        "            glimpse_params (B, 3):  A batch of glimpse parameters (h_center, w_center, delta)\n",
        "        Returns:\n",
        "            (B, glimpse_h, glimpse_w): A batch of glimpses.\n",
        "        \"\"\"\n",
        "        batch_size, image_h, image_w = images.size()\n",
        "\n",
        "        # (B, image_h, glimpse_h)\n",
        "        F_h = self._get_filterbanks(delta_caps=glimpse_params[:, 2], center_caps=glimpse_params[:, 0],\n",
        "                                    image_size=image_h, glimpse_size=self.glimpse_h)\n",
        "\n",
        "        # (B, image_w, glimpse_w)\n",
        "        F_w = self._get_filterbanks(delta_caps=glimpse_params[:, 2], center_caps=glimpse_params[:, 1],\n",
        "                                    image_size=image_w, glimpse_size=self.glimpse_w)\n",
        "\n",
        "        # F_h.T * images * F_w\n",
        "        glimpses = images\n",
        "        glimpses = torch.bmm(F_h.transpose(1, 2), glimpses)\n",
        "        glimpses = torch.bmm(glimpses, F_w)\n",
        "\n",
        "        return glimpses  # (B, glimpse_h, glimpse_w)\n",
        "\n",
        "class ARC(nn.Module):\n",
        "    \"\"\"\n",
        "    This class implements the Attentive Recurrent Comparators. This module has two main parts.\n",
        "    1.) controller: The RNN module that takes as input glimpses from a pair of images and emits a hidden state.\n",
        "    2.) glimpser: A Linear layer that takes the hidden state emitted by the controller and generates the glimpse\n",
        "                    parameters. These glimpse parameters are (h_center, w_center, delta). (h_center, w_center)\n",
        "                    represents the relative position of the center of the glimpse on the image. delta determines\n",
        "                    the zoom factor of the glimpse.\n",
        "    Args:\n",
        "        num_glimpses (int): How many glimpses must the ARC \"see\" before emitting the final hidden state.\n",
        "        glimpse_h (int): The height of the glimpse in pixels.\n",
        "        glimpse_w (int): The width of the glimpse in pixels.\n",
        "        controller_out (int): The size of the hidden state emitted by the controller.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_glimpses: int=8, glimpse_h: int=8, glimpse_w: int=8, controller_out: int=128) -> None:\n",
        "        super().__init__()\n",
        "        self.num_glimpses = num_glimpses\n",
        "        self.glimpse_h = glimpse_h\n",
        "        self.glimpse_w = glimpse_w\n",
        "        self.controller_out = controller_out\n",
        "\n",
        "        # main modules of ARC\n",
        "        self.controller = nn.LSTMCell(input_size=(glimpse_h * glimpse_w), hidden_size=self.controller_out)\n",
        "        self.glimpser = nn.Linear(in_features=self.controller_out, out_features=3)\n",
        "\n",
        "        # this will actually generate glimpses from images using the glimpse parameters.\n",
        "        self.glimpse_window = GlimpseWindow(glimpse_h=self.glimpse_h, glimpse_w=self.glimpse_w)\n",
        "\n",
        "    def forward(self, image_pairs: Variable) -> Variable:\n",
        "        \"\"\"\n",
        "        The method calls the internal _forward() method which returns hidden states for all time steps. This i\n",
        "        Args:\n",
        "            image_pairs (B, 2, h, w):  A batch of pairs of images\n",
        "        Returns:\n",
        "            (B, controller_out): A batch of final hidden states after each pair of image has been shown for num_glimpses\n",
        "            glimpses.\n",
        "        \"\"\"\n",
        "        # return only the last hidden state\n",
        "        all_hidden = self._forward(image_pairs)  # (2*num_glimpses, B, controller_out)\n",
        "        last_hidden = all_hidden[-1, :, :]  # (B, controller_out)\n",
        "\n",
        "        return last_hidden\n",
        "\n",
        "    def _forward(self, image_pairs: Variable) -> Variable:\n",
        "        \"\"\"\n",
        "        The main forward method of ARC. But it returns hidden state from all time steps (all glimpses) as opposed to\n",
        "        just the last one. See the exposed forward() method.\n",
        "        Args:\n",
        "            image_pairs: (B, 2, h, w) A batch of pairs of images\n",
        "        Returns:\n",
        "            (2*num_glimpses, B, controller_out) Hidden states from ALL time steps.\n",
        "        \"\"\"\n",
        "        # convert to images to float.\n",
        "        image_pairs = image_pairs.float()\n",
        "\n",
        "        # calculate the batch size\n",
        "        batch_size = image_pairs.size()[0]\n",
        "\n",
        "        # an array for collecting hidden states from each time step.\n",
        "        all_hidden = []\n",
        "\n",
        "        # initial hidden state of the LSTM.\n",
        "        Hx = Variable(torch.zeros(batch_size, self.controller_out))  # (B, controller_out)\n",
        "        Cx = Variable(torch.zeros(batch_size, self.controller_out))  # (B, controller_out)\n",
        "\n",
        "        if use_cuda:\n",
        "            Hx, Cx = Hx.cuda(), Cx.cuda()\n",
        "\n",
        "        # take `num_glimpses` glimpses for both images, alternatingly.\n",
        "        for turn in range(2*self.num_glimpses):\n",
        "            # select image to show, alternate between the first and second image in the pair\n",
        "            images_to_observe = image_pairs[:,  turn % 2]  # (B, h, w)\n",
        "\n",
        "            # choose a portion from image to glimpse using attention\n",
        "            glimpse_params = torch.tanh(self.glimpser(Hx))  # (B, 3)  a batch of glimpse params (x, y, delta)\n",
        "            glimpses = self.glimpse_window.get_glimpse(images_to_observe, glimpse_params)  # (B, glimpse_h, glimpse_w)\n",
        "            flattened_glimpses = glimpses.view(batch_size, -1)  # (B, glimpse_h * glimpse_w), one time-step\n",
        "\n",
        "            # feed the glimpses and the previous hidden state to the LSTM.\n",
        "            Hx, Cx = self.controller(flattened_glimpses, (Hx, Cx))  # (B, controller_out), (B, controller_out)\n",
        "\n",
        "            # append this hidden state to all states\n",
        "            all_hidden.append(Hx)\n",
        "\n",
        "        all_hidden = torch.stack(all_hidden)  # (2*num_glimpses, B, controller_out)\n",
        "\n",
        "        # return a batch of all hidden states.\n",
        "        return all_hidden\n",
        "\n",
        "class ArcBinaryClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    A binary classifier that uses ARC.\n",
        "    Given a pair of images, feeds them the ARC and uses the final hidden state of ARC to\n",
        "    classify the images as belonging to the same class or not.\n",
        "    Args:\n",
        "        num_glimpses (int): How many glimpses must the ARC \"see\" before emitting the final hidden state.\n",
        "        glimpse_h (int): The height of the glimpse in pixels.\n",
        "        glimpse_w (int): The width of the glimpse in pixels.\n",
        "        controller_out (int): The size of the hidden state emitted by the controller.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_glimpses: int=8, glimpse_h: int=8, glimpse_w: int=8, controller_out: int = 128):\n",
        "        super().__init__()\n",
        "        self.arc = ARC(\n",
        "            num_glimpses=num_glimpses,\n",
        "            glimpse_h=glimpse_h,\n",
        "            glimpse_w=glimpse_w,\n",
        "            controller_out=controller_out)\n",
        "\n",
        "        # two dense layers, which take the hidden state from the controller of ARC and\n",
        "        # classify the images as belonging to the same class or not.\n",
        "        self.dense1 = nn.Linear(controller_out, 64)\n",
        "        self.dense2 = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, image_pairs: Variable) -> Variable:\n",
        "        arc_out = self.arc(image_pairs)\n",
        "\n",
        "        d1 = F.elu(self.dense1(arc_out))\n",
        "        decision = torch.sigmoid(self.dense2(d1))\n",
        "\n",
        "        return decision\n",
        "\n",
        "    def save_to_file(self, file_path: str) -> None:\n",
        "        torch.save(self.state_dict(), file_path)"
      ],
      "metadata": {
        "id": "ESAiij2DD-ra"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('-f') #neccessary null argument for colab compatibility\n",
        "parser.add_argument('--batchSize', type=int, default=128, help='input batch size')\n",
        "parser.add_argument('--imageSize', type=int, default=32, help='the height / width of the input image to ARC')\n",
        "parser.add_argument('--glimpseSize', type=int, default=8, help='the height / width of glimpse seen by ARC')\n",
        "parser.add_argument('--numStates', type=int, default=128, help='number of hidden states in ARC controller')\n",
        "parser.add_argument('--numGlimpses', type=int, default=6, help='the number glimpses of each image in pair seen by ARC')\n",
        "parser.add_argument('--lr', type=float, default=0.0002, help='learning rate, default=0.0002')\n",
        "parser.add_argument('--cuda', action='store_true', help='enables cuda')\n",
        "parser.add_argument('--name', default=None, help='Custom name for this configuration. Needed for saving'\n",
        "                                                 ' model checkpoints in a separate folder.')\n",
        "parser.add_argument('--load', default=None, help='the model to load from. Start fresh if not specified.')\n",
        "\n",
        "def get_pct_accuracy(pred: Variable, target) -> int:\n",
        "    hard_pred = (pred > 0.5).int()\n",
        "    correct = (hard_pred == target).sum().data#[0]\n",
        "    accuracy = float(correct) / target.size()[0]\n",
        "    accuracy = int(accuracy * 100)\n",
        "    return accuracy\n",
        "\n",
        "def train():\n",
        "    opt = parser.parse_args()\n",
        "\n",
        "    if opt.cuda:\n",
        "        batcher.use_cuda = True\n",
        "        models.use_cuda = True\n",
        "\n",
        "    if opt.name is None:\n",
        "        # if no name is given, we generate a name from the parameters.\n",
        "        # only those parameters are taken, which if changed break torch.load compatibility.\n",
        "        opt.name = \"{}_{}_{}_{}\".format(opt.numGlimpses, opt.glimpseSize, opt.numStates,\n",
        "                                        \"cuda\" if opt.cuda else \"cpu\")\n",
        "        \n",
        "    # make directory for storing models.\n",
        "    models_path = os.path.join(\"saved_models\", opt.name)\n",
        "    os.makedirs(models_path, exist_ok=True)\n",
        "\n",
        "    # initialise the model\n",
        "    discriminator = ArcBinaryClassifier(num_glimpses=opt.numGlimpses,\n",
        "                                        glimpse_h=opt.glimpseSize,\n",
        "                                        glimpse_w=opt.glimpseSize,\n",
        "                                        controller_out=opt.numStates)\n",
        "\n",
        "    if opt.cuda:\n",
        "        discriminator.cuda()\n",
        "\n",
        "    # load from a previous checkpoint, if specified.\n",
        "    if opt.load is not None:\n",
        "        discriminator.load_state_dict(torch.load(os.path.join(models_path, opt.load)))\n",
        "\n",
        "    # set up the optimizer.\n",
        "    bce = torch.nn.BCELoss()\n",
        "    if opt.cuda:\n",
        "        bce = bce.cuda()\n",
        "\n",
        "    optimizer = torch.optim.Adam(params=discriminator.parameters(), lr=opt.lr)\n",
        "\n",
        "    # load the dataset in memory.\n",
        "    loader = Batcher(batch_size=opt.batchSize, image_size=opt.imageSize)\n",
        "\n",
        "    # ready to train ...\n",
        "    best_validation_loss = None\n",
        "    saving_threshold = 1.02\n",
        "    last_saved = datetime.utcnow()\n",
        "    save_every = timedelta(minutes=10)\n",
        "\n",
        "    i = -1\n",
        "    while True:\n",
        "        i += 1\n",
        "        X, Y = loader.fetch_batch(\"train\")\n",
        "        pred = discriminator(X)\n",
        "        loss = bce(pred, Y.float())\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            # validate your model\n",
        "            X_val, Y_val = loader.fetch_batch(\"val\")\n",
        "            pred_val = discriminator(X_val)\n",
        "            loss_val = bce(pred_val, Y_val.float())\n",
        "\n",
        "            training_loss = loss.data#[0]\n",
        "            validation_loss = loss_val.data#[0]\n",
        "\n",
        "            print(\"Iteration: {} \\t Train: Acc={}%, Loss={} \\t\\t Validation: Acc={}%, Loss={}\".format(\n",
        "                i, get_pct_accuracy(pred, Y), training_loss, get_pct_accuracy(pred_val, Y_val), validation_loss\n",
        "            ))\n",
        "\n",
        "            if best_validation_loss is None:\n",
        "                best_validation_loss = validation_loss\n",
        "\n",
        "            if best_validation_loss > (saving_threshold * validation_loss):\n",
        "                print(\"Significantly improved validation loss from {} --> {}. Saving...\".format(\n",
        "                    best_validation_loss, validation_loss\n",
        "                ))\n",
        "                discriminator.save_to_file(os.path.join(models_path, str(validation_loss)))\n",
        "                best_validation_loss = validation_loss\n",
        "                last_saved = datetime.utcnow()\n",
        "\n",
        "            if last_saved + save_every < datetime.utcnow():\n",
        "                print(\"It's been too long since we last saved the model. Saving...\")\n",
        "                discriminator.save_to_file(os.path.join(models_path, str(validation_loss)))\n",
        "                last_saved = datetime.utcnow()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "def main() -> None:\n",
        "    train()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KPNmyUcoDodI",
        "outputId": "8ea97e57-6057-44eb-85b8-4cecb0f7fd95"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Iteration: 317290 \t Train: Acc=50%, Loss=0.6966460347175598 \t\t Validation: Acc=53%, Loss=0.6989519596099854\n",
            "Iteration: 317300 \t Train: Acc=54%, Loss=0.6886439919471741 \t\t Validation: Acc=49%, Loss=0.6976310014724731\n",
            "Iteration: 317310 \t Train: Acc=49%, Loss=0.6944337487220764 \t\t Validation: Acc=47%, Loss=0.692829966545105\n",
            "Iteration: 317320 \t Train: Acc=52%, Loss=0.6907536387443542 \t\t Validation: Acc=48%, Loss=0.6939067244529724\n",
            "Iteration: 317330 \t Train: Acc=53%, Loss=0.6915247440338135 \t\t Validation: Acc=55%, Loss=0.6833809614181519\n",
            "Iteration: 317340 \t Train: Acc=50%, Loss=0.6962454915046692 \t\t Validation: Acc=52%, Loss=0.7046203017234802\n",
            "Iteration: 317350 \t Train: Acc=49%, Loss=0.6926323175430298 \t\t Validation: Acc=49%, Loss=0.6952314972877502\n",
            "Iteration: 317360 \t Train: Acc=46%, Loss=0.7011111378669739 \t\t Validation: Acc=50%, Loss=0.706631600856781\n",
            "Iteration: 317370 \t Train: Acc=51%, Loss=0.6977710127830505 \t\t Validation: Acc=53%, Loss=0.6866001486778259\n",
            "Iteration: 317380 \t Train: Acc=53%, Loss=0.6932029128074646 \t\t Validation: Acc=53%, Loss=0.680776059627533\n",
            "Iteration: 317390 \t Train: Acc=50%, Loss=0.6967084407806396 \t\t Validation: Acc=56%, Loss=0.6863075494766235\n",
            "Iteration: 317400 \t Train: Acc=52%, Loss=0.6914525032043457 \t\t Validation: Acc=47%, Loss=0.691836953163147\n",
            "Iteration: 317410 \t Train: Acc=56%, Loss=0.6889247298240662 \t\t Validation: Acc=48%, Loss=0.7046241760253906\n",
            "Iteration: 317420 \t Train: Acc=48%, Loss=0.6959277987480164 \t\t Validation: Acc=52%, Loss=0.6863650679588318\n",
            "Iteration: 317430 \t Train: Acc=53%, Loss=0.6903719305992126 \t\t Validation: Acc=50%, Loss=0.7020272016525269\n",
            "Iteration: 317440 \t Train: Acc=51%, Loss=0.6928368806838989 \t\t Validation: Acc=46%, Loss=0.6958231925964355\n",
            "Iteration: 317450 \t Train: Acc=48%, Loss=0.6944261193275452 \t\t Validation: Acc=52%, Loss=0.6968977451324463\n",
            "Iteration: 317460 \t Train: Acc=43%, Loss=0.6964839696884155 \t\t Validation: Acc=53%, Loss=0.7004841566085815\n",
            "Iteration: 317470 \t Train: Acc=58%, Loss=0.6807196140289307 \t\t Validation: Acc=48%, Loss=0.6977248191833496\n",
            "Iteration: 317480 \t Train: Acc=54%, Loss=0.6898905634880066 \t\t Validation: Acc=51%, Loss=0.6887479424476624\n",
            "Iteration: 317490 \t Train: Acc=53%, Loss=0.6847368478775024 \t\t Validation: Acc=46%, Loss=0.7049396634101868\n",
            "Iteration: 317500 \t Train: Acc=46%, Loss=0.6920739412307739 \t\t Validation: Acc=49%, Loss=0.6978543400764465\n",
            "Iteration: 317510 \t Train: Acc=53%, Loss=0.6953662037849426 \t\t Validation: Acc=49%, Loss=0.6966480016708374\n",
            "Iteration: 317520 \t Train: Acc=47%, Loss=0.6958779692649841 \t\t Validation: Acc=50%, Loss=0.693748950958252\n",
            "Iteration: 317530 \t Train: Acc=50%, Loss=0.6873173713684082 \t\t Validation: Acc=52%, Loss=0.6912966966629028\n",
            "Iteration: 317540 \t Train: Acc=53%, Loss=0.6912402510643005 \t\t Validation: Acc=55%, Loss=0.6833196878433228\n",
            "Iteration: 317550 \t Train: Acc=51%, Loss=0.6888542175292969 \t\t Validation: Acc=53%, Loss=0.6892998218536377\n",
            "Iteration: 317560 \t Train: Acc=47%, Loss=0.6950684189796448 \t\t Validation: Acc=47%, Loss=0.7000958919525146\n",
            "Iteration: 317570 \t Train: Acc=49%, Loss=0.6945673823356628 \t\t Validation: Acc=50%, Loss=0.6913827061653137\n",
            "Iteration: 317580 \t Train: Acc=50%, Loss=0.692926824092865 \t\t Validation: Acc=51%, Loss=0.6969991326332092\n",
            "Iteration: 317590 \t Train: Acc=51%, Loss=0.6888783574104309 \t\t Validation: Acc=46%, Loss=0.68465256690979\n",
            "Iteration: 317600 \t Train: Acc=54%, Loss=0.6868066787719727 \t\t Validation: Acc=53%, Loss=0.6959691047668457\n",
            "Iteration: 317610 \t Train: Acc=51%, Loss=0.693817138671875 \t\t Validation: Acc=50%, Loss=0.6862943172454834\n",
            "Iteration: 317620 \t Train: Acc=50%, Loss=0.6912110447883606 \t\t Validation: Acc=50%, Loss=0.690484881401062\n",
            "Iteration: 317630 \t Train: Acc=53%, Loss=0.6913560032844543 \t\t Validation: Acc=48%, Loss=0.699155867099762\n",
            "Iteration: 317640 \t Train: Acc=50%, Loss=0.6918022632598877 \t\t Validation: Acc=53%, Loss=0.6872049570083618\n",
            "Iteration: 317650 \t Train: Acc=42%, Loss=0.6943749785423279 \t\t Validation: Acc=53%, Loss=0.6918729543685913\n",
            "Iteration: 317660 \t Train: Acc=54%, Loss=0.6919916868209839 \t\t Validation: Acc=53%, Loss=0.701298177242279\n",
            "Iteration: 317670 \t Train: Acc=52%, Loss=0.6934801936149597 \t\t Validation: Acc=49%, Loss=0.6867111325263977\n",
            "Iteration: 317680 \t Train: Acc=52%, Loss=0.6900524497032166 \t\t Validation: Acc=46%, Loss=0.6975679397583008\n",
            "Iteration: 317690 \t Train: Acc=50%, Loss=0.6927996873855591 \t\t Validation: Acc=47%, Loss=0.7044137716293335\n",
            "Iteration: 317700 \t Train: Acc=52%, Loss=0.6997999548912048 \t\t Validation: Acc=50%, Loss=0.6948937177658081\n",
            "Iteration: 317710 \t Train: Acc=57%, Loss=0.6835750341415405 \t\t Validation: Acc=53%, Loss=0.696220874786377\n",
            "Iteration: 317720 \t Train: Acc=51%, Loss=0.6920964121818542 \t\t Validation: Acc=53%, Loss=0.702659010887146\n",
            "Iteration: 317730 \t Train: Acc=43%, Loss=0.6896527409553528 \t\t Validation: Acc=49%, Loss=0.7053068280220032\n",
            "Iteration: 317740 \t Train: Acc=52%, Loss=0.692017674446106 \t\t Validation: Acc=53%, Loss=0.6837725639343262\n",
            "Iteration: 317750 \t Train: Acc=58%, Loss=0.6905527710914612 \t\t Validation: Acc=55%, Loss=0.6838366985321045\n",
            "Iteration: 317760 \t Train: Acc=50%, Loss=0.692794680595398 \t\t Validation: Acc=52%, Loss=0.6959967017173767\n",
            "Iteration: 317770 \t Train: Acc=54%, Loss=0.6782469153404236 \t\t Validation: Acc=52%, Loss=0.6978696584701538\n",
            "Iteration: 317780 \t Train: Acc=50%, Loss=0.6927517056465149 \t\t Validation: Acc=51%, Loss=0.6956861615180969\n",
            "Iteration: 317790 \t Train: Acc=46%, Loss=0.6980147361755371 \t\t Validation: Acc=51%, Loss=0.7057116031646729\n",
            "Iteration: 317800 \t Train: Acc=52%, Loss=0.6936670541763306 \t\t Validation: Acc=51%, Loss=0.6894846558570862\n",
            "Iteration: 317810 \t Train: Acc=58%, Loss=0.6874451637268066 \t\t Validation: Acc=50%, Loss=0.6877705454826355\n",
            "Iteration: 317820 \t Train: Acc=43%, Loss=0.6996028423309326 \t\t Validation: Acc=47%, Loss=0.7087789177894592\n",
            "Iteration: 317830 \t Train: Acc=48%, Loss=0.698833167552948 \t\t Validation: Acc=52%, Loss=0.6913936734199524\n",
            "Iteration: 317840 \t Train: Acc=50%, Loss=0.6932409405708313 \t\t Validation: Acc=50%, Loss=0.7027220129966736\n",
            "Iteration: 317850 \t Train: Acc=54%, Loss=0.6893489956855774 \t\t Validation: Acc=52%, Loss=0.6759567260742188\n",
            "Iteration: 317860 \t Train: Acc=47%, Loss=0.6955589056015015 \t\t Validation: Acc=50%, Loss=0.6810389757156372\n",
            "Iteration: 317870 \t Train: Acc=45%, Loss=0.6970238089561462 \t\t Validation: Acc=53%, Loss=0.695758581161499\n",
            "Iteration: 317880 \t Train: Acc=50%, Loss=0.6879498362541199 \t\t Validation: Acc=48%, Loss=0.7091981768608093\n",
            "Iteration: 317890 \t Train: Acc=52%, Loss=0.687633752822876 \t\t Validation: Acc=48%, Loss=0.6964586973190308\n",
            "Iteration: 317900 \t Train: Acc=54%, Loss=0.6921442151069641 \t\t Validation: Acc=51%, Loss=0.6979422569274902\n",
            "Iteration: 317910 \t Train: Acc=52%, Loss=0.6963215470314026 \t\t Validation: Acc=50%, Loss=0.7182923555374146\n",
            "Iteration: 317920 \t Train: Acc=49%, Loss=0.6842239499092102 \t\t Validation: Acc=48%, Loss=0.700711727142334\n",
            "Iteration: 317930 \t Train: Acc=45%, Loss=0.6985284686088562 \t\t Validation: Acc=52%, Loss=0.6933181881904602\n",
            "Iteration: 317940 \t Train: Acc=49%, Loss=0.6984068751335144 \t\t Validation: Acc=46%, Loss=0.6953165531158447\n",
            "Iteration: 317950 \t Train: Acc=48%, Loss=0.6949052810668945 \t\t Validation: Acc=50%, Loss=0.7008919715881348\n",
            "Iteration: 317960 \t Train: Acc=50%, Loss=0.6898731589317322 \t\t Validation: Acc=49%, Loss=0.7020961046218872\n",
            "Iteration: 317970 \t Train: Acc=53%, Loss=0.6900290250778198 \t\t Validation: Acc=48%, Loss=0.6933683753013611\n",
            "Iteration: 317980 \t Train: Acc=51%, Loss=0.6955649852752686 \t\t Validation: Acc=53%, Loss=0.6926465034484863\n",
            "Iteration: 317990 \t Train: Acc=46%, Loss=0.6970133781433105 \t\t Validation: Acc=50%, Loss=0.6896095871925354\n",
            "Iteration: 318000 \t Train: Acc=47%, Loss=0.689663290977478 \t\t Validation: Acc=53%, Loss=0.6855510473251343\n",
            "Iteration: 318010 \t Train: Acc=50%, Loss=0.6860015988349915 \t\t Validation: Acc=48%, Loss=0.7109452486038208\n",
            "Iteration: 318020 \t Train: Acc=47%, Loss=0.6970973014831543 \t\t Validation: Acc=54%, Loss=0.6910290718078613\n",
            "Iteration: 318030 \t Train: Acc=49%, Loss=0.6942204236984253 \t\t Validation: Acc=55%, Loss=0.6845368146896362\n",
            "Iteration: 318040 \t Train: Acc=50%, Loss=0.6969711184501648 \t\t Validation: Acc=50%, Loss=0.6948336958885193\n",
            "Iteration: 318050 \t Train: Acc=49%, Loss=0.6878535747528076 \t\t Validation: Acc=53%, Loss=0.6932482123374939\n",
            "Iteration: 318060 \t Train: Acc=53%, Loss=0.6907628774642944 \t\t Validation: Acc=50%, Loss=0.6831072568893433\n",
            "Iteration: 318070 \t Train: Acc=50%, Loss=0.6899182796478271 \t\t Validation: Acc=45%, Loss=0.7022038102149963\n",
            "Iteration: 318080 \t Train: Acc=47%, Loss=0.6981467008590698 \t\t Validation: Acc=49%, Loss=0.6957201361656189\n",
            "Iteration: 318090 \t Train: Acc=51%, Loss=0.6912286877632141 \t\t Validation: Acc=50%, Loss=0.6899462938308716\n",
            "Iteration: 318100 \t Train: Acc=50%, Loss=0.6926602125167847 \t\t Validation: Acc=50%, Loss=0.6856077909469604\n",
            "Iteration: 318110 \t Train: Acc=57%, Loss=0.6878297924995422 \t\t Validation: Acc=50%, Loss=0.6924571394920349\n",
            "Iteration: 318120 \t Train: Acc=52%, Loss=0.6833912134170532 \t\t Validation: Acc=54%, Loss=0.6954153776168823\n",
            "Iteration: 318130 \t Train: Acc=47%, Loss=0.7007545828819275 \t\t Validation: Acc=49%, Loss=0.6995949149131775\n",
            "Iteration: 318140 \t Train: Acc=49%, Loss=0.6893901824951172 \t\t Validation: Acc=53%, Loss=0.6869766712188721\n",
            "Iteration: 318150 \t Train: Acc=44%, Loss=0.7020714282989502 \t\t Validation: Acc=50%, Loss=0.6969943046569824\n",
            "Iteration: 318160 \t Train: Acc=52%, Loss=0.6948888897895813 \t\t Validation: Acc=52%, Loss=0.693764865398407\n",
            "Iteration: 318170 \t Train: Acc=54%, Loss=0.6916433572769165 \t\t Validation: Acc=53%, Loss=0.7055212259292603\n",
            "Iteration: 318180 \t Train: Acc=46%, Loss=0.6871169805526733 \t\t Validation: Acc=53%, Loss=0.6967450976371765\n",
            "Iteration: 318190 \t Train: Acc=44%, Loss=0.6951221227645874 \t\t Validation: Acc=48%, Loss=0.6917394995689392\n",
            "Iteration: 318200 \t Train: Acc=46%, Loss=0.6962115168571472 \t\t Validation: Acc=49%, Loss=0.7112188339233398\n",
            "Iteration: 318210 \t Train: Acc=53%, Loss=0.6856403946876526 \t\t Validation: Acc=46%, Loss=0.7135733366012573\n",
            "Iteration: 318220 \t Train: Acc=53%, Loss=0.692055881023407 \t\t Validation: Acc=50%, Loss=0.6954613327980042\n",
            "Iteration: 318230 \t Train: Acc=46%, Loss=0.6944798231124878 \t\t Validation: Acc=50%, Loss=0.6914535760879517\n",
            "Iteration: 318240 \t Train: Acc=50%, Loss=0.6936079859733582 \t\t Validation: Acc=52%, Loss=0.6924246549606323\n",
            "Iteration: 318250 \t Train: Acc=49%, Loss=0.6954613327980042 \t\t Validation: Acc=49%, Loss=0.6952425241470337\n",
            "Iteration: 318260 \t Train: Acc=53%, Loss=0.684054434299469 \t\t Validation: Acc=47%, Loss=0.7044195532798767\n",
            "Iteration: 318270 \t Train: Acc=46%, Loss=0.69777911901474 \t\t Validation: Acc=52%, Loss=0.702536940574646\n",
            "Iteration: 318280 \t Train: Acc=46%, Loss=0.6946878433227539 \t\t Validation: Acc=51%, Loss=0.7073254585266113\n",
            "Iteration: 318290 \t Train: Acc=53%, Loss=0.6906993389129639 \t\t Validation: Acc=50%, Loss=0.7068527936935425\n",
            "Iteration: 318300 \t Train: Acc=54%, Loss=0.6855475306510925 \t\t Validation: Acc=50%, Loss=0.6944469809532166\n",
            "Iteration: 318310 \t Train: Acc=56%, Loss=0.6886777877807617 \t\t Validation: Acc=53%, Loss=0.6981061100959778\n",
            "Iteration: 318320 \t Train: Acc=50%, Loss=0.6937084197998047 \t\t Validation: Acc=51%, Loss=0.6858367919921875\n",
            "Iteration: 318330 \t Train: Acc=52%, Loss=0.6919708847999573 \t\t Validation: Acc=48%, Loss=0.7026585340499878\n",
            "Iteration: 318340 \t Train: Acc=47%, Loss=0.692897379398346 \t\t Validation: Acc=49%, Loss=0.6828063130378723\n",
            "Iteration: 318350 \t Train: Acc=53%, Loss=0.6854348182678223 \t\t Validation: Acc=52%, Loss=0.695690393447876\n",
            "Iteration: 318360 \t Train: Acc=46%, Loss=0.6893390417098999 \t\t Validation: Acc=50%, Loss=0.7029114365577698\n",
            "Iteration: 318370 \t Train: Acc=53%, Loss=0.684496283531189 \t\t Validation: Acc=50%, Loss=0.6988239288330078\n",
            "Iteration: 318380 \t Train: Acc=51%, Loss=0.6952093839645386 \t\t Validation: Acc=47%, Loss=0.6965029239654541\n",
            "Iteration: 318390 \t Train: Acc=54%, Loss=0.6965507864952087 \t\t Validation: Acc=52%, Loss=0.6870413422584534\n",
            "Iteration: 318400 \t Train: Acc=51%, Loss=0.6914284825325012 \t\t Validation: Acc=52%, Loss=0.6894152760505676\n",
            "Iteration: 318410 \t Train: Acc=51%, Loss=0.696517825126648 \t\t Validation: Acc=53%, Loss=0.7000256776809692\n",
            "Iteration: 318420 \t Train: Acc=49%, Loss=0.6917597651481628 \t\t Validation: Acc=48%, Loss=0.688994288444519\n",
            "Iteration: 318430 \t Train: Acc=50%, Loss=0.6880514025688171 \t\t Validation: Acc=50%, Loss=0.7121928930282593\n",
            "Iteration: 318440 \t Train: Acc=50%, Loss=0.6907839179039001 \t\t Validation: Acc=52%, Loss=0.6880179643630981\n",
            "Iteration: 318450 \t Train: Acc=44%, Loss=0.694159746170044 \t\t Validation: Acc=50%, Loss=0.6998034119606018\n",
            "Iteration: 318460 \t Train: Acc=51%, Loss=0.6922177076339722 \t\t Validation: Acc=51%, Loss=0.7026897668838501\n",
            "Iteration: 318470 \t Train: Acc=56%, Loss=0.6917233467102051 \t\t Validation: Acc=49%, Loss=0.6976516246795654\n",
            "Iteration: 318480 \t Train: Acc=60%, Loss=0.6813644766807556 \t\t Validation: Acc=50%, Loss=0.6970345973968506\n",
            "Iteration: 318490 \t Train: Acc=53%, Loss=0.7028799057006836 \t\t Validation: Acc=46%, Loss=0.7083241939544678\n",
            "Iteration: 318500 \t Train: Acc=48%, Loss=0.6916971802711487 \t\t Validation: Acc=49%, Loss=0.6950948238372803\n",
            "Iteration: 318510 \t Train: Acc=50%, Loss=0.6884288191795349 \t\t Validation: Acc=49%, Loss=0.6941052079200745\n",
            "Iteration: 318520 \t Train: Acc=52%, Loss=0.6897587776184082 \t\t Validation: Acc=48%, Loss=0.7091349363327026\n",
            "Iteration: 318530 \t Train: Acc=51%, Loss=0.6919213533401489 \t\t Validation: Acc=50%, Loss=0.6902199983596802\n",
            "Iteration: 318540 \t Train: Acc=54%, Loss=0.6904900074005127 \t\t Validation: Acc=48%, Loss=0.7008734941482544\n",
            "Iteration: 318550 \t Train: Acc=52%, Loss=0.6876715421676636 \t\t Validation: Acc=51%, Loss=0.6903582215309143\n",
            "Iteration: 318560 \t Train: Acc=50%, Loss=0.700636625289917 \t\t Validation: Acc=50%, Loss=0.6967699527740479\n",
            "Iteration: 318570 \t Train: Acc=51%, Loss=0.6878297924995422 \t\t Validation: Acc=52%, Loss=0.6940572261810303\n",
            "Iteration: 318580 \t Train: Acc=50%, Loss=0.6938048601150513 \t\t Validation: Acc=47%, Loss=0.6995879411697388\n",
            "Iteration: 318590 \t Train: Acc=47%, Loss=0.6969518065452576 \t\t Validation: Acc=46%, Loss=0.6935033202171326\n",
            "Iteration: 318600 \t Train: Acc=52%, Loss=0.6825059056282043 \t\t Validation: Acc=48%, Loss=0.6953169107437134\n",
            "Iteration: 318610 \t Train: Acc=54%, Loss=0.6959171295166016 \t\t Validation: Acc=48%, Loss=0.6944723129272461\n",
            "Iteration: 318620 \t Train: Acc=51%, Loss=0.6900289058685303 \t\t Validation: Acc=52%, Loss=0.6960887312889099\n",
            "Iteration: 318630 \t Train: Acc=49%, Loss=0.6901280283927917 \t\t Validation: Acc=47%, Loss=0.7156909704208374\n",
            "Iteration: 318640 \t Train: Acc=50%, Loss=0.6963760852813721 \t\t Validation: Acc=48%, Loss=0.6923716068267822\n",
            "Iteration: 318650 \t Train: Acc=50%, Loss=0.6890594363212585 \t\t Validation: Acc=51%, Loss=0.6940644979476929\n",
            "Iteration: 318660 \t Train: Acc=49%, Loss=0.6907656192779541 \t\t Validation: Acc=48%, Loss=0.7095509767532349\n",
            "Iteration: 318670 \t Train: Acc=46%, Loss=0.6925201416015625 \t\t Validation: Acc=49%, Loss=0.6997836828231812\n",
            "Iteration: 318680 \t Train: Acc=57%, Loss=0.6814839839935303 \t\t Validation: Acc=46%, Loss=0.6876868009567261\n",
            "Iteration: 318690 \t Train: Acc=49%, Loss=0.6903979182243347 \t\t Validation: Acc=48%, Loss=0.7026263475418091\n",
            "Iteration: 318700 \t Train: Acc=53%, Loss=0.6881132125854492 \t\t Validation: Acc=46%, Loss=0.6943237781524658\n",
            "Iteration: 318710 \t Train: Acc=55%, Loss=0.6870421171188354 \t\t Validation: Acc=56%, Loss=0.6995351314544678\n",
            "Iteration: 318720 \t Train: Acc=50%, Loss=0.6958978772163391 \t\t Validation: Acc=49%, Loss=0.7096006274223328\n",
            "Iteration: 318730 \t Train: Acc=50%, Loss=0.6895731687545776 \t\t Validation: Acc=47%, Loss=0.6844555735588074\n",
            "Iteration: 318740 \t Train: Acc=50%, Loss=0.6918526887893677 \t\t Validation: Acc=53%, Loss=0.6954763531684875\n",
            "Iteration: 318750 \t Train: Acc=53%, Loss=0.6910768747329712 \t\t Validation: Acc=52%, Loss=0.7008369565010071\n",
            "Iteration: 318760 \t Train: Acc=56%, Loss=0.6900263428688049 \t\t Validation: Acc=50%, Loss=0.6912712454795837\n",
            "Iteration: 318770 \t Train: Acc=50%, Loss=0.6975035667419434 \t\t Validation: Acc=51%, Loss=0.6857692003250122\n",
            "Iteration: 318780 \t Train: Acc=54%, Loss=0.6884445548057556 \t\t Validation: Acc=42%, Loss=0.7161975502967834\n",
            "Iteration: 318790 \t Train: Acc=51%, Loss=0.6916285157203674 \t\t Validation: Acc=52%, Loss=0.6999109983444214\n",
            "Iteration: 318800 \t Train: Acc=49%, Loss=0.6891918778419495 \t\t Validation: Acc=50%, Loss=0.6969873905181885\n",
            "Iteration: 318810 \t Train: Acc=54%, Loss=0.6899850368499756 \t\t Validation: Acc=48%, Loss=0.6923843026161194\n",
            "Iteration: 318820 \t Train: Acc=50%, Loss=0.6881988048553467 \t\t Validation: Acc=51%, Loss=0.6929616928100586\n",
            "Iteration: 318830 \t Train: Acc=51%, Loss=0.6974276900291443 \t\t Validation: Acc=47%, Loss=0.7027075886726379\n",
            "Iteration: 318840 \t Train: Acc=53%, Loss=0.6935209631919861 \t\t Validation: Acc=48%, Loss=0.6831353902816772\n",
            "Iteration: 318850 \t Train: Acc=49%, Loss=0.6913765668869019 \t\t Validation: Acc=49%, Loss=0.6883224844932556\n",
            "Iteration: 318860 \t Train: Acc=56%, Loss=0.686083197593689 \t\t Validation: Acc=52%, Loss=0.6921755075454712\n",
            "Iteration: 318870 \t Train: Acc=51%, Loss=0.7040905356407166 \t\t Validation: Acc=52%, Loss=0.6965158581733704\n",
            "Iteration: 318880 \t Train: Acc=50%, Loss=0.6890699863433838 \t\t Validation: Acc=49%, Loss=0.7015097737312317\n",
            "Iteration: 318890 \t Train: Acc=57%, Loss=0.6874741315841675 \t\t Validation: Acc=54%, Loss=0.6844491362571716\n",
            "Iteration: 318900 \t Train: Acc=53%, Loss=0.6985105872154236 \t\t Validation: Acc=48%, Loss=0.7167797684669495\n",
            "Iteration: 318910 \t Train: Acc=50%, Loss=0.6900656819343567 \t\t Validation: Acc=49%, Loss=0.6976441144943237\n",
            "Iteration: 318920 \t Train: Acc=60%, Loss=0.6830471754074097 \t\t Validation: Acc=49%, Loss=0.7040596604347229\n",
            "Iteration: 318930 \t Train: Acc=52%, Loss=0.6957302689552307 \t\t Validation: Acc=53%, Loss=0.6908249855041504\n",
            "Iteration: 318940 \t Train: Acc=54%, Loss=0.6953998804092407 \t\t Validation: Acc=53%, Loss=0.6919347643852234\n",
            "Iteration: 318950 \t Train: Acc=53%, Loss=0.6892149448394775 \t\t Validation: Acc=48%, Loss=0.7153571844100952\n",
            "Iteration: 318960 \t Train: Acc=47%, Loss=0.6888209581375122 \t\t Validation: Acc=48%, Loss=0.7179557681083679\n",
            "It's been too long since we last saved the model. Saving...\n",
            "Iteration: 318970 \t Train: Acc=47%, Loss=0.7016316652297974 \t\t Validation: Acc=53%, Loss=0.6993630528450012\n",
            "Iteration: 318980 \t Train: Acc=55%, Loss=0.6881172060966492 \t\t Validation: Acc=50%, Loss=0.705390214920044\n",
            "Iteration: 318990 \t Train: Acc=47%, Loss=0.6966544389724731 \t\t Validation: Acc=53%, Loss=0.7142402529716492\n",
            "Iteration: 319000 \t Train: Acc=50%, Loss=0.6894426941871643 \t\t Validation: Acc=54%, Loss=0.6929538249969482\n",
            "Iteration: 319010 \t Train: Acc=48%, Loss=0.7003013491630554 \t\t Validation: Acc=50%, Loss=0.6943142414093018\n",
            "Iteration: 319020 \t Train: Acc=53%, Loss=0.6915502548217773 \t\t Validation: Acc=50%, Loss=0.7007631063461304\n",
            "Iteration: 319030 \t Train: Acc=48%, Loss=0.6963727474212646 \t\t Validation: Acc=50%, Loss=0.7022516131401062\n",
            "Iteration: 319040 \t Train: Acc=52%, Loss=0.6929250955581665 \t\t Validation: Acc=53%, Loss=0.7070409655570984\n",
            "Iteration: 319050 \t Train: Acc=53%, Loss=0.693027138710022 \t\t Validation: Acc=46%, Loss=0.706994891166687\n",
            "Iteration: 319060 \t Train: Acc=48%, Loss=0.7039997577667236 \t\t Validation: Acc=53%, Loss=0.6841151714324951\n",
            "Iteration: 319070 \t Train: Acc=53%, Loss=0.7016950845718384 \t\t Validation: Acc=51%, Loss=0.7072890400886536\n",
            "Iteration: 319080 \t Train: Acc=51%, Loss=0.6917345523834229 \t\t Validation: Acc=53%, Loss=0.6912136077880859\n",
            "Iteration: 319090 \t Train: Acc=46%, Loss=0.69852614402771 \t\t Validation: Acc=50%, Loss=0.6847472786903381\n",
            "Iteration: 319100 \t Train: Acc=51%, Loss=0.6871108412742615 \t\t Validation: Acc=51%, Loss=0.6813842058181763\n",
            "Iteration: 319110 \t Train: Acc=50%, Loss=0.6907307505607605 \t\t Validation: Acc=48%, Loss=0.7046583890914917\n",
            "Iteration: 319120 \t Train: Acc=48%, Loss=0.6935093998908997 \t\t Validation: Acc=52%, Loss=0.6939863562583923\n",
            "Iteration: 319130 \t Train: Acc=39%, Loss=0.6976636052131653 \t\t Validation: Acc=50%, Loss=0.690611720085144\n",
            "Iteration: 319140 \t Train: Acc=50%, Loss=0.6874696016311646 \t\t Validation: Acc=46%, Loss=0.6859506368637085\n",
            "Iteration: 319150 \t Train: Acc=51%, Loss=0.6868085265159607 \t\t Validation: Acc=51%, Loss=0.6920551061630249\n",
            "Iteration: 319160 \t Train: Acc=53%, Loss=0.6939598321914673 \t\t Validation: Acc=50%, Loss=0.6972109079360962\n",
            "Iteration: 319170 \t Train: Acc=52%, Loss=0.6922557353973389 \t\t Validation: Acc=47%, Loss=0.6971930861473083\n",
            "Iteration: 319180 \t Train: Acc=46%, Loss=0.6950975060462952 \t\t Validation: Acc=52%, Loss=0.7046722769737244\n",
            "Iteration: 319190 \t Train: Acc=48%, Loss=0.694606363773346 \t\t Validation: Acc=46%, Loss=0.7038060426712036\n",
            "Iteration: 319200 \t Train: Acc=56%, Loss=0.6951116323471069 \t\t Validation: Acc=50%, Loss=0.7006446123123169\n",
            "Iteration: 319210 \t Train: Acc=52%, Loss=0.6889407634735107 \t\t Validation: Acc=50%, Loss=0.6949655413627625\n",
            "Iteration: 319220 \t Train: Acc=54%, Loss=0.6909409165382385 \t\t Validation: Acc=50%, Loss=0.6932991743087769\n",
            "Iteration: 319230 \t Train: Acc=52%, Loss=0.688848614692688 \t\t Validation: Acc=47%, Loss=0.696117103099823\n",
            "Iteration: 319240 \t Train: Acc=54%, Loss=0.6880040764808655 \t\t Validation: Acc=50%, Loss=0.6971378922462463\n",
            "Iteration: 319250 \t Train: Acc=55%, Loss=0.6824842095375061 \t\t Validation: Acc=49%, Loss=0.692034900188446\n",
            "Iteration: 319260 \t Train: Acc=52%, Loss=0.6911094188690186 \t\t Validation: Acc=53%, Loss=0.6865462064743042\n",
            "Iteration: 319270 \t Train: Acc=53%, Loss=0.6911556720733643 \t\t Validation: Acc=52%, Loss=0.6959794759750366\n",
            "Iteration: 319280 \t Train: Acc=55%, Loss=0.6867093443870544 \t\t Validation: Acc=50%, Loss=0.694394052028656\n",
            "Iteration: 319290 \t Train: Acc=55%, Loss=0.6870667934417725 \t\t Validation: Acc=52%, Loss=0.6984591484069824\n",
            "Iteration: 319300 \t Train: Acc=47%, Loss=0.691163182258606 \t\t Validation: Acc=53%, Loss=0.687233567237854\n",
            "Iteration: 319310 \t Train: Acc=54%, Loss=0.6905021667480469 \t\t Validation: Acc=52%, Loss=0.7172446250915527\n",
            "Iteration: 319320 \t Train: Acc=54%, Loss=0.6807832717895508 \t\t Validation: Acc=48%, Loss=0.689702570438385\n",
            "Iteration: 319330 \t Train: Acc=50%, Loss=0.6935865879058838 \t\t Validation: Acc=53%, Loss=0.687187910079956\n",
            "Iteration: 319340 \t Train: Acc=60%, Loss=0.6878466010093689 \t\t Validation: Acc=48%, Loss=0.6891055107116699\n",
            "Iteration: 319350 \t Train: Acc=55%, Loss=0.6927294731140137 \t\t Validation: Acc=51%, Loss=0.6841788291931152\n",
            "Iteration: 319360 \t Train: Acc=54%, Loss=0.6909799575805664 \t\t Validation: Acc=52%, Loss=0.6905815601348877\n",
            "Iteration: 319370 \t Train: Acc=53%, Loss=0.6909818649291992 \t\t Validation: Acc=52%, Loss=0.7173551917076111\n",
            "Iteration: 319380 \t Train: Acc=52%, Loss=0.689365029335022 \t\t Validation: Acc=53%, Loss=0.6993987560272217\n",
            "Iteration: 319390 \t Train: Acc=53%, Loss=0.6836901307106018 \t\t Validation: Acc=50%, Loss=0.6936904788017273\n",
            "Iteration: 319400 \t Train: Acc=56%, Loss=0.6902949810028076 \t\t Validation: Acc=53%, Loss=0.693819522857666\n",
            "Iteration: 319410 \t Train: Acc=46%, Loss=0.6959438920021057 \t\t Validation: Acc=50%, Loss=0.7129560112953186\n",
            "Iteration: 319420 \t Train: Acc=52%, Loss=0.6914559006690979 \t\t Validation: Acc=50%, Loss=0.7056580781936646\n",
            "Iteration: 319430 \t Train: Acc=53%, Loss=0.699042022228241 \t\t Validation: Acc=53%, Loss=0.7006093263626099\n",
            "Iteration: 319440 \t Train: Acc=53%, Loss=0.6895848512649536 \t\t Validation: Acc=50%, Loss=0.6962818503379822\n",
            "Iteration: 319450 \t Train: Acc=55%, Loss=0.6880754232406616 \t\t Validation: Acc=51%, Loss=0.6841092109680176\n",
            "Iteration: 319460 \t Train: Acc=51%, Loss=0.6979635953903198 \t\t Validation: Acc=50%, Loss=0.7008774876594543\n",
            "Iteration: 319470 \t Train: Acc=49%, Loss=0.6957896947860718 \t\t Validation: Acc=49%, Loss=0.6941269636154175\n",
            "Iteration: 319480 \t Train: Acc=50%, Loss=0.6896080374717712 \t\t Validation: Acc=53%, Loss=0.6899206042289734\n",
            "Iteration: 319490 \t Train: Acc=49%, Loss=0.7045567035675049 \t\t Validation: Acc=48%, Loss=0.6906564831733704\n",
            "Iteration: 319500 \t Train: Acc=51%, Loss=0.6906930208206177 \t\t Validation: Acc=44%, Loss=0.7015368938446045\n",
            "Iteration: 319510 \t Train: Acc=55%, Loss=0.69034343957901 \t\t Validation: Acc=50%, Loss=0.7011483907699585\n",
            "Iteration: 319520 \t Train: Acc=46%, Loss=0.6927817463874817 \t\t Validation: Acc=49%, Loss=0.7050929665565491\n",
            "Iteration: 319530 \t Train: Acc=49%, Loss=0.6875697374343872 \t\t Validation: Acc=51%, Loss=0.7036100625991821\n",
            "Iteration: 319540 \t Train: Acc=48%, Loss=0.6914001107215881 \t\t Validation: Acc=50%, Loss=0.701076328754425\n",
            "Iteration: 319550 \t Train: Acc=57%, Loss=0.6866880655288696 \t\t Validation: Acc=49%, Loss=0.6882876753807068\n",
            "Iteration: 319560 \t Train: Acc=47%, Loss=0.6920135021209717 \t\t Validation: Acc=51%, Loss=0.6935771107673645\n",
            "Iteration: 319570 \t Train: Acc=51%, Loss=0.6817605495452881 \t\t Validation: Acc=52%, Loss=0.6918867826461792\n",
            "Iteration: 319580 \t Train: Acc=60%, Loss=0.6871153116226196 \t\t Validation: Acc=52%, Loss=0.6877370476722717\n",
            "Iteration: 319590 \t Train: Acc=51%, Loss=0.6966903805732727 \t\t Validation: Acc=47%, Loss=0.6990970969200134\n",
            "Iteration: 319600 \t Train: Acc=50%, Loss=0.6939167976379395 \t\t Validation: Acc=52%, Loss=0.6984654664993286\n",
            "Iteration: 319610 \t Train: Acc=53%, Loss=0.6907469034194946 \t\t Validation: Acc=52%, Loss=0.6983885765075684\n",
            "Iteration: 319620 \t Train: Acc=57%, Loss=0.683082640171051 \t\t Validation: Acc=51%, Loss=0.6949744820594788\n",
            "Iteration: 319630 \t Train: Acc=46%, Loss=0.6942879557609558 \t\t Validation: Acc=48%, Loss=0.7056909203529358\n",
            "Iteration: 319640 \t Train: Acc=55%, Loss=0.6909071207046509 \t\t Validation: Acc=53%, Loss=0.6920400857925415\n",
            "Iteration: 319650 \t Train: Acc=52%, Loss=0.6936224699020386 \t\t Validation: Acc=49%, Loss=0.7019504308700562\n",
            "Iteration: 319660 \t Train: Acc=50%, Loss=0.6884914040565491 \t\t Validation: Acc=50%, Loss=0.6947575807571411\n",
            "Iteration: 319670 \t Train: Acc=57%, Loss=0.6842060089111328 \t\t Validation: Acc=49%, Loss=0.7043123841285706\n",
            "Iteration: 319680 \t Train: Acc=53%, Loss=0.6925166845321655 \t\t Validation: Acc=53%, Loss=0.6933872699737549\n",
            "Iteration: 319690 \t Train: Acc=46%, Loss=0.6901525259017944 \t\t Validation: Acc=50%, Loss=0.702904462814331\n",
            "Iteration: 319700 \t Train: Acc=47%, Loss=0.6918268203735352 \t\t Validation: Acc=46%, Loss=0.7008461952209473\n",
            "Iteration: 319710 \t Train: Acc=53%, Loss=0.692796528339386 \t\t Validation: Acc=50%, Loss=0.6832975149154663\n",
            "Iteration: 319720 \t Train: Acc=52%, Loss=0.6871516108512878 \t\t Validation: Acc=50%, Loss=0.6934813261032104\n",
            "Iteration: 319730 \t Train: Acc=48%, Loss=0.6904630064964294 \t\t Validation: Acc=53%, Loss=0.701208233833313\n",
            "Iteration: 319740 \t Train: Acc=55%, Loss=0.6888841390609741 \t\t Validation: Acc=53%, Loss=0.6818053126335144\n",
            "Iteration: 319750 \t Train: Acc=50%, Loss=0.6976768970489502 \t\t Validation: Acc=51%, Loss=0.703416645526886\n",
            "Iteration: 319760 \t Train: Acc=49%, Loss=0.6900836825370789 \t\t Validation: Acc=53%, Loss=0.6967751979827881\n",
            "Iteration: 319770 \t Train: Acc=49%, Loss=0.6946614384651184 \t\t Validation: Acc=52%, Loss=0.7014657258987427\n",
            "Iteration: 319780 \t Train: Acc=54%, Loss=0.682551383972168 \t\t Validation: Acc=56%, Loss=0.6784841418266296\n",
            "Iteration: 319790 \t Train: Acc=46%, Loss=0.6967638731002808 \t\t Validation: Acc=51%, Loss=0.6947822570800781\n",
            "Iteration: 319800 \t Train: Acc=51%, Loss=0.6879312992095947 \t\t Validation: Acc=48%, Loss=0.7033478617668152\n",
            "Iteration: 319810 \t Train: Acc=57%, Loss=0.6881054639816284 \t\t Validation: Acc=53%, Loss=0.6871312856674194\n",
            "Iteration: 319820 \t Train: Acc=49%, Loss=0.7016631364822388 \t\t Validation: Acc=50%, Loss=0.7040941715240479\n",
            "Iteration: 319830 \t Train: Acc=49%, Loss=0.6947702169418335 \t\t Validation: Acc=48%, Loss=0.6891123056411743\n",
            "Iteration: 319840 \t Train: Acc=50%, Loss=0.6924296617507935 \t\t Validation: Acc=48%, Loss=0.7008855938911438\n",
            "Iteration: 319850 \t Train: Acc=53%, Loss=0.6925185918807983 \t\t Validation: Acc=50%, Loss=0.6875133514404297\n",
            "Iteration: 319860 \t Train: Acc=59%, Loss=0.6835084557533264 \t\t Validation: Acc=50%, Loss=0.703662097454071\n",
            "Iteration: 319870 \t Train: Acc=46%, Loss=0.6984884738922119 \t\t Validation: Acc=50%, Loss=0.6915968656539917\n",
            "Iteration: 319880 \t Train: Acc=50%, Loss=0.6924638152122498 \t\t Validation: Acc=50%, Loss=0.6935911178588867\n",
            "Iteration: 319890 \t Train: Acc=53%, Loss=0.6899260878562927 \t\t Validation: Acc=52%, Loss=0.6910355091094971\n",
            "Iteration: 319900 \t Train: Acc=52%, Loss=0.6885411739349365 \t\t Validation: Acc=47%, Loss=0.6939889192581177\n",
            "Iteration: 319910 \t Train: Acc=46%, Loss=0.689854621887207 \t\t Validation: Acc=52%, Loss=0.6916199922561646\n",
            "Iteration: 319920 \t Train: Acc=53%, Loss=0.6835648417472839 \t\t Validation: Acc=51%, Loss=0.704041063785553\n",
            "Iteration: 319930 \t Train: Acc=55%, Loss=0.6892520189285278 \t\t Validation: Acc=53%, Loss=0.6888708472251892\n",
            "Iteration: 319940 \t Train: Acc=50%, Loss=0.6924167275428772 \t\t Validation: Acc=49%, Loss=0.6998671293258667\n",
            "Iteration: 319950 \t Train: Acc=51%, Loss=0.6947094202041626 \t\t Validation: Acc=47%, Loss=0.7041515707969666\n",
            "Iteration: 319960 \t Train: Acc=50%, Loss=0.6939753890037537 \t\t Validation: Acc=47%, Loss=0.6999127268791199\n",
            "Iteration: 319970 \t Train: Acc=53%, Loss=0.6903324723243713 \t\t Validation: Acc=50%, Loss=0.691356360912323\n",
            "Iteration: 319980 \t Train: Acc=49%, Loss=0.6913743615150452 \t\t Validation: Acc=51%, Loss=0.7001782655715942\n",
            "Iteration: 319990 \t Train: Acc=52%, Loss=0.6936870813369751 \t\t Validation: Acc=50%, Loss=0.6970844864845276\n",
            "Iteration: 320000 \t Train: Acc=46%, Loss=0.6886975765228271 \t\t Validation: Acc=50%, Loss=0.7004845142364502\n",
            "Iteration: 320010 \t Train: Acc=51%, Loss=0.6916552186012268 \t\t Validation: Acc=48%, Loss=0.6952751278877258\n",
            "Iteration: 320020 \t Train: Acc=54%, Loss=0.6902732849121094 \t\t Validation: Acc=47%, Loss=0.6967748403549194\n",
            "Iteration: 320030 \t Train: Acc=49%, Loss=0.6934692859649658 \t\t Validation: Acc=50%, Loss=0.6870615482330322\n",
            "Iteration: 320040 \t Train: Acc=51%, Loss=0.6879063248634338 \t\t Validation: Acc=51%, Loss=0.6988964080810547\n",
            "Iteration: 320050 \t Train: Acc=50%, Loss=0.6974539160728455 \t\t Validation: Acc=52%, Loss=0.69959557056427\n",
            "Iteration: 320060 \t Train: Acc=49%, Loss=0.703351616859436 \t\t Validation: Acc=53%, Loss=0.6963816285133362\n",
            "Iteration: 320070 \t Train: Acc=52%, Loss=0.6930978298187256 \t\t Validation: Acc=53%, Loss=0.7015035152435303\n",
            "Iteration: 320080 \t Train: Acc=51%, Loss=0.6844322681427002 \t\t Validation: Acc=50%, Loss=0.6886419057846069\n",
            "Iteration: 320090 \t Train: Acc=47%, Loss=0.7022776007652283 \t\t Validation: Acc=50%, Loss=0.7060031294822693\n",
            "Iteration: 320100 \t Train: Acc=51%, Loss=0.6961795687675476 \t\t Validation: Acc=50%, Loss=0.6799467206001282\n",
            "Iteration: 320110 \t Train: Acc=49%, Loss=0.6881349086761475 \t\t Validation: Acc=47%, Loss=0.6889239549636841\n",
            "Iteration: 320120 \t Train: Acc=55%, Loss=0.688281238079071 \t\t Validation: Acc=49%, Loss=0.6925853490829468\n",
            "Iteration: 320130 \t Train: Acc=48%, Loss=0.7029284238815308 \t\t Validation: Acc=51%, Loss=0.6873506307601929\n",
            "Iteration: 320140 \t Train: Acc=51%, Loss=0.6850489377975464 \t\t Validation: Acc=51%, Loss=0.6919569969177246\n",
            "Iteration: 320150 \t Train: Acc=51%, Loss=0.6884265542030334 \t\t Validation: Acc=49%, Loss=0.6999335885047913\n",
            "Iteration: 320160 \t Train: Acc=49%, Loss=0.6915419101715088 \t\t Validation: Acc=57%, Loss=0.6772685050964355\n",
            "Iteration: 320170 \t Train: Acc=57%, Loss=0.6869635581970215 \t\t Validation: Acc=52%, Loss=0.6912059783935547\n",
            "Iteration: 320180 \t Train: Acc=50%, Loss=0.692520022392273 \t\t Validation: Acc=48%, Loss=0.7041952610015869\n",
            "Iteration: 320190 \t Train: Acc=54%, Loss=0.682776689529419 \t\t Validation: Acc=51%, Loss=0.697823703289032\n",
            "Iteration: 320200 \t Train: Acc=50%, Loss=0.6888936161994934 \t\t Validation: Acc=47%, Loss=0.7069318294525146\n",
            "Iteration: 320210 \t Train: Acc=57%, Loss=0.6895305514335632 \t\t Validation: Acc=50%, Loss=0.693081259727478\n",
            "Iteration: 320220 \t Train: Acc=45%, Loss=0.6960863471031189 \t\t Validation: Acc=46%, Loss=0.68805992603302\n",
            "Iteration: 320230 \t Train: Acc=48%, Loss=0.6952555179595947 \t\t Validation: Acc=53%, Loss=0.6768393516540527\n",
            "Iteration: 320240 \t Train: Acc=55%, Loss=0.6925616264343262 \t\t Validation: Acc=54%, Loss=0.6902434825897217\n",
            "Iteration: 320250 \t Train: Acc=47%, Loss=0.6928706765174866 \t\t Validation: Acc=52%, Loss=0.6891478896141052\n",
            "Iteration: 320260 \t Train: Acc=44%, Loss=0.6974508762359619 \t\t Validation: Acc=49%, Loss=0.7005223631858826\n",
            "Iteration: 320270 \t Train: Acc=53%, Loss=0.6811050176620483 \t\t Validation: Acc=49%, Loss=0.6886418461799622\n",
            "Iteration: 320280 \t Train: Acc=53%, Loss=0.6892273426055908 \t\t Validation: Acc=53%, Loss=0.6871541738510132\n",
            "Iteration: 320290 \t Train: Acc=50%, Loss=0.6899598240852356 \t\t Validation: Acc=52%, Loss=0.6950848698616028\n",
            "Iteration: 320300 \t Train: Acc=51%, Loss=0.6993396282196045 \t\t Validation: Acc=45%, Loss=0.7034286856651306\n",
            "Iteration: 320310 \t Train: Acc=51%, Loss=0.6905108690261841 \t\t Validation: Acc=56%, Loss=0.6870677471160889\n",
            "Iteration: 320320 \t Train: Acc=46%, Loss=0.6934061646461487 \t\t Validation: Acc=50%, Loss=0.6968375444412231\n",
            "Iteration: 320330 \t Train: Acc=46%, Loss=0.7007129192352295 \t\t Validation: Acc=51%, Loss=0.6844329833984375\n",
            "Iteration: 320340 \t Train: Acc=45%, Loss=0.6945523619651794 \t\t Validation: Acc=51%, Loss=0.6819719076156616\n",
            "Iteration: 320350 \t Train: Acc=52%, Loss=0.6945143938064575 \t\t Validation: Acc=51%, Loss=0.6963112950325012\n",
            "Iteration: 320360 \t Train: Acc=55%, Loss=0.6837465763092041 \t\t Validation: Acc=55%, Loss=0.6982424259185791\n",
            "Iteration: 320370 \t Train: Acc=54%, Loss=0.6904925107955933 \t\t Validation: Acc=53%, Loss=0.682717502117157\n",
            "Iteration: 320380 \t Train: Acc=55%, Loss=0.6883858442306519 \t\t Validation: Acc=49%, Loss=0.6889449954032898\n",
            "Iteration: 320390 \t Train: Acc=52%, Loss=0.6887198090553284 \t\t Validation: Acc=53%, Loss=0.6938616633415222\n",
            "Iteration: 320400 \t Train: Acc=54%, Loss=0.6873816251754761 \t\t Validation: Acc=48%, Loss=0.7034679055213928\n",
            "Iteration: 320410 \t Train: Acc=52%, Loss=0.6867659091949463 \t\t Validation: Acc=52%, Loss=0.6984044909477234\n",
            "Iteration: 320420 \t Train: Acc=53%, Loss=0.6841532588005066 \t\t Validation: Acc=46%, Loss=0.7079032063484192\n",
            "Iteration: 320430 \t Train: Acc=54%, Loss=0.6896063089370728 \t\t Validation: Acc=50%, Loss=0.6888386607170105\n",
            "Iteration: 320440 \t Train: Acc=52%, Loss=0.6969274282455444 \t\t Validation: Acc=49%, Loss=0.7025574445724487\n",
            "Iteration: 320450 \t Train: Acc=50%, Loss=0.6922457218170166 \t\t Validation: Acc=53%, Loss=0.7026838064193726\n",
            "Iteration: 320460 \t Train: Acc=51%, Loss=0.6925070285797119 \t\t Validation: Acc=51%, Loss=0.7024709582328796\n",
            "Iteration: 320470 \t Train: Acc=50%, Loss=0.6945018768310547 \t\t Validation: Acc=45%, Loss=0.7021750211715698\n",
            "Iteration: 320480 \t Train: Acc=49%, Loss=0.6920269727706909 \t\t Validation: Acc=50%, Loss=0.688377320766449\n",
            "Iteration: 320490 \t Train: Acc=47%, Loss=0.6997313499450684 \t\t Validation: Acc=50%, Loss=0.7050439119338989\n",
            "Iteration: 320500 \t Train: Acc=52%, Loss=0.6900803446769714 \t\t Validation: Acc=46%, Loss=0.7010211944580078\n",
            "Iteration: 320510 \t Train: Acc=50%, Loss=0.6911337375640869 \t\t Validation: Acc=50%, Loss=0.7085767388343811\n",
            "Iteration: 320520 \t Train: Acc=53%, Loss=0.6824504733085632 \t\t Validation: Acc=46%, Loss=0.6952135562896729\n",
            "Iteration: 320530 \t Train: Acc=50%, Loss=0.6966075301170349 \t\t Validation: Acc=50%, Loss=0.7011688351631165\n",
            "Iteration: 320540 \t Train: Acc=52%, Loss=0.6875594854354858 \t\t Validation: Acc=50%, Loss=0.7051255702972412\n",
            "Iteration: 320550 \t Train: Acc=56%, Loss=0.6842823028564453 \t\t Validation: Acc=49%, Loss=0.6914989948272705\n",
            "Iteration: 320560 \t Train: Acc=55%, Loss=0.6837307214736938 \t\t Validation: Acc=52%, Loss=0.7036425471305847\n",
            "Iteration: 320570 \t Train: Acc=53%, Loss=0.6885620355606079 \t\t Validation: Acc=54%, Loss=0.6883593797683716\n",
            "Iteration: 320580 \t Train: Acc=54%, Loss=0.693844199180603 \t\t Validation: Acc=50%, Loss=0.6782586574554443\n",
            "Iteration: 320590 \t Train: Acc=58%, Loss=0.6921551823616028 \t\t Validation: Acc=50%, Loss=0.6894679665565491\n",
            "Iteration: 320600 \t Train: Acc=46%, Loss=0.6967207789421082 \t\t Validation: Acc=55%, Loss=0.6839463114738464\n",
            "Iteration: 320610 \t Train: Acc=57%, Loss=0.6865461468696594 \t\t Validation: Acc=50%, Loss=0.7073306441307068\n",
            "Iteration: 320620 \t Train: Acc=49%, Loss=0.6930672526359558 \t\t Validation: Acc=46%, Loss=0.6915097832679749\n",
            "Iteration: 320630 \t Train: Acc=55%, Loss=0.6879387497901917 \t\t Validation: Acc=48%, Loss=0.7004774808883667\n",
            "Iteration: 320640 \t Train: Acc=51%, Loss=0.6950051784515381 \t\t Validation: Acc=46%, Loss=0.6995819211006165\n",
            "Iteration: 320650 \t Train: Acc=52%, Loss=0.692370593547821 \t\t Validation: Acc=47%, Loss=0.6930631399154663\n",
            "Iteration: 320660 \t Train: Acc=43%, Loss=0.6992510557174683 \t\t Validation: Acc=47%, Loss=0.6994431018829346\n",
            "Iteration: 320670 \t Train: Acc=49%, Loss=0.6845587491989136 \t\t Validation: Acc=53%, Loss=0.6847994327545166\n",
            "Iteration: 320680 \t Train: Acc=49%, Loss=0.6931363940238953 \t\t Validation: Acc=46%, Loss=0.6909029483795166\n",
            "Iteration: 320690 \t Train: Acc=52%, Loss=0.6976419687271118 \t\t Validation: Acc=51%, Loss=0.7034546136856079\n",
            "Iteration: 320700 \t Train: Acc=47%, Loss=0.6970930695533752 \t\t Validation: Acc=49%, Loss=0.7133186459541321\n",
            "Iteration: 320710 \t Train: Acc=54%, Loss=0.6865493059158325 \t\t Validation: Acc=50%, Loss=0.7030223608016968\n",
            "Iteration: 320720 \t Train: Acc=50%, Loss=0.6987096667289734 \t\t Validation: Acc=46%, Loss=0.7207169532775879\n",
            "Iteration: 320730 \t Train: Acc=48%, Loss=0.7045559287071228 \t\t Validation: Acc=51%, Loss=0.7224781513214111\n",
            "Iteration: 320740 \t Train: Acc=45%, Loss=0.7052419185638428 \t\t Validation: Acc=50%, Loss=0.6840997934341431\n",
            "Iteration: 320750 \t Train: Acc=50%, Loss=0.6965988278388977 \t\t Validation: Acc=50%, Loss=0.6886351704597473\n",
            "Iteration: 320760 \t Train: Acc=49%, Loss=0.6939373016357422 \t\t Validation: Acc=51%, Loss=0.7044744491577148\n",
            "Iteration: 320770 \t Train: Acc=50%, Loss=0.6952023506164551 \t\t Validation: Acc=46%, Loss=0.7150775194168091\n",
            "Iteration: 320780 \t Train: Acc=57%, Loss=0.687877357006073 \t\t Validation: Acc=47%, Loss=0.6880546808242798\n",
            "Iteration: 320790 \t Train: Acc=50%, Loss=0.6980257034301758 \t\t Validation: Acc=51%, Loss=0.689465343952179\n",
            "Iteration: 320800 \t Train: Acc=49%, Loss=0.7026737928390503 \t\t Validation: Acc=51%, Loss=0.6994031667709351\n",
            "Iteration: 320810 \t Train: Acc=45%, Loss=0.6930925250053406 \t\t Validation: Acc=51%, Loss=0.6920434236526489\n",
            "Iteration: 320820 \t Train: Acc=49%, Loss=0.6911625862121582 \t\t Validation: Acc=50%, Loss=0.6929137110710144\n",
            "Iteration: 320830 \t Train: Acc=50%, Loss=0.6951592564582825 \t\t Validation: Acc=50%, Loss=0.694631814956665\n",
            "Iteration: 320840 \t Train: Acc=60%, Loss=0.6875991821289062 \t\t Validation: Acc=52%, Loss=0.6862151026725769\n",
            "Iteration: 320850 \t Train: Acc=57%, Loss=0.6969525814056396 \t\t Validation: Acc=53%, Loss=0.6977673768997192\n",
            "Iteration: 320860 \t Train: Acc=54%, Loss=0.6895630359649658 \t\t Validation: Acc=55%, Loss=0.6850969195365906\n",
            "Iteration: 320870 \t Train: Acc=53%, Loss=0.690451443195343 \t\t Validation: Acc=47%, Loss=0.6985939741134644\n",
            "Iteration: 320880 \t Train: Acc=57%, Loss=0.684735894203186 \t\t Validation: Acc=53%, Loss=0.6878176331520081\n",
            "Iteration: 320890 \t Train: Acc=51%, Loss=0.6888876557350159 \t\t Validation: Acc=54%, Loss=0.6758288741111755\n",
            "Iteration: 320900 \t Train: Acc=52%, Loss=0.6914098262786865 \t\t Validation: Acc=48%, Loss=0.6953340768814087\n",
            "Iteration: 320910 \t Train: Acc=46%, Loss=0.6978566646575928 \t\t Validation: Acc=52%, Loss=0.6950984001159668\n",
            "Iteration: 320920 \t Train: Acc=45%, Loss=0.7021043300628662 \t\t Validation: Acc=47%, Loss=0.6959924697875977\n",
            "Iteration: 320930 \t Train: Acc=53%, Loss=0.6900395154953003 \t\t Validation: Acc=48%, Loss=0.6880505681037903\n",
            "Iteration: 320940 \t Train: Acc=51%, Loss=0.6872627139091492 \t\t Validation: Acc=46%, Loss=0.6967958211898804\n",
            "Iteration: 320950 \t Train: Acc=48%, Loss=0.6897892951965332 \t\t Validation: Acc=47%, Loss=0.6912779808044434\n",
            "Iteration: 320960 \t Train: Acc=55%, Loss=0.683197557926178 \t\t Validation: Acc=49%, Loss=0.6936376094818115\n",
            "Iteration: 320970 \t Train: Acc=50%, Loss=0.6940076351165771 \t\t Validation: Acc=48%, Loss=0.6990588903427124\n",
            "Iteration: 320980 \t Train: Acc=56%, Loss=0.6864701509475708 \t\t Validation: Acc=54%, Loss=0.7033818960189819\n",
            "Iteration: 320990 \t Train: Acc=52%, Loss=0.6899938583374023 \t\t Validation: Acc=50%, Loss=0.6808030605316162\n",
            "Iteration: 321000 \t Train: Acc=53%, Loss=0.6993613243103027 \t\t Validation: Acc=53%, Loss=0.687329888343811\n",
            "Iteration: 321010 \t Train: Acc=50%, Loss=0.7028416991233826 \t\t Validation: Acc=49%, Loss=0.6845855712890625\n",
            "Iteration: 321020 \t Train: Acc=54%, Loss=0.6962621212005615 \t\t Validation: Acc=50%, Loss=0.6941416263580322\n",
            "Iteration: 321030 \t Train: Acc=48%, Loss=0.6978405714035034 \t\t Validation: Acc=49%, Loss=0.6921728253364563\n",
            "Iteration: 321040 \t Train: Acc=53%, Loss=0.6914102435112 \t\t Validation: Acc=47%, Loss=0.7065066695213318\n",
            "Iteration: 321050 \t Train: Acc=52%, Loss=0.6895556449890137 \t\t Validation: Acc=53%, Loss=0.7059997916221619\n",
            "Iteration: 321060 \t Train: Acc=49%, Loss=0.6934090852737427 \t\t Validation: Acc=51%, Loss=0.7027740478515625\n",
            "Iteration: 321070 \t Train: Acc=49%, Loss=0.6860833764076233 \t\t Validation: Acc=58%, Loss=0.6711621284484863\n",
            "Iteration: 321080 \t Train: Acc=54%, Loss=0.6885597109794617 \t\t Validation: Acc=51%, Loss=0.6839144229888916\n",
            "Iteration: 321090 \t Train: Acc=44%, Loss=0.6935039758682251 \t\t Validation: Acc=53%, Loss=0.6863808631896973\n",
            "Iteration: 321100 \t Train: Acc=51%, Loss=0.69422447681427 \t\t Validation: Acc=51%, Loss=0.6760427355766296\n",
            "Iteration: 321110 \t Train: Acc=51%, Loss=0.6923065781593323 \t\t Validation: Acc=48%, Loss=0.7030327320098877\n",
            "Iteration: 321120 \t Train: Acc=46%, Loss=0.7000586986541748 \t\t Validation: Acc=45%, Loss=0.7052832841873169\n",
            "Iteration: 321130 \t Train: Acc=48%, Loss=0.6941028833389282 \t\t Validation: Acc=48%, Loss=0.698286235332489\n",
            "Iteration: 321140 \t Train: Acc=52%, Loss=0.6945061683654785 \t\t Validation: Acc=47%, Loss=0.6868921518325806\n",
            "Iteration: 321150 \t Train: Acc=47%, Loss=0.6920295357704163 \t\t Validation: Acc=47%, Loss=0.6940459609031677\n",
            "Iteration: 321160 \t Train: Acc=55%, Loss=0.6886955499649048 \t\t Validation: Acc=50%, Loss=0.696526288986206\n",
            "Iteration: 321170 \t Train: Acc=47%, Loss=0.6943618059158325 \t\t Validation: Acc=48%, Loss=0.6897647976875305\n",
            "Iteration: 321180 \t Train: Acc=53%, Loss=0.6850156784057617 \t\t Validation: Acc=51%, Loss=0.702351450920105\n",
            "Iteration: 321190 \t Train: Acc=52%, Loss=0.6910291314125061 \t\t Validation: Acc=52%, Loss=0.6845117807388306\n",
            "Iteration: 321200 \t Train: Acc=52%, Loss=0.6975409388542175 \t\t Validation: Acc=51%, Loss=0.7024285793304443\n",
            "Iteration: 321210 \t Train: Acc=52%, Loss=0.6880759000778198 \t\t Validation: Acc=50%, Loss=0.6914222240447998\n",
            "Iteration: 321220 \t Train: Acc=48%, Loss=0.6937904357910156 \t\t Validation: Acc=48%, Loss=0.6936256289482117\n",
            "Iteration: 321230 \t Train: Acc=50%, Loss=0.6913309097290039 \t\t Validation: Acc=49%, Loss=0.6864345073699951\n",
            "Iteration: 321240 \t Train: Acc=52%, Loss=0.6907281875610352 \t\t Validation: Acc=52%, Loss=0.7045911550521851\n",
            "Iteration: 321250 \t Train: Acc=51%, Loss=0.687738299369812 \t\t Validation: Acc=54%, Loss=0.69173264503479\n",
            "Iteration: 321260 \t Train: Acc=50%, Loss=0.6939798593521118 \t\t Validation: Acc=50%, Loss=0.6960781812667847\n",
            "Iteration: 321270 \t Train: Acc=51%, Loss=0.7043749094009399 \t\t Validation: Acc=50%, Loss=0.7048108577728271\n",
            "Iteration: 321280 \t Train: Acc=53%, Loss=0.6911433935165405 \t\t Validation: Acc=52%, Loss=0.6801198720932007\n",
            "Iteration: 321290 \t Train: Acc=54%, Loss=0.6903656125068665 \t\t Validation: Acc=51%, Loss=0.6858571171760559\n",
            "Iteration: 321300 \t Train: Acc=46%, Loss=0.694923996925354 \t\t Validation: Acc=48%, Loss=0.7073240876197815\n",
            "Iteration: 321310 \t Train: Acc=48%, Loss=0.6963798999786377 \t\t Validation: Acc=53%, Loss=0.6815648674964905\n",
            "Iteration: 321320 \t Train: Acc=48%, Loss=0.7025677561759949 \t\t Validation: Acc=48%, Loss=0.6995507478713989\n",
            "Iteration: 321330 \t Train: Acc=51%, Loss=0.6838744878768921 \t\t Validation: Acc=52%, Loss=0.697257399559021\n",
            "Iteration: 321340 \t Train: Acc=46%, Loss=0.6941748857498169 \t\t Validation: Acc=54%, Loss=0.6979782581329346\n",
            "Iteration: 321350 \t Train: Acc=45%, Loss=0.6911870241165161 \t\t Validation: Acc=51%, Loss=0.6884117126464844\n",
            "Iteration: 321360 \t Train: Acc=50%, Loss=0.6896911859512329 \t\t Validation: Acc=52%, Loss=0.6876004934310913\n",
            "Iteration: 321370 \t Train: Acc=58%, Loss=0.6837083697319031 \t\t Validation: Acc=50%, Loss=0.705662727355957\n",
            "Iteration: 321380 \t Train: Acc=46%, Loss=0.7010540962219238 \t\t Validation: Acc=52%, Loss=0.700560986995697\n",
            "Iteration: 321390 \t Train: Acc=48%, Loss=0.6882150769233704 \t\t Validation: Acc=50%, Loss=0.6772817373275757\n",
            "Iteration: 321400 \t Train: Acc=53%, Loss=0.6920390129089355 \t\t Validation: Acc=49%, Loss=0.7041913866996765\n",
            "Iteration: 321410 \t Train: Acc=56%, Loss=0.6840545535087585 \t\t Validation: Acc=50%, Loss=0.6867208480834961\n",
            "Iteration: 321420 \t Train: Acc=50%, Loss=0.6965325474739075 \t\t Validation: Acc=48%, Loss=0.6871741414070129\n",
            "Iteration: 321430 \t Train: Acc=53%, Loss=0.6857867240905762 \t\t Validation: Acc=51%, Loss=0.6953567862510681\n",
            "Iteration: 321440 \t Train: Acc=53%, Loss=0.6946575045585632 \t\t Validation: Acc=50%, Loss=0.6853107810020447\n",
            "Iteration: 321450 \t Train: Acc=50%, Loss=0.694733202457428 \t\t Validation: Acc=47%, Loss=0.6922906637191772\n",
            "Iteration: 321460 \t Train: Acc=46%, Loss=0.6957436800003052 \t\t Validation: Acc=48%, Loss=0.6884124279022217\n",
            "Iteration: 321470 \t Train: Acc=53%, Loss=0.6808645725250244 \t\t Validation: Acc=45%, Loss=0.696351945400238\n",
            "Iteration: 321480 \t Train: Acc=53%, Loss=0.6885503530502319 \t\t Validation: Acc=51%, Loss=0.6897006034851074\n",
            "Iteration: 321490 \t Train: Acc=56%, Loss=0.6852965950965881 \t\t Validation: Acc=50%, Loss=0.7071951627731323\n",
            "Iteration: 321500 \t Train: Acc=51%, Loss=0.6887679100036621 \t\t Validation: Acc=50%, Loss=0.7055034637451172\n",
            "Iteration: 321510 \t Train: Acc=51%, Loss=0.6909434795379639 \t\t Validation: Acc=53%, Loss=0.695277214050293\n",
            "Iteration: 321520 \t Train: Acc=51%, Loss=0.6918776035308838 \t\t Validation: Acc=46%, Loss=0.6963523030281067\n",
            "Iteration: 321530 \t Train: Acc=44%, Loss=0.6962242126464844 \t\t Validation: Acc=52%, Loss=0.7007021903991699\n",
            "Iteration: 321540 \t Train: Acc=53%, Loss=0.6893531680107117 \t\t Validation: Acc=44%, Loss=0.7209811210632324\n",
            "Iteration: 321550 \t Train: Acc=43%, Loss=0.6950376033782959 \t\t Validation: Acc=54%, Loss=0.6884695887565613\n",
            "Iteration: 321560 \t Train: Acc=51%, Loss=0.6951647996902466 \t\t Validation: Acc=49%, Loss=0.7008436322212219\n",
            "Iteration: 321570 \t Train: Acc=50%, Loss=0.6892328262329102 \t\t Validation: Acc=49%, Loss=0.7014862895011902\n",
            "Iteration: 321580 \t Train: Acc=50%, Loss=0.6916411519050598 \t\t Validation: Acc=50%, Loss=0.6869713664054871\n",
            "Iteration: 321590 \t Train: Acc=57%, Loss=0.6920096278190613 \t\t Validation: Acc=50%, Loss=0.6938128471374512\n",
            "Iteration: 321600 \t Train: Acc=46%, Loss=0.6966018676757812 \t\t Validation: Acc=47%, Loss=0.6920914649963379\n",
            "Iteration: 321610 \t Train: Acc=50%, Loss=0.6879199147224426 \t\t Validation: Acc=51%, Loss=0.7007025480270386\n",
            "Iteration: 321620 \t Train: Acc=49%, Loss=0.6938228011131287 \t\t Validation: Acc=49%, Loss=0.6906988620758057\n",
            "Iteration: 321630 \t Train: Acc=51%, Loss=0.7024426460266113 \t\t Validation: Acc=55%, Loss=0.6778469681739807\n",
            "Iteration: 321640 \t Train: Acc=51%, Loss=0.6944395303726196 \t\t Validation: Acc=49%, Loss=0.7048473954200745\n",
            "Iteration: 321650 \t Train: Acc=50%, Loss=0.6899653077125549 \t\t Validation: Acc=48%, Loss=0.6943492889404297\n",
            "Iteration: 321660 \t Train: Acc=43%, Loss=0.701467752456665 \t\t Validation: Acc=46%, Loss=0.6955428123474121\n",
            "Iteration: 321670 \t Train: Acc=50%, Loss=0.6864636540412903 \t\t Validation: Acc=50%, Loss=0.6957340836524963\n",
            "Iteration: 321680 \t Train: Acc=53%, Loss=0.6842663288116455 \t\t Validation: Acc=51%, Loss=0.6981094479560852\n",
            "Iteration: 321690 \t Train: Acc=53%, Loss=0.6907082200050354 \t\t Validation: Acc=45%, Loss=0.7049365639686584\n",
            "Iteration: 321700 \t Train: Acc=53%, Loss=0.6894234418869019 \t\t Validation: Acc=51%, Loss=0.6856557726860046\n",
            "Iteration: 321710 \t Train: Acc=50%, Loss=0.691268265247345 \t\t Validation: Acc=47%, Loss=0.6947343349456787\n",
            "Iteration: 321720 \t Train: Acc=50%, Loss=0.6959450244903564 \t\t Validation: Acc=50%, Loss=0.7079870700836182\n",
            "Iteration: 321730 \t Train: Acc=55%, Loss=0.6828010678291321 \t\t Validation: Acc=55%, Loss=0.6888161897659302\n",
            "Iteration: 321740 \t Train: Acc=52%, Loss=0.6871377229690552 \t\t Validation: Acc=50%, Loss=0.6784859299659729\n",
            "Iteration: 321750 \t Train: Acc=50%, Loss=0.6982831358909607 \t\t Validation: Acc=49%, Loss=0.6876612901687622\n",
            "Iteration: 321760 \t Train: Acc=52%, Loss=0.6891269683837891 \t\t Validation: Acc=46%, Loss=0.6892330646514893\n",
            "Iteration: 321770 \t Train: Acc=52%, Loss=0.6859624981880188 \t\t Validation: Acc=50%, Loss=0.6951813697814941\n",
            "Iteration: 321780 \t Train: Acc=53%, Loss=0.6899992227554321 \t\t Validation: Acc=45%, Loss=0.7079634666442871\n",
            "Iteration: 321790 \t Train: Acc=50%, Loss=0.691992461681366 \t\t Validation: Acc=56%, Loss=0.7025548219680786\n",
            "Iteration: 321800 \t Train: Acc=51%, Loss=0.6894299387931824 \t\t Validation: Acc=51%, Loss=0.6875377893447876\n",
            "Iteration: 321810 \t Train: Acc=49%, Loss=0.6965974569320679 \t\t Validation: Acc=50%, Loss=0.7005506753921509\n",
            "Iteration: 321820 \t Train: Acc=54%, Loss=0.6844353675842285 \t\t Validation: Acc=54%, Loss=0.6696547865867615\n",
            "Iteration: 321830 \t Train: Acc=51%, Loss=0.6896148324012756 \t\t Validation: Acc=49%, Loss=0.706588625907898\n",
            "Iteration: 321840 \t Train: Acc=53%, Loss=0.686133861541748 \t\t Validation: Acc=47%, Loss=0.7007928490638733\n",
            "Iteration: 321850 \t Train: Acc=54%, Loss=0.686582088470459 \t\t Validation: Acc=50%, Loss=0.69144606590271\n",
            "Iteration: 321860 \t Train: Acc=50%, Loss=0.6909527778625488 \t\t Validation: Acc=56%, Loss=0.6787195205688477\n",
            "Iteration: 321870 \t Train: Acc=53%, Loss=0.6928000450134277 \t\t Validation: Acc=54%, Loss=0.6974353790283203\n",
            "Iteration: 321880 \t Train: Acc=49%, Loss=0.7005206346511841 \t\t Validation: Acc=47%, Loss=0.6929079294204712\n",
            "Iteration: 321890 \t Train: Acc=48%, Loss=0.697921872138977 \t\t Validation: Acc=44%, Loss=0.7047742605209351\n",
            "Iteration: 321900 \t Train: Acc=55%, Loss=0.6917088031768799 \t\t Validation: Acc=49%, Loss=0.6951201558113098\n",
            "Iteration: 321910 \t Train: Acc=46%, Loss=0.6981987953186035 \t\t Validation: Acc=53%, Loss=0.6933305859565735\n",
            "Iteration: 321920 \t Train: Acc=48%, Loss=0.6965461373329163 \t\t Validation: Acc=50%, Loss=0.69859379529953\n",
            "Iteration: 321930 \t Train: Acc=53%, Loss=0.7039452791213989 \t\t Validation: Acc=49%, Loss=0.6978697180747986\n",
            "Iteration: 321940 \t Train: Acc=45%, Loss=0.695824921131134 \t\t Validation: Acc=48%, Loss=0.6964221000671387\n",
            "Iteration: 321950 \t Train: Acc=51%, Loss=0.6930561065673828 \t\t Validation: Acc=47%, Loss=0.6963577270507812\n",
            "Iteration: 321960 \t Train: Acc=49%, Loss=0.6943823099136353 \t\t Validation: Acc=50%, Loss=0.694362223148346\n",
            "Iteration: 321970 \t Train: Acc=53%, Loss=0.689531147480011 \t\t Validation: Acc=46%, Loss=0.7125551104545593\n",
            "Iteration: 321980 \t Train: Acc=55%, Loss=0.6801090836524963 \t\t Validation: Acc=47%, Loss=0.6993959546089172\n",
            "Iteration: 321990 \t Train: Acc=46%, Loss=0.6966261267662048 \t\t Validation: Acc=50%, Loss=0.7015635967254639\n",
            "Iteration: 322000 \t Train: Acc=46%, Loss=0.6943092346191406 \t\t Validation: Acc=46%, Loss=0.7074651122093201\n",
            "Iteration: 322010 \t Train: Acc=47%, Loss=0.6933357119560242 \t\t Validation: Acc=53%, Loss=0.6838096380233765\n",
            "Iteration: 322020 \t Train: Acc=54%, Loss=0.6891661286354065 \t\t Validation: Acc=55%, Loss=0.6864560842514038\n",
            "Iteration: 322030 \t Train: Acc=50%, Loss=0.695925235748291 \t\t Validation: Acc=59%, Loss=0.6882344484329224\n",
            "Iteration: 322040 \t Train: Acc=45%, Loss=0.6951748728752136 \t\t Validation: Acc=52%, Loss=0.6970560550689697\n",
            "Iteration: 322050 \t Train: Acc=47%, Loss=0.7014338970184326 \t\t Validation: Acc=50%, Loss=0.6953762769699097\n",
            "Iteration: 322060 \t Train: Acc=55%, Loss=0.6868743896484375 \t\t Validation: Acc=52%, Loss=0.6876985430717468\n",
            "Iteration: 322070 \t Train: Acc=52%, Loss=0.6895131468772888 \t\t Validation: Acc=52%, Loss=0.7007968425750732\n",
            "Iteration: 322080 \t Train: Acc=50%, Loss=0.6913440823554993 \t\t Validation: Acc=49%, Loss=0.7019322514533997\n",
            "Iteration: 322090 \t Train: Acc=47%, Loss=0.6979928016662598 \t\t Validation: Acc=48%, Loss=0.708016574382782\n",
            "Iteration: 322100 \t Train: Acc=51%, Loss=0.6919286251068115 \t\t Validation: Acc=53%, Loss=0.698671817779541\n",
            "Iteration: 322110 \t Train: Acc=53%, Loss=0.6852234601974487 \t\t Validation: Acc=46%, Loss=0.6860267519950867\n",
            "Iteration: 322120 \t Train: Acc=53%, Loss=0.6853287220001221 \t\t Validation: Acc=53%, Loss=0.687983512878418\n",
            "Iteration: 322130 \t Train: Acc=50%, Loss=0.6926513910293579 \t\t Validation: Acc=50%, Loss=0.6889469027519226\n",
            "Iteration: 322140 \t Train: Acc=52%, Loss=0.6866697072982788 \t\t Validation: Acc=52%, Loss=0.6905880570411682\n",
            "Iteration: 322150 \t Train: Acc=48%, Loss=0.6876587867736816 \t\t Validation: Acc=50%, Loss=0.7013813853263855\n",
            "Iteration: 322160 \t Train: Acc=52%, Loss=0.6901916265487671 \t\t Validation: Acc=49%, Loss=0.7025465369224548\n",
            "Iteration: 322170 \t Train: Acc=50%, Loss=0.6977947354316711 \t\t Validation: Acc=48%, Loss=0.687584400177002\n",
            "Iteration: 322180 \t Train: Acc=46%, Loss=0.698508083820343 \t\t Validation: Acc=46%, Loss=0.6988996863365173\n",
            "Iteration: 322190 \t Train: Acc=50%, Loss=0.690132737159729 \t\t Validation: Acc=46%, Loss=0.7058240175247192\n",
            "Iteration: 322200 \t Train: Acc=50%, Loss=0.6913629174232483 \t\t Validation: Acc=51%, Loss=0.6879797577857971\n",
            "Iteration: 322210 \t Train: Acc=55%, Loss=0.6929804682731628 \t\t Validation: Acc=51%, Loss=0.6965847015380859\n",
            "Iteration: 322220 \t Train: Acc=52%, Loss=0.6911768913269043 \t\t Validation: Acc=53%, Loss=0.6834008693695068\n",
            "Iteration: 322230 \t Train: Acc=48%, Loss=0.6917742490768433 \t\t Validation: Acc=50%, Loss=0.7012045383453369\n",
            "Iteration: 322240 \t Train: Acc=53%, Loss=0.6922973394393921 \t\t Validation: Acc=48%, Loss=0.7061081528663635\n",
            "Iteration: 322250 \t Train: Acc=45%, Loss=0.7022711634635925 \t\t Validation: Acc=50%, Loss=0.7024596333503723\n",
            "Iteration: 322260 \t Train: Acc=51%, Loss=0.6829656362533569 \t\t Validation: Acc=52%, Loss=0.693280816078186\n",
            "Iteration: 322270 \t Train: Acc=53%, Loss=0.6901644468307495 \t\t Validation: Acc=50%, Loss=0.6943399906158447\n",
            "Iteration: 322280 \t Train: Acc=55%, Loss=0.6899024248123169 \t\t Validation: Acc=50%, Loss=0.688833475112915\n",
            "Iteration: 322290 \t Train: Acc=51%, Loss=0.6949674487113953 \t\t Validation: Acc=53%, Loss=0.6891965270042419\n",
            "Iteration: 322300 \t Train: Acc=52%, Loss=0.6944820880889893 \t\t Validation: Acc=51%, Loss=0.6875080466270447\n",
            "Iteration: 322310 \t Train: Acc=55%, Loss=0.689212441444397 \t\t Validation: Acc=49%, Loss=0.7062687873840332\n",
            "Iteration: 322320 \t Train: Acc=46%, Loss=0.6969857811927795 \t\t Validation: Acc=50%, Loss=0.6924557685852051\n",
            "Iteration: 322330 \t Train: Acc=53%, Loss=0.6898804903030396 \t\t Validation: Acc=51%, Loss=0.701978862285614\n",
            "Iteration: 322340 \t Train: Acc=54%, Loss=0.6831773519515991 \t\t Validation: Acc=50%, Loss=0.697798490524292\n",
            "Iteration: 322350 \t Train: Acc=51%, Loss=0.7004576325416565 \t\t Validation: Acc=50%, Loss=0.6986982822418213\n",
            "Iteration: 322360 \t Train: Acc=48%, Loss=0.6946239471435547 \t\t Validation: Acc=51%, Loss=0.7078372836112976\n",
            "Iteration: 322370 \t Train: Acc=52%, Loss=0.6884161233901978 \t\t Validation: Acc=53%, Loss=0.6976306438446045\n",
            "Iteration: 322380 \t Train: Acc=50%, Loss=0.6871735453605652 \t\t Validation: Acc=49%, Loss=0.6993875503540039\n",
            "Iteration: 322390 \t Train: Acc=55%, Loss=0.678159236907959 \t\t Validation: Acc=53%, Loss=0.7008041143417358\n",
            "Iteration: 322400 \t Train: Acc=46%, Loss=0.6911625862121582 \t\t Validation: Acc=52%, Loss=0.6947118043899536\n",
            "Iteration: 322410 \t Train: Acc=47%, Loss=0.6897248029708862 \t\t Validation: Acc=48%, Loss=0.701134204864502\n",
            "Iteration: 322420 \t Train: Acc=50%, Loss=0.6918407678604126 \t\t Validation: Acc=50%, Loss=0.700356662273407\n",
            "Iteration: 322430 \t Train: Acc=48%, Loss=0.6916004419326782 \t\t Validation: Acc=53%, Loss=0.6984428763389587\n",
            "Iteration: 322440 \t Train: Acc=50%, Loss=0.6915822625160217 \t\t Validation: Acc=47%, Loss=0.6891512870788574\n",
            "Iteration: 322450 \t Train: Acc=52%, Loss=0.688155472278595 \t\t Validation: Acc=48%, Loss=0.6975788474082947\n",
            "Iteration: 322460 \t Train: Acc=51%, Loss=0.6902691125869751 \t\t Validation: Acc=46%, Loss=0.7094002962112427\n",
            "Iteration: 322470 \t Train: Acc=50%, Loss=0.6929511427879333 \t\t Validation: Acc=46%, Loss=0.7004127502441406\n",
            "Iteration: 322480 \t Train: Acc=49%, Loss=0.6971719264984131 \t\t Validation: Acc=52%, Loss=0.6837394833564758\n",
            "Iteration: 322490 \t Train: Acc=49%, Loss=0.7060287594795227 \t\t Validation: Acc=52%, Loss=0.6877359747886658\n",
            "Iteration: 322500 \t Train: Acc=53%, Loss=0.6969014406204224 \t\t Validation: Acc=56%, Loss=0.6843776702880859\n",
            "Iteration: 322510 \t Train: Acc=53%, Loss=0.6935261487960815 \t\t Validation: Acc=53%, Loss=0.6883002519607544\n",
            "Iteration: 322520 \t Train: Acc=53%, Loss=0.6893442869186401 \t\t Validation: Acc=51%, Loss=0.6913608908653259\n",
            "Iteration: 322530 \t Train: Acc=49%, Loss=0.689958393573761 \t\t Validation: Acc=49%, Loss=0.7050119638442993\n",
            "Iteration: 322540 \t Train: Acc=50%, Loss=0.6903272867202759 \t\t Validation: Acc=49%, Loss=0.7001276016235352\n",
            "Iteration: 322550 \t Train: Acc=53%, Loss=0.6892846822738647 \t\t Validation: Acc=45%, Loss=0.7067707777023315\n",
            "Iteration: 322560 \t Train: Acc=53%, Loss=0.6898489594459534 \t\t Validation: Acc=48%, Loss=0.6900216937065125\n",
            "Iteration: 322570 \t Train: Acc=50%, Loss=0.6879047751426697 \t\t Validation: Acc=48%, Loss=0.6976065635681152\n",
            "Iteration: 322580 \t Train: Acc=50%, Loss=0.6975699067115784 \t\t Validation: Acc=47%, Loss=0.6960147023200989\n",
            "Iteration: 322590 \t Train: Acc=49%, Loss=0.6988534927368164 \t\t Validation: Acc=50%, Loss=0.7025784850120544\n",
            "Iteration: 322600 \t Train: Acc=53%, Loss=0.6947000026702881 \t\t Validation: Acc=49%, Loss=0.6947565078735352\n",
            "Iteration: 322610 \t Train: Acc=48%, Loss=0.6948235630989075 \t\t Validation: Acc=46%, Loss=0.6974350214004517\n",
            "Iteration: 322620 \t Train: Acc=49%, Loss=0.6933114528656006 \t\t Validation: Acc=53%, Loss=0.6912330389022827\n",
            "Iteration: 322630 \t Train: Acc=50%, Loss=0.6925109624862671 \t\t Validation: Acc=53%, Loss=0.6847115755081177\n",
            "Iteration: 322640 \t Train: Acc=55%, Loss=0.6901346445083618 \t\t Validation: Acc=57%, Loss=0.6958961486816406\n",
            "Iteration: 322650 \t Train: Acc=53%, Loss=0.6860571503639221 \t\t Validation: Acc=48%, Loss=0.6978814005851746\n",
            "Iteration: 322660 \t Train: Acc=53%, Loss=0.6840318441390991 \t\t Validation: Acc=50%, Loss=0.7043818235397339\n",
            "Iteration: 322670 \t Train: Acc=52%, Loss=0.6874918341636658 \t\t Validation: Acc=50%, Loss=0.6856770515441895\n",
            "Iteration: 322680 \t Train: Acc=50%, Loss=0.6991689205169678 \t\t Validation: Acc=50%, Loss=0.6871253848075867\n",
            "Iteration: 322690 \t Train: Acc=53%, Loss=0.687926173210144 \t\t Validation: Acc=52%, Loss=0.7037646174430847\n",
            "Iteration: 322700 \t Train: Acc=55%, Loss=0.6863022446632385 \t\t Validation: Acc=46%, Loss=0.7007007002830505\n",
            "Iteration: 322710 \t Train: Acc=49%, Loss=0.6919285655021667 \t\t Validation: Acc=52%, Loss=0.687899649143219\n",
            "Iteration: 322720 \t Train: Acc=46%, Loss=0.6974043846130371 \t\t Validation: Acc=53%, Loss=0.6824037432670593\n",
            "Iteration: 322730 \t Train: Acc=50%, Loss=0.6929616928100586 \t\t Validation: Acc=50%, Loss=0.6902040839195251\n",
            "Iteration: 322740 \t Train: Acc=56%, Loss=0.6822291612625122 \t\t Validation: Acc=46%, Loss=0.7071768641471863\n",
            "Iteration: 322750 \t Train: Acc=49%, Loss=0.6896570920944214 \t\t Validation: Acc=53%, Loss=0.6859236359596252\n",
            "Iteration: 322760 \t Train: Acc=54%, Loss=0.6872654557228088 \t\t Validation: Acc=50%, Loss=0.7007265090942383\n",
            "Iteration: 322770 \t Train: Acc=48%, Loss=0.6925926208496094 \t\t Validation: Acc=47%, Loss=0.7096360921859741\n",
            "Iteration: 322780 \t Train: Acc=53%, Loss=0.693020224571228 \t\t Validation: Acc=53%, Loss=0.691275954246521\n",
            "Iteration: 322790 \t Train: Acc=48%, Loss=0.6942278146743774 \t\t Validation: Acc=45%, Loss=0.7057507038116455\n",
            "Iteration: 322800 \t Train: Acc=52%, Loss=0.6890493631362915 \t\t Validation: Acc=49%, Loss=0.6895466446876526\n",
            "Iteration: 322810 \t Train: Acc=48%, Loss=0.6986284255981445 \t\t Validation: Acc=50%, Loss=0.6899497509002686\n",
            "Iteration: 322820 \t Train: Acc=49%, Loss=0.6965007781982422 \t\t Validation: Acc=47%, Loss=0.6922435760498047\n",
            "Iteration: 322830 \t Train: Acc=50%, Loss=0.6925370693206787 \t\t Validation: Acc=53%, Loss=0.6898078322410583\n",
            "Iteration: 322840 \t Train: Acc=50%, Loss=0.6893471479415894 \t\t Validation: Acc=53%, Loss=0.6820074319839478\n",
            "Iteration: 322850 \t Train: Acc=54%, Loss=0.6876805424690247 \t\t Validation: Acc=53%, Loss=0.6937291026115417\n",
            "Iteration: 322860 \t Train: Acc=51%, Loss=0.695109486579895 \t\t Validation: Acc=54%, Loss=0.6993672251701355\n",
            "Iteration: 322870 \t Train: Acc=55%, Loss=0.689883291721344 \t\t Validation: Acc=50%, Loss=0.6902974247932434\n",
            "Iteration: 322880 \t Train: Acc=53%, Loss=0.6893682479858398 \t\t Validation: Acc=53%, Loss=0.6913966536521912\n",
            "Iteration: 322890 \t Train: Acc=51%, Loss=0.6878321766853333 \t\t Validation: Acc=50%, Loss=0.6952624320983887\n",
            "Iteration: 322900 \t Train: Acc=50%, Loss=0.6945855617523193 \t\t Validation: Acc=52%, Loss=0.6963817477226257\n",
            "Iteration: 322910 \t Train: Acc=51%, Loss=0.6983808279037476 \t\t Validation: Acc=49%, Loss=0.6868065595626831\n",
            "Iteration: 322920 \t Train: Acc=48%, Loss=0.692995011806488 \t\t Validation: Acc=48%, Loss=0.6935824155807495\n",
            "Iteration: 322930 \t Train: Acc=46%, Loss=0.6985653042793274 \t\t Validation: Acc=46%, Loss=0.6935777068138123\n",
            "Iteration: 322940 \t Train: Acc=49%, Loss=0.69168621301651 \t\t Validation: Acc=50%, Loss=0.6932996511459351\n",
            "Iteration: 322950 \t Train: Acc=53%, Loss=0.6898455023765564 \t\t Validation: Acc=50%, Loss=0.6915873885154724\n",
            "Iteration: 322960 \t Train: Acc=53%, Loss=0.6976863145828247 \t\t Validation: Acc=53%, Loss=0.6939351558685303\n",
            "Iteration: 322970 \t Train: Acc=55%, Loss=0.6914332509040833 \t\t Validation: Acc=53%, Loss=0.6899154186248779\n",
            "Iteration: 322980 \t Train: Acc=52%, Loss=0.6880974769592285 \t\t Validation: Acc=50%, Loss=0.6917126178741455\n",
            "Iteration: 322990 \t Train: Acc=51%, Loss=0.6883140802383423 \t\t Validation: Acc=53%, Loss=0.6844470500946045\n",
            "Iteration: 323000 \t Train: Acc=53%, Loss=0.6983984708786011 \t\t Validation: Acc=52%, Loss=0.6866996884346008\n",
            "Iteration: 323010 \t Train: Acc=51%, Loss=0.6892731189727783 \t\t Validation: Acc=55%, Loss=0.6898998618125916\n",
            "Iteration: 323020 \t Train: Acc=46%, Loss=0.6991315484046936 \t\t Validation: Acc=53%, Loss=0.6889854669570923\n",
            "Iteration: 323030 \t Train: Acc=52%, Loss=0.6886342167854309 \t\t Validation: Acc=52%, Loss=0.6954852342605591\n",
            "Iteration: 323040 \t Train: Acc=46%, Loss=0.693208634853363 \t\t Validation: Acc=51%, Loss=0.6968262195587158\n",
            "Iteration: 323050 \t Train: Acc=57%, Loss=0.685122549533844 \t\t Validation: Acc=47%, Loss=0.6914081573486328\n",
            "Iteration: 323060 \t Train: Acc=52%, Loss=0.6895726323127747 \t\t Validation: Acc=50%, Loss=0.6905997395515442\n",
            "Iteration: 323070 \t Train: Acc=47%, Loss=0.6989758014678955 \t\t Validation: Acc=46%, Loss=0.6934764385223389\n",
            "Iteration: 323080 \t Train: Acc=52%, Loss=0.6850980520248413 \t\t Validation: Acc=50%, Loss=0.6912928223609924\n",
            "Iteration: 323090 \t Train: Acc=50%, Loss=0.6899662613868713 \t\t Validation: Acc=53%, Loss=0.6938481330871582\n",
            "Iteration: 323100 \t Train: Acc=53%, Loss=0.6913047432899475 \t\t Validation: Acc=48%, Loss=0.6971412897109985\n",
            "Iteration: 323110 \t Train: Acc=50%, Loss=0.6897442936897278 \t\t Validation: Acc=52%, Loss=0.7050729393959045\n",
            "Iteration: 323120 \t Train: Acc=56%, Loss=0.6871723532676697 \t\t Validation: Acc=50%, Loss=0.6868994235992432\n",
            "Iteration: 323130 \t Train: Acc=49%, Loss=0.6917586326599121 \t\t Validation: Acc=50%, Loss=0.6896296739578247\n",
            "Iteration: 323140 \t Train: Acc=48%, Loss=0.6937477588653564 \t\t Validation: Acc=49%, Loss=0.6931551098823547\n",
            "Iteration: 323150 \t Train: Acc=52%, Loss=0.6907154321670532 \t\t Validation: Acc=46%, Loss=0.6881796717643738\n",
            "Iteration: 323160 \t Train: Acc=53%, Loss=0.6865558624267578 \t\t Validation: Acc=48%, Loss=0.6950058937072754\n",
            "Iteration: 323170 \t Train: Acc=53%, Loss=0.6872668266296387 \t\t Validation: Acc=50%, Loss=0.6904929876327515\n",
            "Iteration: 323180 \t Train: Acc=50%, Loss=0.6926143169403076 \t\t Validation: Acc=49%, Loss=0.6941428780555725\n",
            "Iteration: 323190 \t Train: Acc=50%, Loss=0.6859649419784546 \t\t Validation: Acc=50%, Loss=0.6929091215133667\n",
            "Iteration: 323200 \t Train: Acc=52%, Loss=0.688382089138031 \t\t Validation: Acc=49%, Loss=0.6909835338592529\n",
            "Iteration: 323210 \t Train: Acc=53%, Loss=0.6892322897911072 \t\t Validation: Acc=54%, Loss=0.6927109956741333\n",
            "Iteration: 323220 \t Train: Acc=55%, Loss=0.6909407377243042 \t\t Validation: Acc=49%, Loss=0.6986084580421448\n",
            "Iteration: 323230 \t Train: Acc=51%, Loss=0.702541708946228 \t\t Validation: Acc=51%, Loss=0.686332643032074\n",
            "Iteration: 323240 \t Train: Acc=51%, Loss=0.6900350451469421 \t\t Validation: Acc=55%, Loss=0.6883592009544373\n",
            "Iteration: 323250 \t Train: Acc=54%, Loss=0.68899005651474 \t\t Validation: Acc=53%, Loss=0.6988876461982727\n",
            "Iteration: 323260 \t Train: Acc=46%, Loss=0.6907269358634949 \t\t Validation: Acc=51%, Loss=0.690372884273529\n",
            "Iteration: 323270 \t Train: Acc=47%, Loss=0.6911184787750244 \t\t Validation: Acc=53%, Loss=0.6870658993721008\n",
            "Iteration: 323280 \t Train: Acc=49%, Loss=0.6977685689926147 \t\t Validation: Acc=51%, Loss=0.6887858510017395\n",
            "Iteration: 323290 \t Train: Acc=49%, Loss=0.6945664286613464 \t\t Validation: Acc=57%, Loss=0.6852270364761353\n",
            "Iteration: 323300 \t Train: Acc=53%, Loss=0.6929933428764343 \t\t Validation: Acc=47%, Loss=0.6961395740509033\n",
            "Iteration: 323310 \t Train: Acc=53%, Loss=0.6955598592758179 \t\t Validation: Acc=54%, Loss=0.6910587549209595\n",
            "Iteration: 323320 \t Train: Acc=54%, Loss=0.6904280781745911 \t\t Validation: Acc=49%, Loss=0.6867232918739319\n",
            "Iteration: 323330 \t Train: Acc=60%, Loss=0.6864204406738281 \t\t Validation: Acc=50%, Loss=0.6947696805000305\n",
            "Iteration: 323340 \t Train: Acc=54%, Loss=0.6909363269805908 \t\t Validation: Acc=50%, Loss=0.6895464062690735\n",
            "Iteration: 323350 \t Train: Acc=50%, Loss=0.6990649700164795 \t\t Validation: Acc=46%, Loss=0.6981154680252075\n",
            "Iteration: 323360 \t Train: Acc=46%, Loss=0.6946855187416077 \t\t Validation: Acc=45%, Loss=0.6877250671386719\n",
            "Iteration: 323370 \t Train: Acc=47%, Loss=0.6953657269477844 \t\t Validation: Acc=47%, Loss=0.6943408250808716\n",
            "Iteration: 323380 \t Train: Acc=48%, Loss=0.692704439163208 \t\t Validation: Acc=50%, Loss=0.6922923922538757\n",
            "Iteration: 323390 \t Train: Acc=50%, Loss=0.6939725875854492 \t\t Validation: Acc=53%, Loss=0.6975893378257751\n",
            "Iteration: 323400 \t Train: Acc=50%, Loss=0.6920693516731262 \t\t Validation: Acc=52%, Loss=0.6995388865470886\n",
            "Iteration: 323410 \t Train: Acc=53%, Loss=0.686616063117981 \t\t Validation: Acc=51%, Loss=0.6892296075820923\n",
            "Iteration: 323420 \t Train: Acc=46%, Loss=0.691516637802124 \t\t Validation: Acc=46%, Loss=0.6958240866661072\n",
            "Iteration: 323430 \t Train: Acc=54%, Loss=0.6889669299125671 \t\t Validation: Acc=50%, Loss=0.6969262361526489\n",
            "Iteration: 323440 \t Train: Acc=48%, Loss=0.6960994005203247 \t\t Validation: Acc=52%, Loss=0.6792998313903809\n",
            "Iteration: 323450 \t Train: Acc=48%, Loss=0.6976260542869568 \t\t Validation: Acc=50%, Loss=0.6973060965538025\n",
            "Iteration: 323460 \t Train: Acc=45%, Loss=0.7009910941123962 \t\t Validation: Acc=50%, Loss=0.693364679813385\n",
            "Iteration: 323470 \t Train: Acc=50%, Loss=0.6949754357337952 \t\t Validation: Acc=50%, Loss=0.6940621137619019\n",
            "Iteration: 323480 \t Train: Acc=45%, Loss=0.691700279712677 \t\t Validation: Acc=48%, Loss=0.6984347105026245\n",
            "Iteration: 323490 \t Train: Acc=50%, Loss=0.6941819190979004 \t\t Validation: Acc=49%, Loss=0.7015884518623352\n",
            "Iteration: 323500 \t Train: Acc=54%, Loss=0.6886082291603088 \t\t Validation: Acc=50%, Loss=0.6900563836097717\n",
            "Iteration: 323510 \t Train: Acc=53%, Loss=0.6973864436149597 \t\t Validation: Acc=49%, Loss=0.6985230445861816\n",
            "Iteration: 323520 \t Train: Acc=47%, Loss=0.6936046481132507 \t\t Validation: Acc=52%, Loss=0.6935788989067078\n",
            "Iteration: 323530 \t Train: Acc=52%, Loss=0.6901369094848633 \t\t Validation: Acc=47%, Loss=0.6962516903877258\n",
            "Iteration: 323540 \t Train: Acc=51%, Loss=0.6936317086219788 \t\t Validation: Acc=48%, Loss=0.6939855813980103\n",
            "Iteration: 323550 \t Train: Acc=46%, Loss=0.6955204010009766 \t\t Validation: Acc=51%, Loss=0.6885714530944824\n",
            "Iteration: 323560 \t Train: Acc=53%, Loss=0.6970109343528748 \t\t Validation: Acc=53%, Loss=0.6897332668304443\n",
            "Iteration: 323570 \t Train: Acc=52%, Loss=0.6961417198181152 \t\t Validation: Acc=47%, Loss=0.6981053352355957\n",
            "Iteration: 323580 \t Train: Acc=54%, Loss=0.6851930022239685 \t\t Validation: Acc=54%, Loss=0.6883881092071533\n",
            "Iteration: 323590 \t Train: Acc=49%, Loss=0.6965005397796631 \t\t Validation: Acc=55%, Loss=0.6970850825309753\n",
            "Iteration: 323600 \t Train: Acc=44%, Loss=0.6926758289337158 \t\t Validation: Acc=50%, Loss=0.6891727447509766\n",
            "Iteration: 323610 \t Train: Acc=52%, Loss=0.6973007321357727 \t\t Validation: Acc=50%, Loss=0.6935439109802246\n",
            "Iteration: 323620 \t Train: Acc=48%, Loss=0.6940925717353821 \t\t Validation: Acc=53%, Loss=0.6794003844261169\n",
            "Iteration: 323630 \t Train: Acc=45%, Loss=0.6934165954589844 \t\t Validation: Acc=49%, Loss=0.6953807473182678\n",
            "Iteration: 323640 \t Train: Acc=55%, Loss=0.6896312832832336 \t\t Validation: Acc=51%, Loss=0.694898247718811\n",
            "Iteration: 323650 \t Train: Acc=50%, Loss=0.6923040151596069 \t\t Validation: Acc=48%, Loss=0.7041282057762146\n",
            "Iteration: 323660 \t Train: Acc=45%, Loss=0.6986700296401978 \t\t Validation: Acc=49%, Loss=0.6965146660804749\n",
            "Iteration: 323670 \t Train: Acc=51%, Loss=0.6967366337776184 \t\t Validation: Acc=46%, Loss=0.6915097236633301\n",
            "Iteration: 323680 \t Train: Acc=49%, Loss=0.6980938911437988 \t\t Validation: Acc=51%, Loss=0.6890084147453308\n",
            "Iteration: 323690 \t Train: Acc=50%, Loss=0.6951898336410522 \t\t Validation: Acc=47%, Loss=0.6994266510009766\n",
            "Iteration: 323700 \t Train: Acc=51%, Loss=0.6898363828659058 \t\t Validation: Acc=48%, Loss=0.6860619783401489\n",
            "Iteration: 323710 \t Train: Acc=50%, Loss=0.6909231543540955 \t\t Validation: Acc=51%, Loss=0.689666211605072\n",
            "Iteration: 323720 \t Train: Acc=50%, Loss=0.6879631280899048 \t\t Validation: Acc=50%, Loss=0.695581316947937\n",
            "Iteration: 323730 \t Train: Acc=50%, Loss=0.7017927765846252 \t\t Validation: Acc=49%, Loss=0.695482075214386\n",
            "Iteration: 323740 \t Train: Acc=51%, Loss=0.6842474937438965 \t\t Validation: Acc=49%, Loss=0.7010773420333862\n",
            "Iteration: 323750 \t Train: Acc=53%, Loss=0.6886534690856934 \t\t Validation: Acc=53%, Loss=0.6929224729537964\n",
            "Iteration: 323760 \t Train: Acc=50%, Loss=0.6938354969024658 \t\t Validation: Acc=49%, Loss=0.6954892873764038\n",
            "Iteration: 323770 \t Train: Acc=47%, Loss=0.6950380802154541 \t\t Validation: Acc=48%, Loss=0.6974768042564392\n",
            "Iteration: 323780 \t Train: Acc=44%, Loss=0.6965740919113159 \t\t Validation: Acc=52%, Loss=0.691987156867981\n",
            "Iteration: 323790 \t Train: Acc=48%, Loss=0.6920357942581177 \t\t Validation: Acc=51%, Loss=0.6948334574699402\n",
            "Iteration: 323800 \t Train: Acc=50%, Loss=0.6940420866012573 \t\t Validation: Acc=57%, Loss=0.6897359490394592\n",
            "Iteration: 323810 \t Train: Acc=50%, Loss=0.6885761618614197 \t\t Validation: Acc=56%, Loss=0.6914821863174438\n",
            "Iteration: 323820 \t Train: Acc=55%, Loss=0.6808251738548279 \t\t Validation: Acc=50%, Loss=0.6869516372680664\n",
            "Iteration: 323830 \t Train: Acc=48%, Loss=0.6942253112792969 \t\t Validation: Acc=42%, Loss=0.6862908601760864\n",
            "Iteration: 323840 \t Train: Acc=53%, Loss=0.6883209943771362 \t\t Validation: Acc=53%, Loss=0.687086820602417\n",
            "Iteration: 323850 \t Train: Acc=54%, Loss=0.684569239616394 \t\t Validation: Acc=52%, Loss=0.6918131113052368\n",
            "Iteration: 323860 \t Train: Acc=53%, Loss=0.690167248249054 \t\t Validation: Acc=52%, Loss=0.6865091323852539\n",
            "Iteration: 323870 \t Train: Acc=51%, Loss=0.6933338046073914 \t\t Validation: Acc=46%, Loss=0.6957326531410217\n",
            "Iteration: 323880 \t Train: Acc=51%, Loss=0.6912089586257935 \t\t Validation: Acc=50%, Loss=0.6961629390716553\n",
            "Iteration: 323890 \t Train: Acc=52%, Loss=0.6961241364479065 \t\t Validation: Acc=54%, Loss=0.6950407028198242\n",
            "Iteration: 323900 \t Train: Acc=54%, Loss=0.6876924633979797 \t\t Validation: Acc=50%, Loss=0.6964437961578369\n",
            "Iteration: 323910 \t Train: Acc=54%, Loss=0.6890397667884827 \t\t Validation: Acc=50%, Loss=0.6951117515563965\n",
            "Iteration: 323920 \t Train: Acc=47%, Loss=0.6933580636978149 \t\t Validation: Acc=49%, Loss=0.6952930688858032\n",
            "Iteration: 323930 \t Train: Acc=50%, Loss=0.6899757385253906 \t\t Validation: Acc=52%, Loss=0.6944969892501831\n",
            "Iteration: 323940 \t Train: Acc=47%, Loss=0.6955405473709106 \t\t Validation: Acc=50%, Loss=0.6894078254699707\n",
            "Iteration: 323950 \t Train: Acc=54%, Loss=0.6897945404052734 \t\t Validation: Acc=54%, Loss=0.6888073086738586\n",
            "Iteration: 323960 \t Train: Acc=58%, Loss=0.6876846551895142 \t\t Validation: Acc=46%, Loss=0.6878267526626587\n",
            "Iteration: 323970 \t Train: Acc=46%, Loss=0.6966426372528076 \t\t Validation: Acc=56%, Loss=0.6850462555885315\n",
            "Iteration: 323980 \t Train: Acc=51%, Loss=0.6866409778594971 \t\t Validation: Acc=54%, Loss=0.6935434341430664\n",
            "Iteration: 323990 \t Train: Acc=52%, Loss=0.6911393404006958 \t\t Validation: Acc=50%, Loss=0.6970474123954773\n",
            "Iteration: 324000 \t Train: Acc=50%, Loss=0.6901359558105469 \t\t Validation: Acc=51%, Loss=0.693539559841156\n",
            "Iteration: 324010 \t Train: Acc=49%, Loss=0.6928331851959229 \t\t Validation: Acc=53%, Loss=0.6877403259277344\n",
            "Iteration: 324020 \t Train: Acc=50%, Loss=0.6972770690917969 \t\t Validation: Acc=57%, Loss=0.6836543679237366\n",
            "Iteration: 324030 \t Train: Acc=52%, Loss=0.6908323168754578 \t\t Validation: Acc=48%, Loss=0.7009627819061279\n",
            "Iteration: 324040 \t Train: Acc=50%, Loss=0.6945022940635681 \t\t Validation: Acc=53%, Loss=0.6856738328933716\n",
            "Iteration: 324050 \t Train: Acc=43%, Loss=0.6982804536819458 \t\t Validation: Acc=52%, Loss=0.691351056098938\n",
            "Iteration: 324060 \t Train: Acc=48%, Loss=0.6929994821548462 \t\t Validation: Acc=50%, Loss=0.6885507702827454\n",
            "Iteration: 324070 \t Train: Acc=48%, Loss=0.6918756365776062 \t\t Validation: Acc=53%, Loss=0.6893236637115479\n",
            "Iteration: 324080 \t Train: Acc=51%, Loss=0.6959882378578186 \t\t Validation: Acc=48%, Loss=0.6980525255203247\n",
            "Iteration: 324090 \t Train: Acc=47%, Loss=0.6989805698394775 \t\t Validation: Acc=50%, Loss=0.6920565962791443\n",
            "Iteration: 324100 \t Train: Acc=46%, Loss=0.6993874311447144 \t\t Validation: Acc=47%, Loss=0.6937402486801147\n",
            "Iteration: 324110 \t Train: Acc=51%, Loss=0.6960775256156921 \t\t Validation: Acc=51%, Loss=0.688197672367096\n",
            "Iteration: 324120 \t Train: Acc=50%, Loss=0.6988524794578552 \t\t Validation: Acc=51%, Loss=0.6923330426216125\n",
            "Iteration: 324130 \t Train: Acc=53%, Loss=0.6953428983688354 \t\t Validation: Acc=49%, Loss=0.6963394284248352\n",
            "Iteration: 324140 \t Train: Acc=50%, Loss=0.6900286674499512 \t\t Validation: Acc=50%, Loss=0.6962146759033203\n",
            "Iteration: 324150 \t Train: Acc=50%, Loss=0.6841205954551697 \t\t Validation: Acc=50%, Loss=0.6902551054954529\n",
            "Iteration: 324160 \t Train: Acc=48%, Loss=0.6917375326156616 \t\t Validation: Acc=53%, Loss=0.6878077983856201\n",
            "Iteration: 324170 \t Train: Acc=50%, Loss=0.689682126045227 \t\t Validation: Acc=49%, Loss=0.6969642639160156\n",
            "Iteration: 324180 \t Train: Acc=47%, Loss=0.6951137185096741 \t\t Validation: Acc=45%, Loss=0.6910735368728638\n",
            "Iteration: 324190 \t Train: Acc=51%, Loss=0.687022864818573 \t\t Validation: Acc=55%, Loss=0.675981879234314\n",
            "Iteration: 324200 \t Train: Acc=50%, Loss=0.6886703372001648 \t\t Validation: Acc=53%, Loss=0.6860215663909912\n",
            "Iteration: 324210 \t Train: Acc=53%, Loss=0.691737174987793 \t\t Validation: Acc=53%, Loss=0.6833999752998352\n",
            "Iteration: 324220 \t Train: Acc=54%, Loss=0.685901403427124 \t\t Validation: Acc=50%, Loss=0.6901166439056396\n",
            "Iteration: 324230 \t Train: Acc=53%, Loss=0.6944383382797241 \t\t Validation: Acc=50%, Loss=0.6966919898986816\n",
            "Iteration: 324240 \t Train: Acc=54%, Loss=0.6899005174636841 \t\t Validation: Acc=47%, Loss=0.6922783851623535\n",
            "Iteration: 324250 \t Train: Acc=59%, Loss=0.6815598607063293 \t\t Validation: Acc=48%, Loss=0.6974464654922485\n",
            "Iteration: 324260 \t Train: Acc=49%, Loss=0.6841632723808289 \t\t Validation: Acc=53%, Loss=0.6843605041503906\n",
            "Iteration: 324270 \t Train: Acc=50%, Loss=0.7003088593482971 \t\t Validation: Acc=51%, Loss=0.6953088641166687\n",
            "Iteration: 324280 \t Train: Acc=48%, Loss=0.6953991651535034 \t\t Validation: Acc=50%, Loss=0.697629451751709\n",
            "Iteration: 324290 \t Train: Acc=53%, Loss=0.6870801448822021 \t\t Validation: Acc=50%, Loss=0.6891797780990601\n",
            "Iteration: 324300 \t Train: Acc=54%, Loss=0.6901183128356934 \t\t Validation: Acc=50%, Loss=0.6937650442123413\n",
            "Iteration: 324310 \t Train: Acc=49%, Loss=0.6943763494491577 \t\t Validation: Acc=50%, Loss=0.6974149346351624\n",
            "Iteration: 324320 \t Train: Acc=54%, Loss=0.6890731453895569 \t\t Validation: Acc=50%, Loss=0.6975675225257874\n",
            "Iteration: 324330 \t Train: Acc=53%, Loss=0.6847720146179199 \t\t Validation: Acc=50%, Loss=0.6899828910827637\n",
            "Iteration: 324340 \t Train: Acc=51%, Loss=0.6890795230865479 \t\t Validation: Acc=50%, Loss=0.6953186988830566\n",
            "Iteration: 324350 \t Train: Acc=50%, Loss=0.690517008304596 \t\t Validation: Acc=49%, Loss=0.6978217959403992\n",
            "Iteration: 324360 \t Train: Acc=46%, Loss=0.7027708888053894 \t\t Validation: Acc=50%, Loss=0.6940441727638245\n",
            "Iteration: 324370 \t Train: Acc=51%, Loss=0.6896229982376099 \t\t Validation: Acc=53%, Loss=0.6818810105323792\n",
            "Iteration: 324380 \t Train: Acc=48%, Loss=0.6924657821655273 \t\t Validation: Acc=50%, Loss=0.6906774640083313\n",
            "Iteration: 324390 \t Train: Acc=50%, Loss=0.7019015550613403 \t\t Validation: Acc=55%, Loss=0.6892270445823669\n",
            "Iteration: 324400 \t Train: Acc=53%, Loss=0.689976155757904 \t\t Validation: Acc=51%, Loss=0.6900181770324707\n",
            "Iteration: 324410 \t Train: Acc=47%, Loss=0.6917021870613098 \t\t Validation: Acc=50%, Loss=0.6972981691360474\n",
            "Iteration: 324420 \t Train: Acc=44%, Loss=0.7005020380020142 \t\t Validation: Acc=46%, Loss=0.6928587555885315\n",
            "Iteration: 324430 \t Train: Acc=50%, Loss=0.6919044852256775 \t\t Validation: Acc=50%, Loss=0.6902133822441101\n",
            "Iteration: 324440 \t Train: Acc=50%, Loss=0.6951819658279419 \t\t Validation: Acc=48%, Loss=0.6946759223937988\n",
            "Iteration: 324450 \t Train: Acc=51%, Loss=0.6883093118667603 \t\t Validation: Acc=53%, Loss=0.6865548491477966\n",
            "Iteration: 324460 \t Train: Acc=53%, Loss=0.6855282783508301 \t\t Validation: Acc=50%, Loss=0.6905341148376465\n",
            "Iteration: 324470 \t Train: Acc=51%, Loss=0.6979158520698547 \t\t Validation: Acc=51%, Loss=0.6901059746742249\n",
            "Iteration: 324480 \t Train: Acc=53%, Loss=0.6902104616165161 \t\t Validation: Acc=51%, Loss=0.689981997013092\n",
            "Iteration: 324490 \t Train: Acc=50%, Loss=0.6958643794059753 \t\t Validation: Acc=52%, Loss=0.6907937526702881\n",
            "Iteration: 324500 \t Train: Acc=53%, Loss=0.6860694885253906 \t\t Validation: Acc=50%, Loss=0.6927674412727356\n",
            "Iteration: 324510 \t Train: Acc=49%, Loss=0.6910591125488281 \t\t Validation: Acc=50%, Loss=0.6905750036239624\n",
            "Iteration: 324520 \t Train: Acc=53%, Loss=0.6861599087715149 \t\t Validation: Acc=47%, Loss=0.6906114816665649\n",
            "Iteration: 324530 \t Train: Acc=45%, Loss=0.6972358226776123 \t\t Validation: Acc=50%, Loss=0.6909999847412109\n",
            "Iteration: 324540 \t Train: Acc=50%, Loss=0.6920448541641235 \t\t Validation: Acc=47%, Loss=0.6919872164726257\n",
            "Iteration: 324550 \t Train: Acc=50%, Loss=0.6874313950538635 \t\t Validation: Acc=50%, Loss=0.6879209280014038\n",
            "Iteration: 324560 \t Train: Acc=46%, Loss=0.691643476486206 \t\t Validation: Acc=53%, Loss=0.6916781663894653\n",
            "Iteration: 324570 \t Train: Acc=47%, Loss=0.6933945417404175 \t\t Validation: Acc=51%, Loss=0.6980775594711304\n",
            "Iteration: 324580 \t Train: Acc=53%, Loss=0.6886518597602844 \t\t Validation: Acc=46%, Loss=0.6970482468605042\n",
            "Iteration: 324590 \t Train: Acc=57%, Loss=0.6846531629562378 \t\t Validation: Acc=50%, Loss=0.7009719610214233\n",
            "Iteration: 324600 \t Train: Acc=53%, Loss=0.6975216269493103 \t\t Validation: Acc=49%, Loss=0.6958227157592773\n",
            "Iteration: 324610 \t Train: Acc=51%, Loss=0.6934648752212524 \t\t Validation: Acc=54%, Loss=0.687299907207489\n",
            "Iteration: 324620 \t Train: Acc=53%, Loss=0.6913036108016968 \t\t Validation: Acc=47%, Loss=0.7014906406402588\n",
            "Iteration: 324630 \t Train: Acc=50%, Loss=0.6954485774040222 \t\t Validation: Acc=47%, Loss=0.6916908621788025\n",
            "Iteration: 324640 \t Train: Acc=52%, Loss=0.6852148771286011 \t\t Validation: Acc=50%, Loss=0.6872926354408264\n",
            "Iteration: 324650 \t Train: Acc=54%, Loss=0.6862752437591553 \t\t Validation: Acc=53%, Loss=0.686468243598938\n",
            "Iteration: 324660 \t Train: Acc=56%, Loss=0.6842396855354309 \t\t Validation: Acc=49%, Loss=0.6977124810218811\n",
            "Iteration: 324670 \t Train: Acc=54%, Loss=0.6913666129112244 \t\t Validation: Acc=49%, Loss=0.6949835419654846\n",
            "Iteration: 324680 \t Train: Acc=47%, Loss=0.6947833895683289 \t\t Validation: Acc=51%, Loss=0.6872991323471069\n",
            "Iteration: 324690 \t Train: Acc=46%, Loss=0.6961835026741028 \t\t Validation: Acc=45%, Loss=0.6980745792388916\n",
            "Iteration: 324700 \t Train: Acc=50%, Loss=0.702879011631012 \t\t Validation: Acc=53%, Loss=0.6928553581237793\n",
            "Iteration: 324710 \t Train: Acc=50%, Loss=0.6949357390403748 \t\t Validation: Acc=47%, Loss=0.697698712348938\n",
            "Iteration: 324720 \t Train: Acc=50%, Loss=0.6890600323677063 \t\t Validation: Acc=51%, Loss=0.6924654245376587\n",
            "Iteration: 324730 \t Train: Acc=53%, Loss=0.6977654695510864 \t\t Validation: Acc=46%, Loss=0.699451208114624\n",
            "Iteration: 324740 \t Train: Acc=49%, Loss=0.6911758780479431 \t\t Validation: Acc=53%, Loss=0.6944904923439026\n",
            "Iteration: 324750 \t Train: Acc=58%, Loss=0.6848183870315552 \t\t Validation: Acc=51%, Loss=0.6869843006134033\n",
            "Iteration: 324760 \t Train: Acc=54%, Loss=0.6883025765419006 \t\t Validation: Acc=48%, Loss=0.6930325031280518\n",
            "Iteration: 324770 \t Train: Acc=50%, Loss=0.6998569965362549 \t\t Validation: Acc=50%, Loss=0.6971395611763\n",
            "Iteration: 324780 \t Train: Acc=48%, Loss=0.696100652217865 \t\t Validation: Acc=51%, Loss=0.6873877048492432\n",
            "Iteration: 324790 \t Train: Acc=50%, Loss=0.6891489624977112 \t\t Validation: Acc=53%, Loss=0.6914793848991394\n",
            "Iteration: 324800 \t Train: Acc=50%, Loss=0.6914172172546387 \t\t Validation: Acc=50%, Loss=0.6924383640289307\n",
            "Iteration: 324810 \t Train: Acc=53%, Loss=0.6867470145225525 \t\t Validation: Acc=53%, Loss=0.6912024021148682\n",
            "Iteration: 324820 \t Train: Acc=49%, Loss=0.696551501750946 \t\t Validation: Acc=50%, Loss=0.6967769861221313\n",
            "Iteration: 324830 \t Train: Acc=46%, Loss=0.693657398223877 \t\t Validation: Acc=53%, Loss=0.6871123313903809\n",
            "Iteration: 324840 \t Train: Acc=44%, Loss=0.6940430998802185 \t\t Validation: Acc=54%, Loss=0.6880821585655212\n",
            "Iteration: 324850 \t Train: Acc=57%, Loss=0.6873477101325989 \t\t Validation: Acc=49%, Loss=0.6921129822731018\n",
            "Iteration: 324860 \t Train: Acc=56%, Loss=0.689935564994812 \t\t Validation: Acc=53%, Loss=0.6784347891807556\n",
            "Iteration: 324870 \t Train: Acc=47%, Loss=0.697746992111206 \t\t Validation: Acc=50%, Loss=0.6921382546424866\n",
            "Iteration: 324880 \t Train: Acc=50%, Loss=0.6884384155273438 \t\t Validation: Acc=46%, Loss=0.6945478320121765\n",
            "Iteration: 324890 \t Train: Acc=53%, Loss=0.6882472038269043 \t\t Validation: Acc=48%, Loss=0.7062378525733948\n",
            "Iteration: 324900 \t Train: Acc=52%, Loss=0.6920663118362427 \t\t Validation: Acc=50%, Loss=0.6965087652206421\n",
            "Iteration: 324910 \t Train: Acc=46%, Loss=0.7003094553947449 \t\t Validation: Acc=52%, Loss=0.6941299438476562\n",
            "Iteration: 324920 \t Train: Acc=46%, Loss=0.6953436136245728 \t\t Validation: Acc=50%, Loss=0.6957091093063354\n",
            "Iteration: 324930 \t Train: Acc=45%, Loss=0.6966040134429932 \t\t Validation: Acc=51%, Loss=0.6978590488433838\n",
            "Iteration: 324940 \t Train: Acc=45%, Loss=0.6943530440330505 \t\t Validation: Acc=49%, Loss=0.694068431854248\n",
            "Iteration: 324950 \t Train: Acc=55%, Loss=0.6962233185768127 \t\t Validation: Acc=55%, Loss=0.6872775554656982\n",
            "Iteration: 324960 \t Train: Acc=43%, Loss=0.6943838596343994 \t\t Validation: Acc=48%, Loss=0.6989405751228333\n",
            "Iteration: 324970 \t Train: Acc=50%, Loss=0.6922125220298767 \t\t Validation: Acc=48%, Loss=0.6987918615341187\n",
            "Iteration: 324980 \t Train: Acc=52%, Loss=0.6943848729133606 \t\t Validation: Acc=50%, Loss=0.695494532585144\n",
            "Iteration: 324990 \t Train: Acc=48%, Loss=0.6934911012649536 \t\t Validation: Acc=46%, Loss=0.6969878077507019\n",
            "Iteration: 325000 \t Train: Acc=48%, Loss=0.6967121362686157 \t\t Validation: Acc=49%, Loss=0.694039523601532\n",
            "Iteration: 325010 \t Train: Acc=44%, Loss=0.6990599036216736 \t\t Validation: Acc=51%, Loss=0.6853141784667969\n",
            "Iteration: 325020 \t Train: Acc=61%, Loss=0.6905351281166077 \t\t Validation: Acc=48%, Loss=0.7003806829452515\n",
            "Iteration: 325030 \t Train: Acc=55%, Loss=0.6899811625480652 \t\t Validation: Acc=48%, Loss=0.6896598935127258\n",
            "Iteration: 325040 \t Train: Acc=54%, Loss=0.6890921592712402 \t\t Validation: Acc=50%, Loss=0.6898157000541687\n",
            "Iteration: 325050 \t Train: Acc=49%, Loss=0.6962769031524658 \t\t Validation: Acc=51%, Loss=0.6900607943534851\n",
            "Iteration: 325060 \t Train: Acc=51%, Loss=0.6883273720741272 \t\t Validation: Acc=47%, Loss=0.6949084997177124\n",
            "Iteration: 325070 \t Train: Acc=50%, Loss=0.6836779117584229 \t\t Validation: Acc=50%, Loss=0.6933284997940063\n",
            "Iteration: 325080 \t Train: Acc=47%, Loss=0.6888353824615479 \t\t Validation: Acc=49%, Loss=0.6916556358337402\n",
            "Iteration: 325090 \t Train: Acc=46%, Loss=0.6976157426834106 \t\t Validation: Acc=57%, Loss=0.6816869974136353\n",
            "Iteration: 325100 \t Train: Acc=50%, Loss=0.6910800933837891 \t\t Validation: Acc=49%, Loss=0.6938326358795166\n",
            "Iteration: 325110 \t Train: Acc=54%, Loss=0.6894308924674988 \t\t Validation: Acc=51%, Loss=0.691196858882904\n",
            "Iteration: 325120 \t Train: Acc=50%, Loss=0.6924384236335754 \t\t Validation: Acc=50%, Loss=0.6959213614463806\n",
            "Iteration: 325130 \t Train: Acc=55%, Loss=0.6877927780151367 \t\t Validation: Acc=47%, Loss=0.68773353099823\n",
            "Iteration: 325140 \t Train: Acc=52%, Loss=0.6870222687721252 \t\t Validation: Acc=50%, Loss=0.6923069953918457\n",
            "Iteration: 325150 \t Train: Acc=49%, Loss=0.6948226094245911 \t\t Validation: Acc=51%, Loss=0.6948376297950745\n",
            "Iteration: 325160 \t Train: Acc=53%, Loss=0.6905854940414429 \t\t Validation: Acc=50%, Loss=0.6856531500816345\n",
            "Iteration: 325170 \t Train: Acc=51%, Loss=0.6858674883842468 \t\t Validation: Acc=48%, Loss=0.7003341913223267\n",
            "Iteration: 325180 \t Train: Acc=50%, Loss=0.6959072351455688 \t\t Validation: Acc=50%, Loss=0.6897228956222534\n",
            "Iteration: 325190 \t Train: Acc=48%, Loss=0.6976456046104431 \t\t Validation: Acc=50%, Loss=0.6905844211578369\n",
            "Iteration: 325200 \t Train: Acc=53%, Loss=0.7004191279411316 \t\t Validation: Acc=53%, Loss=0.6976683735847473\n",
            "Iteration: 325210 \t Train: Acc=52%, Loss=0.6879944801330566 \t\t Validation: Acc=51%, Loss=0.6903219819068909\n",
            "Iteration: 325220 \t Train: Acc=49%, Loss=0.6980468034744263 \t\t Validation: Acc=46%, Loss=0.6963458061218262\n",
            "Iteration: 325230 \t Train: Acc=46%, Loss=0.6980597972869873 \t\t Validation: Acc=50%, Loss=0.6908470988273621\n",
            "Iteration: 325240 \t Train: Acc=48%, Loss=0.6929824948310852 \t\t Validation: Acc=53%, Loss=0.6915573477745056\n",
            "Iteration: 325250 \t Train: Acc=57%, Loss=0.6903015971183777 \t\t Validation: Acc=50%, Loss=0.6932651996612549\n",
            "Iteration: 325260 \t Train: Acc=52%, Loss=0.692348062992096 \t\t Validation: Acc=50%, Loss=0.6934388875961304\n",
            "Iteration: 325270 \t Train: Acc=47%, Loss=0.6895691752433777 \t\t Validation: Acc=52%, Loss=0.6909028887748718\n",
            "Iteration: 325280 \t Train: Acc=56%, Loss=0.6855242848396301 \t\t Validation: Acc=48%, Loss=0.6972331404685974\n",
            "Iteration: 325290 \t Train: Acc=42%, Loss=0.6985033750534058 \t\t Validation: Acc=53%, Loss=0.6897181272506714\n",
            "Iteration: 325300 \t Train: Acc=55%, Loss=0.6909204721450806 \t\t Validation: Acc=50%, Loss=0.6882309317588806\n",
            "Iteration: 325310 \t Train: Acc=55%, Loss=0.6821836233139038 \t\t Validation: Acc=48%, Loss=0.7036982774734497\n",
            "Iteration: 325320 \t Train: Acc=47%, Loss=0.6932084560394287 \t\t Validation: Acc=50%, Loss=0.6893664598464966\n",
            "Iteration: 325330 \t Train: Acc=57%, Loss=0.6920076608657837 \t\t Validation: Acc=50%, Loss=0.6982818841934204\n",
            "Iteration: 325340 \t Train: Acc=56%, Loss=0.6887590289115906 \t\t Validation: Acc=53%, Loss=0.6912996768951416\n",
            "Iteration: 325350 \t Train: Acc=48%, Loss=0.6980528831481934 \t\t Validation: Acc=51%, Loss=0.6960842609405518\n",
            "Iteration: 325360 \t Train: Acc=44%, Loss=0.6992084980010986 \t\t Validation: Acc=54%, Loss=0.6840230226516724\n",
            "Iteration: 325370 \t Train: Acc=52%, Loss=0.6916811466217041 \t\t Validation: Acc=50%, Loss=0.6913307309150696\n",
            "Iteration: 325380 \t Train: Acc=52%, Loss=0.6873582601547241 \t\t Validation: Acc=50%, Loss=0.6877909302711487\n",
            "Iteration: 325390 \t Train: Acc=55%, Loss=0.6928738355636597 \t\t Validation: Acc=54%, Loss=0.6902870535850525\n",
            "Iteration: 325400 \t Train: Acc=46%, Loss=0.6934839487075806 \t\t Validation: Acc=46%, Loss=0.7076864242553711\n",
            "Iteration: 325410 \t Train: Acc=51%, Loss=0.6913959383964539 \t\t Validation: Acc=48%, Loss=0.694786787033081\n",
            "Iteration: 325420 \t Train: Acc=48%, Loss=0.6950845718383789 \t\t Validation: Acc=51%, Loss=0.6966087818145752\n",
            "Iteration: 325430 \t Train: Acc=55%, Loss=0.6848105192184448 \t\t Validation: Acc=51%, Loss=0.6944704055786133\n",
            "Iteration: 325440 \t Train: Acc=51%, Loss=0.6902215480804443 \t\t Validation: Acc=51%, Loss=0.693412184715271\n",
            "Iteration: 325450 \t Train: Acc=46%, Loss=0.6907232999801636 \t\t Validation: Acc=50%, Loss=0.6949430704116821\n",
            "Iteration: 325460 \t Train: Acc=56%, Loss=0.6900990605354309 \t\t Validation: Acc=50%, Loss=0.6979599595069885\n",
            "Iteration: 325470 \t Train: Acc=47%, Loss=0.692579448223114 \t\t Validation: Acc=50%, Loss=0.6898642182350159\n",
            "Iteration: 325480 \t Train: Acc=51%, Loss=0.6950531005859375 \t\t Validation: Acc=52%, Loss=0.6880566477775574\n",
            "Iteration: 325490 \t Train: Acc=50%, Loss=0.6914216876029968 \t\t Validation: Acc=50%, Loss=0.6849629878997803\n",
            "Iteration: 325500 \t Train: Acc=50%, Loss=0.6918968558311462 \t\t Validation: Acc=50%, Loss=0.6949613094329834\n",
            "Iteration: 325510 \t Train: Acc=47%, Loss=0.6918391585350037 \t\t Validation: Acc=52%, Loss=0.6874438524246216\n",
            "Iteration: 325520 \t Train: Acc=51%, Loss=0.6832464337348938 \t\t Validation: Acc=53%, Loss=0.7011911869049072\n",
            "Iteration: 325530 \t Train: Acc=52%, Loss=0.6871636509895325 \t\t Validation: Acc=48%, Loss=0.7004667520523071\n",
            "Iteration: 325540 \t Train: Acc=46%, Loss=0.7021843194961548 \t\t Validation: Acc=52%, Loss=0.6948806047439575\n",
            "Iteration: 325550 \t Train: Acc=56%, Loss=0.689613401889801 \t\t Validation: Acc=52%, Loss=0.690466046333313\n",
            "Iteration: 325560 \t Train: Acc=47%, Loss=0.6910883784294128 \t\t Validation: Acc=46%, Loss=0.6950010061264038\n",
            "Iteration: 325570 \t Train: Acc=53%, Loss=0.6887720823287964 \t\t Validation: Acc=52%, Loss=0.6891711354255676\n",
            "Iteration: 325580 \t Train: Acc=60%, Loss=0.6798746585845947 \t\t Validation: Acc=50%, Loss=0.6879754066467285\n",
            "Iteration: 325590 \t Train: Acc=52%, Loss=0.6891146302223206 \t\t Validation: Acc=53%, Loss=0.6890018582344055\n",
            "Iteration: 325600 \t Train: Acc=57%, Loss=0.6908682584762573 \t\t Validation: Acc=54%, Loss=0.6926251649856567\n",
            "Iteration: 325610 \t Train: Acc=53%, Loss=0.6891669034957886 \t\t Validation: Acc=50%, Loss=0.6933028101921082\n",
            "Iteration: 325620 \t Train: Acc=48%, Loss=0.6975390911102295 \t\t Validation: Acc=49%, Loss=0.6939781904220581\n",
            "Iteration: 325630 \t Train: Acc=52%, Loss=0.6903300285339355 \t\t Validation: Acc=53%, Loss=0.6955123543739319\n",
            "Iteration: 325640 \t Train: Acc=50%, Loss=0.6928701996803284 \t\t Validation: Acc=49%, Loss=0.705127477645874\n",
            "Iteration: 325650 \t Train: Acc=45%, Loss=0.6939409971237183 \t\t Validation: Acc=47%, Loss=0.6957410573959351\n",
            "Iteration: 325660 \t Train: Acc=53%, Loss=0.6887360215187073 \t\t Validation: Acc=52%, Loss=0.6857029795646667\n",
            "Iteration: 325670 \t Train: Acc=44%, Loss=0.7073183059692383 \t\t Validation: Acc=51%, Loss=0.6845448017120361\n",
            "Iteration: 325680 \t Train: Acc=52%, Loss=0.6954590082168579 \t\t Validation: Acc=50%, Loss=0.6915801763534546\n",
            "Iteration: 325690 \t Train: Acc=55%, Loss=0.6960256099700928 \t\t Validation: Acc=50%, Loss=0.6951280832290649\n",
            "Iteration: 325700 \t Train: Acc=51%, Loss=0.6861563324928284 \t\t Validation: Acc=51%, Loss=0.6861376762390137\n",
            "Iteration: 325710 \t Train: Acc=50%, Loss=0.6856683492660522 \t\t Validation: Acc=52%, Loss=0.6883919835090637\n",
            "Iteration: 325720 \t Train: Acc=47%, Loss=0.6940126419067383 \t\t Validation: Acc=53%, Loss=0.6855651140213013\n",
            "Iteration: 325730 \t Train: Acc=53%, Loss=0.6874349117279053 \t\t Validation: Acc=52%, Loss=0.6905050873756409\n",
            "Iteration: 325740 \t Train: Acc=51%, Loss=0.6945414543151855 \t\t Validation: Acc=49%, Loss=0.6943514943122864\n",
            "Iteration: 325750 \t Train: Acc=46%, Loss=0.6971726417541504 \t\t Validation: Acc=50%, Loss=0.6988982558250427\n",
            "Iteration: 325760 \t Train: Acc=50%, Loss=0.6989983320236206 \t\t Validation: Acc=52%, Loss=0.7005949020385742\n",
            "Iteration: 325770 \t Train: Acc=46%, Loss=0.6998859643936157 \t\t Validation: Acc=48%, Loss=0.6862055063247681\n",
            "Iteration: 325780 \t Train: Acc=49%, Loss=0.6906982660293579 \t\t Validation: Acc=49%, Loss=0.6893444657325745\n",
            "Iteration: 325790 \t Train: Acc=49%, Loss=0.7008712291717529 \t\t Validation: Acc=53%, Loss=0.6945891380310059\n",
            "Iteration: 325800 \t Train: Acc=54%, Loss=0.6910292506217957 \t\t Validation: Acc=51%, Loss=0.6924915909767151\n",
            "Iteration: 325810 \t Train: Acc=50%, Loss=0.694132387638092 \t\t Validation: Acc=50%, Loss=0.7024489641189575\n",
            "Iteration: 325820 \t Train: Acc=50%, Loss=0.6973167061805725 \t\t Validation: Acc=46%, Loss=0.6891993284225464\n",
            "Iteration: 325830 \t Train: Acc=57%, Loss=0.6822537779808044 \t\t Validation: Acc=46%, Loss=0.6928777098655701\n",
            "Iteration: 325840 \t Train: Acc=49%, Loss=0.6968749761581421 \t\t Validation: Acc=53%, Loss=0.6982630491256714\n",
            "Iteration: 325850 \t Train: Acc=43%, Loss=0.6992993354797363 \t\t Validation: Acc=49%, Loss=0.6872935891151428\n",
            "Iteration: 325860 \t Train: Acc=50%, Loss=0.6875341534614563 \t\t Validation: Acc=51%, Loss=0.6907099485397339\n",
            "Iteration: 325870 \t Train: Acc=52%, Loss=0.6913914680480957 \t\t Validation: Acc=51%, Loss=0.6934051513671875\n",
            "Iteration: 325880 \t Train: Acc=46%, Loss=0.6955430507659912 \t\t Validation: Acc=52%, Loss=0.6911364793777466\n",
            "Iteration: 325890 \t Train: Acc=51%, Loss=0.6947964429855347 \t\t Validation: Acc=52%, Loss=0.6954554915428162\n",
            "Iteration: 325900 \t Train: Acc=52%, Loss=0.689825713634491 \t\t Validation: Acc=46%, Loss=0.6922793388366699\n",
            "Iteration: 325910 \t Train: Acc=53%, Loss=0.688094973564148 \t\t Validation: Acc=53%, Loss=0.6906872391700745\n",
            "Iteration: 325920 \t Train: Acc=55%, Loss=0.6867887377738953 \t\t Validation: Acc=53%, Loss=0.6931142807006836\n",
            "Iteration: 325930 \t Train: Acc=46%, Loss=0.6919808387756348 \t\t Validation: Acc=49%, Loss=0.6967110633850098\n",
            "Iteration: 325940 \t Train: Acc=50%, Loss=0.690723717212677 \t\t Validation: Acc=49%, Loss=0.6762980222702026\n",
            "Iteration: 325950 \t Train: Acc=54%, Loss=0.6937698721885681 \t\t Validation: Acc=51%, Loss=0.6970957517623901\n",
            "Iteration: 325960 \t Train: Acc=52%, Loss=0.6942775845527649 \t\t Validation: Acc=52%, Loss=0.6966258883476257\n",
            "Iteration: 325970 \t Train: Acc=53%, Loss=0.688910961151123 \t\t Validation: Acc=50%, Loss=0.6898391842842102\n",
            "Iteration: 325980 \t Train: Acc=51%, Loss=0.6886558532714844 \t\t Validation: Acc=51%, Loss=0.6910439133644104\n",
            "Iteration: 325990 \t Train: Acc=55%, Loss=0.6906237602233887 \t\t Validation: Acc=53%, Loss=0.6855320334434509\n",
            "Iteration: 326000 \t Train: Acc=53%, Loss=0.6909513473510742 \t\t Validation: Acc=54%, Loss=0.6905266046524048\n",
            "Iteration: 326010 \t Train: Acc=53%, Loss=0.6927632689476013 \t\t Validation: Acc=53%, Loss=0.6877771019935608\n",
            "Iteration: 326020 \t Train: Acc=50%, Loss=0.6963678002357483 \t\t Validation: Acc=49%, Loss=0.6908241510391235\n",
            "Iteration: 326030 \t Train: Acc=46%, Loss=0.6923098564147949 \t\t Validation: Acc=47%, Loss=0.704342246055603\n",
            "Iteration: 326040 \t Train: Acc=60%, Loss=0.6867759823799133 \t\t Validation: Acc=47%, Loss=0.7018536329269409\n",
            "Iteration: 326050 \t Train: Acc=50%, Loss=0.689071536064148 \t\t Validation: Acc=46%, Loss=0.6976594924926758\n",
            "Iteration: 326060 \t Train: Acc=46%, Loss=0.6942700147628784 \t\t Validation: Acc=51%, Loss=0.6893297433853149\n",
            "Iteration: 326070 \t Train: Acc=48%, Loss=0.6914226412773132 \t\t Validation: Acc=50%, Loss=0.6930872797966003\n",
            "Iteration: 326080 \t Train: Acc=54%, Loss=0.6870160698890686 \t\t Validation: Acc=50%, Loss=0.6969138383865356\n",
            "Iteration: 326090 \t Train: Acc=50%, Loss=0.693240225315094 \t\t Validation: Acc=49%, Loss=0.6945174336433411\n",
            "Iteration: 326100 \t Train: Acc=48%, Loss=0.6927231550216675 \t\t Validation: Acc=49%, Loss=0.6918095350265503\n",
            "Iteration: 326110 \t Train: Acc=50%, Loss=0.685113251209259 \t\t Validation: Acc=49%, Loss=0.6872646808624268\n",
            "Iteration: 326120 \t Train: Acc=48%, Loss=0.6925289034843445 \t\t Validation: Acc=50%, Loss=0.6922854781150818\n",
            "Iteration: 326130 \t Train: Acc=50%, Loss=0.6886832118034363 \t\t Validation: Acc=49%, Loss=0.6960026621818542\n",
            "Iteration: 326140 \t Train: Acc=56%, Loss=0.6946884989738464 \t\t Validation: Acc=50%, Loss=0.6918208599090576\n",
            "Iteration: 326150 \t Train: Acc=56%, Loss=0.686896800994873 \t\t Validation: Acc=52%, Loss=0.690813422203064\n",
            "Iteration: 326160 \t Train: Acc=56%, Loss=0.6865569949150085 \t\t Validation: Acc=48%, Loss=0.6913431286811829\n",
            "Iteration: 326170 \t Train: Acc=48%, Loss=0.690035879611969 \t\t Validation: Acc=52%, Loss=0.6901399493217468\n",
            "Iteration: 326180 \t Train: Acc=53%, Loss=0.6834530830383301 \t\t Validation: Acc=49%, Loss=0.6910300850868225\n",
            "Iteration: 326190 \t Train: Acc=47%, Loss=0.6983698606491089 \t\t Validation: Acc=50%, Loss=0.6862105131149292\n",
            "Iteration: 326200 \t Train: Acc=46%, Loss=0.691020667552948 \t\t Validation: Acc=50%, Loss=0.6960198879241943\n",
            "Iteration: 326210 \t Train: Acc=48%, Loss=0.6960755586624146 \t\t Validation: Acc=48%, Loss=0.6940984725952148\n",
            "Iteration: 326220 \t Train: Acc=48%, Loss=0.6956108212471008 \t\t Validation: Acc=47%, Loss=0.7011004686355591\n",
            "Iteration: 326230 \t Train: Acc=51%, Loss=0.6840923428535461 \t\t Validation: Acc=50%, Loss=0.6899667978286743\n",
            "Iteration: 326240 \t Train: Acc=59%, Loss=0.6883559226989746 \t\t Validation: Acc=50%, Loss=0.6934020519256592\n",
            "Iteration: 326250 \t Train: Acc=50%, Loss=0.6926950812339783 \t\t Validation: Acc=50%, Loss=0.698775053024292\n",
            "Iteration: 326260 \t Train: Acc=51%, Loss=0.6911075115203857 \t\t Validation: Acc=51%, Loss=0.6947283744812012\n",
            "Iteration: 326270 \t Train: Acc=54%, Loss=0.689868688583374 \t\t Validation: Acc=50%, Loss=0.7018587589263916\n",
            "Iteration: 326280 \t Train: Acc=52%, Loss=0.6861884593963623 \t\t Validation: Acc=51%, Loss=0.6893061399459839\n",
            "Iteration: 326290 \t Train: Acc=52%, Loss=0.6827812194824219 \t\t Validation: Acc=50%, Loss=0.692607045173645\n",
            "Iteration: 326300 \t Train: Acc=57%, Loss=0.6883541345596313 \t\t Validation: Acc=49%, Loss=0.6856170296669006\n",
            "Iteration: 326310 \t Train: Acc=49%, Loss=0.6951176524162292 \t\t Validation: Acc=49%, Loss=0.6871973276138306\n",
            "Iteration: 326320 \t Train: Acc=53%, Loss=0.6886173486709595 \t\t Validation: Acc=49%, Loss=0.6915363669395447\n",
            "Iteration: 326330 \t Train: Acc=52%, Loss=0.6895262598991394 \t\t Validation: Acc=52%, Loss=0.6898221969604492\n",
            "Iteration: 326340 \t Train: Acc=43%, Loss=0.6852719783782959 \t\t Validation: Acc=52%, Loss=0.6944588422775269\n",
            "Iteration: 326350 \t Train: Acc=52%, Loss=0.69140625 \t\t Validation: Acc=50%, Loss=0.6936119198799133\n",
            "Iteration: 326360 \t Train: Acc=51%, Loss=0.6913032531738281 \t\t Validation: Acc=48%, Loss=0.7002726793289185\n",
            "Iteration: 326370 \t Train: Acc=46%, Loss=0.6902576684951782 \t\t Validation: Acc=49%, Loss=0.6979511976242065\n",
            "Iteration: 326380 \t Train: Acc=53%, Loss=0.6892332434654236 \t\t Validation: Acc=48%, Loss=0.6951836347579956\n",
            "Iteration: 326390 \t Train: Acc=50%, Loss=0.6909956932067871 \t\t Validation: Acc=50%, Loss=0.6917701363563538\n",
            "Iteration: 326400 \t Train: Acc=50%, Loss=0.6883047223091125 \t\t Validation: Acc=50%, Loss=0.702014148235321\n",
            "Iteration: 326410 \t Train: Acc=48%, Loss=0.6944133043289185 \t\t Validation: Acc=52%, Loss=0.691916286945343\n",
            "Iteration: 326420 \t Train: Acc=47%, Loss=0.6919905543327332 \t\t Validation: Acc=48%, Loss=0.6961557865142822\n",
            "Iteration: 326430 \t Train: Acc=54%, Loss=0.6914430260658264 \t\t Validation: Acc=54%, Loss=0.6826412677764893\n",
            "Iteration: 326440 \t Train: Acc=51%, Loss=0.688901424407959 \t\t Validation: Acc=50%, Loss=0.6860765218734741\n",
            "Iteration: 326450 \t Train: Acc=53%, Loss=0.6923298835754395 \t\t Validation: Acc=45%, Loss=0.6987841725349426\n",
            "Iteration: 326460 \t Train: Acc=50%, Loss=0.6898186802864075 \t\t Validation: Acc=50%, Loss=0.692774772644043\n",
            "Iteration: 326470 \t Train: Acc=52%, Loss=0.6856628656387329 \t\t Validation: Acc=49%, Loss=0.6994163990020752\n",
            "Iteration: 326480 \t Train: Acc=46%, Loss=0.6934342384338379 \t\t Validation: Acc=47%, Loss=0.6942514777183533\n",
            "Iteration: 326490 \t Train: Acc=52%, Loss=0.6829351782798767 \t\t Validation: Acc=48%, Loss=0.6983746290206909\n",
            "Iteration: 326500 \t Train: Acc=53%, Loss=0.6798072457313538 \t\t Validation: Acc=50%, Loss=0.6907714605331421\n",
            "Iteration: 326510 \t Train: Acc=47%, Loss=0.689369797706604 \t\t Validation: Acc=46%, Loss=0.691485583782196\n",
            "Iteration: 326520 \t Train: Acc=50%, Loss=0.694781482219696 \t\t Validation: Acc=46%, Loss=0.7090128064155579\n",
            "Iteration: 326530 \t Train: Acc=51%, Loss=0.689629852771759 \t\t Validation: Acc=52%, Loss=0.6890444755554199\n",
            "Iteration: 326540 \t Train: Acc=50%, Loss=0.693241536617279 \t\t Validation: Acc=54%, Loss=0.6894566416740417\n",
            "Iteration: 326550 \t Train: Acc=50%, Loss=0.6923226118087769 \t\t Validation: Acc=50%, Loss=0.6969273090362549\n",
            "Iteration: 326560 \t Train: Acc=47%, Loss=0.6961753964424133 \t\t Validation: Acc=46%, Loss=0.705755352973938\n",
            "Iteration: 326570 \t Train: Acc=52%, Loss=0.6891128420829773 \t\t Validation: Acc=44%, Loss=0.7147634625434875\n",
            "Iteration: 326580 \t Train: Acc=46%, Loss=0.6940496563911438 \t\t Validation: Acc=52%, Loss=0.6915833353996277\n",
            "Iteration: 326590 \t Train: Acc=50%, Loss=0.6963616013526917 \t\t Validation: Acc=53%, Loss=0.6841943264007568\n",
            "Iteration: 326600 \t Train: Acc=53%, Loss=0.6871702671051025 \t\t Validation: Acc=50%, Loss=0.6869634985923767\n",
            "Iteration: 326610 \t Train: Acc=50%, Loss=0.6935887336730957 \t\t Validation: Acc=52%, Loss=0.6930962800979614\n",
            "Iteration: 326620 \t Train: Acc=51%, Loss=0.6869029998779297 \t\t Validation: Acc=50%, Loss=0.6917746663093567\n",
            "Iteration: 326630 \t Train: Acc=50%, Loss=0.6998085975646973 \t\t Validation: Acc=49%, Loss=0.6982771754264832\n",
            "Iteration: 326640 \t Train: Acc=52%, Loss=0.6898547410964966 \t\t Validation: Acc=55%, Loss=0.6901888251304626\n",
            "Iteration: 326650 \t Train: Acc=46%, Loss=0.6893974542617798 \t\t Validation: Acc=51%, Loss=0.6947046518325806\n",
            "Iteration: 326660 \t Train: Acc=52%, Loss=0.6940714716911316 \t\t Validation: Acc=50%, Loss=0.6948981285095215\n",
            "Iteration: 326670 \t Train: Acc=53%, Loss=0.6854724287986755 \t\t Validation: Acc=50%, Loss=0.6932133436203003\n",
            "Iteration: 326680 \t Train: Acc=51%, Loss=0.6994922757148743 \t\t Validation: Acc=47%, Loss=0.6968667507171631\n",
            "Iteration: 326690 \t Train: Acc=50%, Loss=0.6925567388534546 \t\t Validation: Acc=51%, Loss=0.6907864809036255\n",
            "Iteration: 326700 \t Train: Acc=45%, Loss=0.6899445652961731 \t\t Validation: Acc=48%, Loss=0.6943767666816711\n",
            "Iteration: 326710 \t Train: Acc=50%, Loss=0.6926674246788025 \t\t Validation: Acc=48%, Loss=0.6916349530220032\n",
            "Iteration: 326720 \t Train: Acc=50%, Loss=0.6885323524475098 \t\t Validation: Acc=49%, Loss=0.6884855031967163\n",
            "Iteration: 326730 \t Train: Acc=50%, Loss=0.6913852095603943 \t\t Validation: Acc=51%, Loss=0.6866050362586975\n",
            "Iteration: 326740 \t Train: Acc=49%, Loss=0.6877454519271851 \t\t Validation: Acc=57%, Loss=0.6880306005477905\n",
            "Iteration: 326750 \t Train: Acc=48%, Loss=0.6933568120002747 \t\t Validation: Acc=51%, Loss=0.6959178447723389\n",
            "Iteration: 326760 \t Train: Acc=49%, Loss=0.6981180906295776 \t\t Validation: Acc=50%, Loss=0.6974552869796753\n",
            "Iteration: 326770 \t Train: Acc=52%, Loss=0.6848911046981812 \t\t Validation: Acc=52%, Loss=0.6875548958778381\n",
            "Iteration: 326780 \t Train: Acc=48%, Loss=0.6975001692771912 \t\t Validation: Acc=49%, Loss=0.6960254907608032\n",
            "Iteration: 326790 \t Train: Acc=53%, Loss=0.6882014274597168 \t\t Validation: Acc=50%, Loss=0.6887328624725342\n",
            "Iteration: 326800 \t Train: Acc=51%, Loss=0.688346266746521 \t\t Validation: Acc=50%, Loss=0.6998633146286011\n",
            "Iteration: 326810 \t Train: Acc=50%, Loss=0.6921485662460327 \t\t Validation: Acc=47%, Loss=0.6849943399429321\n",
            "Iteration: 326820 \t Train: Acc=52%, Loss=0.6867054104804993 \t\t Validation: Acc=52%, Loss=0.7045097947120667\n",
            "Iteration: 326830 \t Train: Acc=57%, Loss=0.6933177709579468 \t\t Validation: Acc=50%, Loss=0.691741406917572\n",
            "Iteration: 326840 \t Train: Acc=52%, Loss=0.6926064491271973 \t\t Validation: Acc=50%, Loss=0.6960360407829285\n",
            "Iteration: 326850 \t Train: Acc=54%, Loss=0.6894457340240479 \t\t Validation: Acc=50%, Loss=0.6933324337005615\n",
            "Iteration: 326860 \t Train: Acc=49%, Loss=0.6932629346847534 \t\t Validation: Acc=50%, Loss=0.6961212158203125\n",
            "Iteration: 326870 \t Train: Acc=52%, Loss=0.6901286244392395 \t\t Validation: Acc=49%, Loss=0.6941458582878113\n",
            "Iteration: 326880 \t Train: Acc=52%, Loss=0.6881016492843628 \t\t Validation: Acc=50%, Loss=0.6984021663665771\n",
            "Iteration: 326890 \t Train: Acc=55%, Loss=0.6912441849708557 \t\t Validation: Acc=53%, Loss=0.6821519732475281\n",
            "Iteration: 326900 \t Train: Acc=50%, Loss=0.6963990330696106 \t\t Validation: Acc=45%, Loss=0.6955912709236145\n",
            "Iteration: 326910 \t Train: Acc=59%, Loss=0.6884489059448242 \t\t Validation: Acc=50%, Loss=0.6963654160499573\n",
            "Iteration: 326920 \t Train: Acc=56%, Loss=0.6905439496040344 \t\t Validation: Acc=46%, Loss=0.7024673223495483\n",
            "Iteration: 326930 \t Train: Acc=46%, Loss=0.7001606225967407 \t\t Validation: Acc=49%, Loss=0.6933879852294922\n",
            "Iteration: 326940 \t Train: Acc=50%, Loss=0.6953923106193542 \t\t Validation: Acc=48%, Loss=0.697364866733551\n",
            "Iteration: 326950 \t Train: Acc=49%, Loss=0.6899274587631226 \t\t Validation: Acc=55%, Loss=0.6844341158866882\n",
            "Iteration: 326960 \t Train: Acc=57%, Loss=0.6871179938316345 \t\t Validation: Acc=53%, Loss=0.6777213215827942\n",
            "Iteration: 326970 \t Train: Acc=56%, Loss=0.684499979019165 \t\t Validation: Acc=50%, Loss=0.6917200088500977\n",
            "Iteration: 326980 \t Train: Acc=49%, Loss=0.6973499059677124 \t\t Validation: Acc=50%, Loss=0.6919677257537842\n",
            "Iteration: 326990 \t Train: Acc=49%, Loss=0.6971343159675598 \t\t Validation: Acc=53%, Loss=0.6807828545570374\n",
            "Iteration: 327000 \t Train: Acc=53%, Loss=0.6929892301559448 \t\t Validation: Acc=51%, Loss=0.6977987289428711\n",
            "Iteration: 327010 \t Train: Acc=50%, Loss=0.6908529996871948 \t\t Validation: Acc=50%, Loss=0.6974117755889893\n",
            "Iteration: 327020 \t Train: Acc=56%, Loss=0.6870799660682678 \t\t Validation: Acc=57%, Loss=0.6915243864059448\n",
            "Iteration: 327030 \t Train: Acc=53%, Loss=0.692565381526947 \t\t Validation: Acc=50%, Loss=0.6908578872680664\n",
            "Iteration: 327040 \t Train: Acc=53%, Loss=0.6890379190444946 \t\t Validation: Acc=51%, Loss=0.6929956674575806\n",
            "Iteration: 327050 \t Train: Acc=47%, Loss=0.6956663727760315 \t\t Validation: Acc=50%, Loss=0.6962953805923462\n",
            "Iteration: 327060 \t Train: Acc=54%, Loss=0.6871444582939148 \t\t Validation: Acc=50%, Loss=0.6859101057052612\n",
            "Iteration: 327070 \t Train: Acc=56%, Loss=0.6870028972625732 \t\t Validation: Acc=50%, Loss=0.6891108751296997\n",
            "Iteration: 327080 \t Train: Acc=48%, Loss=0.6859232187271118 \t\t Validation: Acc=53%, Loss=0.683488667011261\n",
            "Iteration: 327090 \t Train: Acc=47%, Loss=0.6915771961212158 \t\t Validation: Acc=50%, Loss=0.6942043304443359\n",
            "Iteration: 327100 \t Train: Acc=49%, Loss=0.6865363121032715 \t\t Validation: Acc=50%, Loss=0.6948744654655457\n",
            "Iteration: 327110 \t Train: Acc=54%, Loss=0.6929962635040283 \t\t Validation: Acc=47%, Loss=0.6936967968940735\n",
            "Iteration: 327120 \t Train: Acc=49%, Loss=0.6931567788124084 \t\t Validation: Acc=43%, Loss=0.7001032829284668\n",
            "Iteration: 327130 \t Train: Acc=54%, Loss=0.6926131248474121 \t\t Validation: Acc=51%, Loss=0.6801702976226807\n",
            "Iteration: 327140 \t Train: Acc=52%, Loss=0.6921348571777344 \t\t Validation: Acc=53%, Loss=0.6835726499557495\n",
            "Iteration: 327150 \t Train: Acc=57%, Loss=0.6889644861221313 \t\t Validation: Acc=52%, Loss=0.6884347200393677\n",
            "Iteration: 327160 \t Train: Acc=45%, Loss=0.6892216801643372 \t\t Validation: Acc=51%, Loss=0.6911213397979736\n",
            "It's been too long since we last saved the model. Saving...\n",
            "Iteration: 327170 \t Train: Acc=47%, Loss=0.6890825629234314 \t\t Validation: Acc=45%, Loss=0.6889159083366394\n",
            "Iteration: 327180 \t Train: Acc=47%, Loss=0.6991331577301025 \t\t Validation: Acc=47%, Loss=0.6926289796829224\n",
            "Iteration: 327190 \t Train: Acc=48%, Loss=0.688927948474884 \t\t Validation: Acc=50%, Loss=0.6997449398040771\n",
            "Iteration: 327200 \t Train: Acc=54%, Loss=0.6924775242805481 \t\t Validation: Acc=51%, Loss=0.68839430809021\n",
            "Iteration: 327210 \t Train: Acc=51%, Loss=0.6993630528450012 \t\t Validation: Acc=50%, Loss=0.6975835561752319\n",
            "Iteration: 327220 \t Train: Acc=43%, Loss=0.6956958174705505 \t\t Validation: Acc=50%, Loss=0.6899724006652832\n",
            "Iteration: 327230 \t Train: Acc=54%, Loss=0.6900772452354431 \t\t Validation: Acc=46%, Loss=0.6934974789619446\n",
            "Iteration: 327240 \t Train: Acc=50%, Loss=0.6925769448280334 \t\t Validation: Acc=49%, Loss=0.6931101083755493\n",
            "Iteration: 327250 \t Train: Acc=55%, Loss=0.686206042766571 \t\t Validation: Acc=50%, Loss=0.6928904056549072\n",
            "Iteration: 327260 \t Train: Acc=55%, Loss=0.6926896572113037 \t\t Validation: Acc=50%, Loss=0.6933000087738037\n",
            "Iteration: 327270 \t Train: Acc=48%, Loss=0.6893558502197266 \t\t Validation: Acc=46%, Loss=0.6937322020530701\n",
            "Iteration: 327280 \t Train: Acc=52%, Loss=0.6874480247497559 \t\t Validation: Acc=52%, Loss=0.688444972038269\n",
            "Iteration: 327290 \t Train: Acc=47%, Loss=0.6908297538757324 \t\t Validation: Acc=51%, Loss=0.6935367584228516\n",
            "Iteration: 327300 \t Train: Acc=55%, Loss=0.6897856593132019 \t\t Validation: Acc=52%, Loss=0.6851575374603271\n",
            "Iteration: 327310 \t Train: Acc=49%, Loss=0.6939433813095093 \t\t Validation: Acc=47%, Loss=0.6993797421455383\n",
            "Iteration: 327320 \t Train: Acc=50%, Loss=0.6930758953094482 \t\t Validation: Acc=43%, Loss=0.6998635530471802\n",
            "Iteration: 327330 \t Train: Acc=50%, Loss=0.6920225620269775 \t\t Validation: Acc=49%, Loss=0.6957539916038513\n",
            "Iteration: 327340 \t Train: Acc=49%, Loss=0.6966105699539185 \t\t Validation: Acc=50%, Loss=0.6958824992179871\n",
            "Iteration: 327350 \t Train: Acc=50%, Loss=0.6905562877655029 \t\t Validation: Acc=50%, Loss=0.6960409879684448\n",
            "Iteration: 327360 \t Train: Acc=50%, Loss=0.6966747641563416 \t\t Validation: Acc=52%, Loss=0.6905750036239624\n",
            "Iteration: 327370 \t Train: Acc=52%, Loss=0.6878407597541809 \t\t Validation: Acc=50%, Loss=0.6865661144256592\n",
            "Iteration: 327380 \t Train: Acc=48%, Loss=0.6972104907035828 \t\t Validation: Acc=48%, Loss=0.6971592307090759\n",
            "Iteration: 327390 \t Train: Acc=48%, Loss=0.6917440891265869 \t\t Validation: Acc=55%, Loss=0.6905547380447388\n",
            "Iteration: 327400 \t Train: Acc=56%, Loss=0.688349723815918 \t\t Validation: Acc=51%, Loss=0.6955299973487854\n",
            "Iteration: 327410 \t Train: Acc=50%, Loss=0.689725399017334 \t\t Validation: Acc=46%, Loss=0.6909278035163879\n",
            "Iteration: 327420 \t Train: Acc=52%, Loss=0.6910344362258911 \t\t Validation: Acc=50%, Loss=0.7000251412391663\n",
            "Iteration: 327430 \t Train: Acc=53%, Loss=0.6899111270904541 \t\t Validation: Acc=53%, Loss=0.6932206153869629\n",
            "Iteration: 327440 \t Train: Acc=50%, Loss=0.6827924251556396 \t\t Validation: Acc=55%, Loss=0.692499577999115\n",
            "Iteration: 327450 \t Train: Acc=44%, Loss=0.6986543536186218 \t\t Validation: Acc=50%, Loss=0.7006711959838867\n",
            "Iteration: 327460 \t Train: Acc=47%, Loss=0.7015542387962341 \t\t Validation: Acc=55%, Loss=0.6853268146514893\n",
            "Iteration: 327470 \t Train: Acc=48%, Loss=0.6936498880386353 \t\t Validation: Acc=52%, Loss=0.6830415725708008\n",
            "Iteration: 327480 \t Train: Acc=57%, Loss=0.6878544092178345 \t\t Validation: Acc=50%, Loss=0.6944467425346375\n",
            "Iteration: 327490 \t Train: Acc=54%, Loss=0.685427188873291 \t\t Validation: Acc=50%, Loss=0.6920014023780823\n",
            "Iteration: 327500 \t Train: Acc=48%, Loss=0.6888542175292969 \t\t Validation: Acc=46%, Loss=0.6932042241096497\n",
            "Iteration: 327510 \t Train: Acc=56%, Loss=0.6879592537879944 \t\t Validation: Acc=49%, Loss=0.6972650289535522\n",
            "Iteration: 327520 \t Train: Acc=50%, Loss=0.693565309047699 \t\t Validation: Acc=53%, Loss=0.6832451224327087\n",
            "Iteration: 327530 \t Train: Acc=51%, Loss=0.6927579641342163 \t\t Validation: Acc=53%, Loss=0.692725658416748\n",
            "Iteration: 327540 \t Train: Acc=54%, Loss=0.6922144293785095 \t\t Validation: Acc=48%, Loss=0.6968265771865845\n",
            "Iteration: 327550 \t Train: Acc=54%, Loss=0.6865941882133484 \t\t Validation: Acc=56%, Loss=0.6833609342575073\n",
            "Iteration: 327560 \t Train: Acc=51%, Loss=0.6931257247924805 \t\t Validation: Acc=49%, Loss=0.6865348219871521\n",
            "Iteration: 327570 \t Train: Acc=48%, Loss=0.6951886415481567 \t\t Validation: Acc=49%, Loss=0.6889992952346802\n",
            "Iteration: 327580 \t Train: Acc=46%, Loss=0.6861425042152405 \t\t Validation: Acc=53%, Loss=0.6876099705696106\n",
            "Iteration: 327590 \t Train: Acc=50%, Loss=0.6829914450645447 \t\t Validation: Acc=51%, Loss=0.6960331201553345\n",
            "Iteration: 327600 \t Train: Acc=46%, Loss=0.697532057762146 \t\t Validation: Acc=54%, Loss=0.694412112236023\n",
            "Iteration: 327610 \t Train: Acc=56%, Loss=0.6833924055099487 \t\t Validation: Acc=53%, Loss=0.7018452286720276\n",
            "Iteration: 327620 \t Train: Acc=47%, Loss=0.6940363645553589 \t\t Validation: Acc=51%, Loss=0.6911212801933289\n",
            "Iteration: 327630 \t Train: Acc=46%, Loss=0.6967451572418213 \t\t Validation: Acc=51%, Loss=0.696014940738678\n",
            "Iteration: 327640 \t Train: Acc=51%, Loss=0.6931411027908325 \t\t Validation: Acc=50%, Loss=0.6936088800430298\n",
            "Iteration: 327650 \t Train: Acc=53%, Loss=0.6846413612365723 \t\t Validation: Acc=46%, Loss=0.7043277621269226\n",
            "Iteration: 327660 \t Train: Acc=49%, Loss=0.6938535571098328 \t\t Validation: Acc=51%, Loss=0.6886952519416809\n",
            "Iteration: 327670 \t Train: Acc=43%, Loss=0.6932246088981628 \t\t Validation: Acc=47%, Loss=0.6929205060005188\n",
            "Iteration: 327680 \t Train: Acc=50%, Loss=0.6949678063392639 \t\t Validation: Acc=46%, Loss=0.7000705003738403\n",
            "Iteration: 327690 \t Train: Acc=61%, Loss=0.6862884759902954 \t\t Validation: Acc=49%, Loss=0.7044951319694519\n",
            "Iteration: 327700 \t Train: Acc=53%, Loss=0.6879926919937134 \t\t Validation: Acc=50%, Loss=0.6909074783325195\n",
            "Iteration: 327710 \t Train: Acc=51%, Loss=0.6902444362640381 \t\t Validation: Acc=43%, Loss=0.7048909664154053\n",
            "Iteration: 327720 \t Train: Acc=51%, Loss=0.687760591506958 \t\t Validation: Acc=50%, Loss=0.6935135722160339\n",
            "Iteration: 327730 \t Train: Acc=57%, Loss=0.6880147457122803 \t\t Validation: Acc=52%, Loss=0.6935446262359619\n",
            "Iteration: 327740 \t Train: Acc=53%, Loss=0.6861844062805176 \t\t Validation: Acc=50%, Loss=0.6914016008377075\n",
            "Iteration: 327750 \t Train: Acc=52%, Loss=0.6970741152763367 \t\t Validation: Acc=53%, Loss=0.6923205256462097\n",
            "Iteration: 327760 \t Train: Acc=47%, Loss=0.6933330297470093 \t\t Validation: Acc=47%, Loss=0.7040786743164062\n",
            "Iteration: 327770 \t Train: Acc=51%, Loss=0.6934606432914734 \t\t Validation: Acc=55%, Loss=0.6872161626815796\n",
            "Iteration: 327780 \t Train: Acc=49%, Loss=0.6915960311889648 \t\t Validation: Acc=54%, Loss=0.6869223117828369\n",
            "Iteration: 327790 \t Train: Acc=51%, Loss=0.6893036365509033 \t\t Validation: Acc=50%, Loss=0.6927788853645325\n",
            "Iteration: 327800 \t Train: Acc=50%, Loss=0.6977608799934387 \t\t Validation: Acc=53%, Loss=0.6945775747299194\n",
            "Iteration: 327810 \t Train: Acc=51%, Loss=0.6911388039588928 \t\t Validation: Acc=53%, Loss=0.6825717091560364\n",
            "Iteration: 327820 \t Train: Acc=56%, Loss=0.6854557394981384 \t\t Validation: Acc=55%, Loss=0.6944692134857178\n",
            "Iteration: 327830 \t Train: Acc=50%, Loss=0.6889047622680664 \t\t Validation: Acc=49%, Loss=0.6917309165000916\n",
            "Iteration: 327840 \t Train: Acc=52%, Loss=0.690271258354187 \t\t Validation: Acc=54%, Loss=0.6901190280914307\n",
            "Iteration: 327850 \t Train: Acc=50%, Loss=0.6848797798156738 \t\t Validation: Acc=50%, Loss=0.6993201971054077\n",
            "Iteration: 327860 \t Train: Acc=53%, Loss=0.6989110112190247 \t\t Validation: Acc=53%, Loss=0.6896693110466003\n",
            "Iteration: 327870 \t Train: Acc=51%, Loss=0.6968175768852234 \t\t Validation: Acc=48%, Loss=0.6952687501907349\n",
            "Iteration: 327880 \t Train: Acc=46%, Loss=0.6944732666015625 \t\t Validation: Acc=52%, Loss=0.6914334893226624\n",
            "Iteration: 327890 \t Train: Acc=49%, Loss=0.6998140215873718 \t\t Validation: Acc=52%, Loss=0.6975976824760437\n",
            "Iteration: 327900 \t Train: Acc=51%, Loss=0.695124089717865 \t\t Validation: Acc=52%, Loss=0.6945526003837585\n",
            "Iteration: 327910 \t Train: Acc=50%, Loss=0.6930999159812927 \t\t Validation: Acc=51%, Loss=0.6892247200012207\n",
            "Iteration: 327920 \t Train: Acc=51%, Loss=0.6918879747390747 \t\t Validation: Acc=52%, Loss=0.6896869540214539\n",
            "Iteration: 327930 \t Train: Acc=50%, Loss=0.6981577277183533 \t\t Validation: Acc=53%, Loss=0.6805325746536255\n",
            "Iteration: 327940 \t Train: Acc=46%, Loss=0.6906381249427795 \t\t Validation: Acc=54%, Loss=0.6942483186721802\n",
            "Iteration: 327950 \t Train: Acc=50%, Loss=0.6917831301689148 \t\t Validation: Acc=53%, Loss=0.692655622959137\n",
            "Iteration: 327960 \t Train: Acc=48%, Loss=0.6942026615142822 \t\t Validation: Acc=53%, Loss=0.6924872398376465\n",
            "Iteration: 327970 \t Train: Acc=49%, Loss=0.6922850012779236 \t\t Validation: Acc=55%, Loss=0.6888366937637329\n",
            "Iteration: 327980 \t Train: Acc=52%, Loss=0.6897788643836975 \t\t Validation: Acc=45%, Loss=0.6965664029121399\n",
            "Iteration: 327990 \t Train: Acc=49%, Loss=0.6951366066932678 \t\t Validation: Acc=48%, Loss=0.7016938924789429\n",
            "Iteration: 328000 \t Train: Acc=54%, Loss=0.695113480091095 \t\t Validation: Acc=48%, Loss=0.6915989518165588\n",
            "Iteration: 328010 \t Train: Acc=56%, Loss=0.6927565932273865 \t\t Validation: Acc=54%, Loss=0.685337483882904\n",
            "Iteration: 328020 \t Train: Acc=53%, Loss=0.6969233751296997 \t\t Validation: Acc=50%, Loss=0.6916448473930359\n",
            "Iteration: 328030 \t Train: Acc=51%, Loss=0.6899231672286987 \t\t Validation: Acc=51%, Loss=0.6905003190040588\n",
            "Iteration: 328040 \t Train: Acc=51%, Loss=0.7003896236419678 \t\t Validation: Acc=53%, Loss=0.689795970916748\n",
            "Iteration: 328050 \t Train: Acc=56%, Loss=0.6835054159164429 \t\t Validation: Acc=50%, Loss=0.6967809796333313\n",
            "Iteration: 328060 \t Train: Acc=57%, Loss=0.6831514239311218 \t\t Validation: Acc=53%, Loss=0.6918336749076843\n",
            "Iteration: 328070 \t Train: Acc=56%, Loss=0.6850967407226562 \t\t Validation: Acc=49%, Loss=0.6924914717674255\n",
            "Iteration: 328080 \t Train: Acc=46%, Loss=0.7008026838302612 \t\t Validation: Acc=55%, Loss=0.6933969259262085\n",
            "Iteration: 328090 \t Train: Acc=50%, Loss=0.6935948729515076 \t\t Validation: Acc=53%, Loss=0.6903194189071655\n",
            "Iteration: 328100 \t Train: Acc=50%, Loss=0.6934317350387573 \t\t Validation: Acc=60%, Loss=0.681698203086853\n",
            "Iteration: 328110 \t Train: Acc=46%, Loss=0.7019026279449463 \t\t Validation: Acc=53%, Loss=0.6890942454338074\n",
            "Iteration: 328120 \t Train: Acc=49%, Loss=0.6987390518188477 \t\t Validation: Acc=50%, Loss=0.6930587291717529\n",
            "Iteration: 328130 \t Train: Acc=47%, Loss=0.6989867091178894 \t\t Validation: Acc=51%, Loss=0.6869080662727356\n",
            "Iteration: 328140 \t Train: Acc=59%, Loss=0.6833007335662842 \t\t Validation: Acc=49%, Loss=0.6928931474685669\n",
            "Iteration: 328150 \t Train: Acc=47%, Loss=0.696678102016449 \t\t Validation: Acc=53%, Loss=0.686841607093811\n",
            "Iteration: 328160 \t Train: Acc=52%, Loss=0.6933079957962036 \t\t Validation: Acc=50%, Loss=0.6897646188735962\n",
            "Iteration: 328170 \t Train: Acc=47%, Loss=0.6875420212745667 \t\t Validation: Acc=50%, Loss=0.6922909021377563\n",
            "Iteration: 328180 \t Train: Acc=52%, Loss=0.6882966756820679 \t\t Validation: Acc=49%, Loss=0.6929500102996826\n",
            "Iteration: 328190 \t Train: Acc=59%, Loss=0.685509443283081 \t\t Validation: Acc=50%, Loss=0.695929765701294\n",
            "Iteration: 328200 \t Train: Acc=47%, Loss=0.6969247460365295 \t\t Validation: Acc=56%, Loss=0.6882157921791077\n",
            "Iteration: 328210 \t Train: Acc=47%, Loss=0.6945207118988037 \t\t Validation: Acc=52%, Loss=0.6905062198638916\n",
            "Iteration: 328220 \t Train: Acc=56%, Loss=0.686087965965271 \t\t Validation: Acc=50%, Loss=0.6969136595726013\n",
            "Iteration: 328230 \t Train: Acc=50%, Loss=0.690178394317627 \t\t Validation: Acc=44%, Loss=0.705355703830719\n",
            "Iteration: 328240 \t Train: Acc=50%, Loss=0.6921828389167786 \t\t Validation: Acc=51%, Loss=0.691244900226593\n",
            "Iteration: 328250 \t Train: Acc=48%, Loss=0.6960031390190125 \t\t Validation: Acc=49%, Loss=0.7007943391799927\n",
            "Iteration: 328260 \t Train: Acc=46%, Loss=0.6977712512016296 \t\t Validation: Acc=51%, Loss=0.6918961405754089\n",
            "Iteration: 328270 \t Train: Acc=51%, Loss=0.6905542612075806 \t\t Validation: Acc=51%, Loss=0.6925074458122253\n",
            "Iteration: 328280 \t Train: Acc=51%, Loss=0.6911960244178772 \t\t Validation: Acc=56%, Loss=0.6874409914016724\n",
            "Iteration: 328290 \t Train: Acc=50%, Loss=0.6858396530151367 \t\t Validation: Acc=48%, Loss=0.7038170695304871\n",
            "Iteration: 328300 \t Train: Acc=52%, Loss=0.6926344037055969 \t\t Validation: Acc=50%, Loss=0.6922980546951294\n",
            "Iteration: 328310 \t Train: Acc=45%, Loss=0.6968911290168762 \t\t Validation: Acc=48%, Loss=0.6972376108169556\n",
            "Iteration: 328320 \t Train: Acc=52%, Loss=0.6910064220428467 \t\t Validation: Acc=50%, Loss=0.7031105756759644\n",
            "Iteration: 328330 \t Train: Acc=46%, Loss=0.6946212649345398 \t\t Validation: Acc=57%, Loss=0.687050461769104\n",
            "Iteration: 328340 \t Train: Acc=53%, Loss=0.6917380690574646 \t\t Validation: Acc=50%, Loss=0.7004053592681885\n",
            "Iteration: 328350 \t Train: Acc=45%, Loss=0.6935670375823975 \t\t Validation: Acc=51%, Loss=0.6881852746009827\n",
            "Iteration: 328360 \t Train: Acc=51%, Loss=0.6953250169754028 \t\t Validation: Acc=49%, Loss=0.6950350999832153\n",
            "Iteration: 328370 \t Train: Acc=47%, Loss=0.6918681263923645 \t\t Validation: Acc=46%, Loss=0.6973114013671875\n",
            "Iteration: 328380 \t Train: Acc=52%, Loss=0.6893611550331116 \t\t Validation: Acc=47%, Loss=0.691582441329956\n",
            "Iteration: 328390 \t Train: Acc=46%, Loss=0.6948370337486267 \t\t Validation: Acc=50%, Loss=0.6913018822669983\n",
            "Iteration: 328400 \t Train: Acc=53%, Loss=0.6923865079879761 \t\t Validation: Acc=47%, Loss=0.6957480907440186\n",
            "Iteration: 328410 \t Train: Acc=50%, Loss=0.6913436651229858 \t\t Validation: Acc=51%, Loss=0.6930120587348938\n",
            "Iteration: 328420 \t Train: Acc=53%, Loss=0.6867998242378235 \t\t Validation: Acc=52%, Loss=0.6887776851654053\n",
            "Iteration: 328430 \t Train: Acc=44%, Loss=0.6976168751716614 \t\t Validation: Acc=53%, Loss=0.6929930448532104\n",
            "Iteration: 328440 \t Train: Acc=54%, Loss=0.6877131462097168 \t\t Validation: Acc=47%, Loss=0.7020373344421387\n",
            "Iteration: 328450 \t Train: Acc=53%, Loss=0.6838924288749695 \t\t Validation: Acc=52%, Loss=0.6945779919624329\n",
            "Iteration: 328460 \t Train: Acc=51%, Loss=0.6910368204116821 \t\t Validation: Acc=55%, Loss=0.686802089214325\n",
            "Iteration: 328470 \t Train: Acc=49%, Loss=0.6918370127677917 \t\t Validation: Acc=53%, Loss=0.6959631443023682\n",
            "Iteration: 328480 \t Train: Acc=50%, Loss=0.6924792528152466 \t\t Validation: Acc=53%, Loss=0.6860165596008301\n",
            "Iteration: 328490 \t Train: Acc=51%, Loss=0.690631628036499 \t\t Validation: Acc=53%, Loss=0.693954348564148\n",
            "Iteration: 328500 \t Train: Acc=48%, Loss=0.6963043212890625 \t\t Validation: Acc=50%, Loss=0.6954771280288696\n",
            "Iteration: 328510 \t Train: Acc=51%, Loss=0.6917396783828735 \t\t Validation: Acc=47%, Loss=0.693297266960144\n",
            "Iteration: 328520 \t Train: Acc=46%, Loss=0.7039474844932556 \t\t Validation: Acc=49%, Loss=0.6940772533416748\n",
            "Iteration: 328530 \t Train: Acc=50%, Loss=0.6881501078605652 \t\t Validation: Acc=50%, Loss=0.6854106187820435\n",
            "Iteration: 328540 \t Train: Acc=46%, Loss=0.7003876566886902 \t\t Validation: Acc=52%, Loss=0.6858026385307312\n",
            "Iteration: 328550 \t Train: Acc=55%, Loss=0.6888208985328674 \t\t Validation: Acc=50%, Loss=0.6890525221824646\n",
            "Iteration: 328560 \t Train: Acc=44%, Loss=0.6957664489746094 \t\t Validation: Acc=48%, Loss=0.6951510310173035\n",
            "Iteration: 328570 \t Train: Acc=52%, Loss=0.6873496770858765 \t\t Validation: Acc=55%, Loss=0.6865101456642151\n",
            "Iteration: 328580 \t Train: Acc=52%, Loss=0.6942835450172424 \t\t Validation: Acc=50%, Loss=0.6920915842056274\n",
            "Iteration: 328590 \t Train: Acc=57%, Loss=0.687495231628418 \t\t Validation: Acc=48%, Loss=0.6975687742233276\n",
            "Iteration: 328600 \t Train: Acc=45%, Loss=0.6942757368087769 \t\t Validation: Acc=47%, Loss=0.6930129528045654\n",
            "Iteration: 328610 \t Train: Acc=54%, Loss=0.6903830766677856 \t\t Validation: Acc=50%, Loss=0.6903575658798218\n",
            "Iteration: 328620 \t Train: Acc=52%, Loss=0.6880477666854858 \t\t Validation: Acc=49%, Loss=0.6956779360771179\n",
            "Iteration: 328630 \t Train: Acc=48%, Loss=0.6927906274795532 \t\t Validation: Acc=49%, Loss=0.6913699507713318\n",
            "Iteration: 328640 \t Train: Acc=54%, Loss=0.6838961243629456 \t\t Validation: Acc=50%, Loss=0.6941692233085632\n",
            "Iteration: 328650 \t Train: Acc=53%, Loss=0.6904006600379944 \t\t Validation: Acc=51%, Loss=0.6981754899024963\n",
            "Iteration: 328660 \t Train: Acc=53%, Loss=0.6871296167373657 \t\t Validation: Acc=49%, Loss=0.6994773149490356\n",
            "Iteration: 328670 \t Train: Acc=54%, Loss=0.6911827325820923 \t\t Validation: Acc=55%, Loss=0.6831916570663452\n",
            "Iteration: 328680 \t Train: Acc=53%, Loss=0.6900328397750854 \t\t Validation: Acc=50%, Loss=0.6988253593444824\n",
            "Iteration: 328690 \t Train: Acc=49%, Loss=0.6936176419258118 \t\t Validation: Acc=51%, Loss=0.6874247789382935\n",
            "Iteration: 328700 \t Train: Acc=50%, Loss=0.6928316354751587 \t\t Validation: Acc=54%, Loss=0.6895911693572998\n",
            "Iteration: 328710 \t Train: Acc=49%, Loss=0.6950210332870483 \t\t Validation: Acc=49%, Loss=0.6973876953125\n",
            "Iteration: 328720 \t Train: Acc=50%, Loss=0.695913553237915 \t\t Validation: Acc=51%, Loss=0.6832303404808044\n",
            "Iteration: 328730 \t Train: Acc=52%, Loss=0.6930265426635742 \t\t Validation: Acc=51%, Loss=0.6934479475021362\n",
            "Iteration: 328740 \t Train: Acc=53%, Loss=0.6946481466293335 \t\t Validation: Acc=46%, Loss=0.6978606581687927\n",
            "Iteration: 328750 \t Train: Acc=46%, Loss=0.6929956078529358 \t\t Validation: Acc=53%, Loss=0.6866023540496826\n",
            "Iteration: 328760 \t Train: Acc=50%, Loss=0.692955732345581 \t\t Validation: Acc=53%, Loss=0.6922995448112488\n",
            "Iteration: 328770 \t Train: Acc=50%, Loss=0.6944385766983032 \t\t Validation: Acc=53%, Loss=0.6932265758514404\n",
            "Iteration: 328780 \t Train: Acc=50%, Loss=0.6857205033302307 \t\t Validation: Acc=52%, Loss=0.6973680853843689\n",
            "Iteration: 328790 \t Train: Acc=52%, Loss=0.6927589774131775 \t\t Validation: Acc=52%, Loss=0.6938672661781311\n",
            "Iteration: 328800 \t Train: Acc=52%, Loss=0.6902459859848022 \t\t Validation: Acc=51%, Loss=0.6837149262428284\n",
            "Iteration: 328810 \t Train: Acc=54%, Loss=0.6862693428993225 \t\t Validation: Acc=50%, Loss=0.6923660039901733\n",
            "Iteration: 328820 \t Train: Acc=47%, Loss=0.6944987773895264 \t\t Validation: Acc=53%, Loss=0.6939297318458557\n",
            "Iteration: 328830 \t Train: Acc=53%, Loss=0.6919821500778198 \t\t Validation: Acc=50%, Loss=0.6961978673934937\n",
            "Iteration: 328840 \t Train: Acc=54%, Loss=0.6922517418861389 \t\t Validation: Acc=50%, Loss=0.6932240724563599\n",
            "Iteration: 328850 \t Train: Acc=50%, Loss=0.6958780884742737 \t\t Validation: Acc=50%, Loss=0.6933928728103638\n",
            "Iteration: 328860 \t Train: Acc=49%, Loss=0.695429801940918 \t\t Validation: Acc=50%, Loss=0.6933432817459106\n",
            "Iteration: 328870 \t Train: Acc=54%, Loss=0.6918826699256897 \t\t Validation: Acc=47%, Loss=0.6959010362625122\n",
            "Iteration: 328880 \t Train: Acc=47%, Loss=0.6940975189208984 \t\t Validation: Acc=51%, Loss=0.6928157210350037\n",
            "Iteration: 328890 \t Train: Acc=50%, Loss=0.6968774199485779 \t\t Validation: Acc=50%, Loss=0.6951249837875366\n",
            "Iteration: 328900 \t Train: Acc=52%, Loss=0.6877463459968567 \t\t Validation: Acc=48%, Loss=0.6862150430679321\n",
            "Iteration: 328910 \t Train: Acc=48%, Loss=0.6970746517181396 \t\t Validation: Acc=53%, Loss=0.6832022666931152\n",
            "Iteration: 328920 \t Train: Acc=54%, Loss=0.685297429561615 \t\t Validation: Acc=53%, Loss=0.6928703188896179\n",
            "Iteration: 328930 \t Train: Acc=50%, Loss=0.689753532409668 \t\t Validation: Acc=47%, Loss=0.7004058361053467\n",
            "Iteration: 328940 \t Train: Acc=50%, Loss=0.6923733949661255 \t\t Validation: Acc=46%, Loss=0.699607789516449\n",
            "Iteration: 328950 \t Train: Acc=43%, Loss=0.6904959082603455 \t\t Validation: Acc=49%, Loss=0.7019878625869751\n",
            "Iteration: 328960 \t Train: Acc=50%, Loss=0.6905385255813599 \t\t Validation: Acc=50%, Loss=0.6921875476837158\n",
            "Iteration: 328970 \t Train: Acc=53%, Loss=0.6857397556304932 \t\t Validation: Acc=53%, Loss=0.6900675892829895\n",
            "Iteration: 328980 \t Train: Acc=50%, Loss=0.6912069320678711 \t\t Validation: Acc=49%, Loss=0.6944125294685364\n",
            "Iteration: 328990 \t Train: Acc=51%, Loss=0.6929840445518494 \t\t Validation: Acc=50%, Loss=0.6957817077636719\n",
            "Iteration: 329000 \t Train: Acc=55%, Loss=0.6911298036575317 \t\t Validation: Acc=50%, Loss=0.6943328976631165\n",
            "Iteration: 329010 \t Train: Acc=53%, Loss=0.6895368099212646 \t\t Validation: Acc=48%, Loss=0.6913388967514038\n",
            "Iteration: 329020 \t Train: Acc=49%, Loss=0.6914129853248596 \t\t Validation: Acc=49%, Loss=0.6859211921691895\n",
            "Iteration: 329030 \t Train: Acc=53%, Loss=0.6883454918861389 \t\t Validation: Acc=46%, Loss=0.6969993114471436\n",
            "Iteration: 329040 \t Train: Acc=48%, Loss=0.6936439275741577 \t\t Validation: Acc=53%, Loss=0.684539794921875\n",
            "Iteration: 329050 \t Train: Acc=54%, Loss=0.6866719722747803 \t\t Validation: Acc=49%, Loss=0.6913070678710938\n",
            "Iteration: 329060 \t Train: Acc=39%, Loss=0.7021112442016602 \t\t Validation: Acc=47%, Loss=0.6962134838104248\n",
            "Iteration: 329070 \t Train: Acc=53%, Loss=0.6953112483024597 \t\t Validation: Acc=50%, Loss=0.6917571425437927\n",
            "Iteration: 329080 \t Train: Acc=47%, Loss=0.6908819675445557 \t\t Validation: Acc=47%, Loss=0.6925660967826843\n",
            "Iteration: 329090 \t Train: Acc=48%, Loss=0.6920921206474304 \t\t Validation: Acc=57%, Loss=0.6880896687507629\n",
            "Iteration: 329100 \t Train: Acc=54%, Loss=0.6951944828033447 \t\t Validation: Acc=52%, Loss=0.6933269500732422\n",
            "Iteration: 329110 \t Train: Acc=50%, Loss=0.6912212371826172 \t\t Validation: Acc=50%, Loss=0.6920692920684814\n",
            "Iteration: 329120 \t Train: Acc=50%, Loss=0.6920605301856995 \t\t Validation: Acc=47%, Loss=0.7002961039543152\n",
            "Iteration: 329130 \t Train: Acc=49%, Loss=0.6958873271942139 \t\t Validation: Acc=47%, Loss=0.6930844187736511\n",
            "Iteration: 329140 \t Train: Acc=50%, Loss=0.6905707120895386 \t\t Validation: Acc=46%, Loss=0.6994656324386597\n",
            "Iteration: 329150 \t Train: Acc=51%, Loss=0.692997932434082 \t\t Validation: Acc=50%, Loss=0.6943307518959045\n",
            "Iteration: 329160 \t Train: Acc=48%, Loss=0.6946557760238647 \t\t Validation: Acc=51%, Loss=0.7010340094566345\n",
            "Iteration: 329170 \t Train: Acc=46%, Loss=0.6951014995574951 \t\t Validation: Acc=49%, Loss=0.6934477090835571\n",
            "Iteration: 329180 \t Train: Acc=51%, Loss=0.6904847621917725 \t\t Validation: Acc=45%, Loss=0.6941924095153809\n",
            "Iteration: 329190 \t Train: Acc=48%, Loss=0.6912553310394287 \t\t Validation: Acc=53%, Loss=0.6918927431106567\n",
            "Iteration: 329200 \t Train: Acc=53%, Loss=0.6890003681182861 \t\t Validation: Acc=57%, Loss=0.6859747171401978\n",
            "Iteration: 329210 \t Train: Acc=52%, Loss=0.6936156749725342 \t\t Validation: Acc=51%, Loss=0.7052557468414307\n",
            "Iteration: 329220 \t Train: Acc=52%, Loss=0.6917204856872559 \t\t Validation: Acc=48%, Loss=0.691740870475769\n",
            "Iteration: 329230 \t Train: Acc=48%, Loss=0.6918708682060242 \t\t Validation: Acc=56%, Loss=0.6900822520256042\n",
            "Iteration: 329240 \t Train: Acc=52%, Loss=0.6876325607299805 \t\t Validation: Acc=51%, Loss=0.6888428330421448\n",
            "Iteration: 329250 \t Train: Acc=53%, Loss=0.698703408241272 \t\t Validation: Acc=48%, Loss=0.6994439959526062\n",
            "Iteration: 329260 \t Train: Acc=51%, Loss=0.7015215754508972 \t\t Validation: Acc=48%, Loss=0.6955689787864685\n",
            "Iteration: 329270 \t Train: Acc=51%, Loss=0.6847225427627563 \t\t Validation: Acc=55%, Loss=0.6925613880157471\n",
            "Iteration: 329280 \t Train: Acc=55%, Loss=0.6894130110740662 \t\t Validation: Acc=52%, Loss=0.6908876895904541\n",
            "Iteration: 329290 \t Train: Acc=54%, Loss=0.6853370666503906 \t\t Validation: Acc=48%, Loss=0.6989960074424744\n",
            "Iteration: 329300 \t Train: Acc=51%, Loss=0.6897091865539551 \t\t Validation: Acc=49%, Loss=0.6933404803276062\n",
            "Iteration: 329310 \t Train: Acc=51%, Loss=0.6855606436729431 \t\t Validation: Acc=50%, Loss=0.6939786076545715\n",
            "Iteration: 329320 \t Train: Acc=53%, Loss=0.6883087158203125 \t\t Validation: Acc=51%, Loss=0.697434663772583\n",
            "Iteration: 329330 \t Train: Acc=50%, Loss=0.695264458656311 \t\t Validation: Acc=50%, Loss=0.698619544506073\n",
            "Iteration: 329340 \t Train: Acc=51%, Loss=0.6896346211433411 \t\t Validation: Acc=52%, Loss=0.6900762319564819\n",
            "Iteration: 329350 \t Train: Acc=47%, Loss=0.6945217847824097 \t\t Validation: Acc=53%, Loss=0.6863868832588196\n",
            "Iteration: 329360 \t Train: Acc=56%, Loss=0.6888651847839355 \t\t Validation: Acc=48%, Loss=0.6931377053260803\n",
            "Iteration: 329370 \t Train: Acc=47%, Loss=0.6894915699958801 \t\t Validation: Acc=50%, Loss=0.6918125748634338\n",
            "Iteration: 329380 \t Train: Acc=53%, Loss=0.6968762874603271 \t\t Validation: Acc=52%, Loss=0.6927193403244019\n",
            "Iteration: 329390 \t Train: Acc=45%, Loss=0.6926606893539429 \t\t Validation: Acc=51%, Loss=0.6925963163375854\n",
            "Iteration: 329400 \t Train: Acc=53%, Loss=0.6900337338447571 \t\t Validation: Acc=51%, Loss=0.6895159482955933\n",
            "Iteration: 329410 \t Train: Acc=50%, Loss=0.6927638649940491 \t\t Validation: Acc=51%, Loss=0.685741662979126\n",
            "Iteration: 329420 \t Train: Acc=54%, Loss=0.689065158367157 \t\t Validation: Acc=50%, Loss=0.6927964687347412\n",
            "Iteration: 329430 \t Train: Acc=47%, Loss=0.6963464021682739 \t\t Validation: Acc=50%, Loss=0.6893166899681091\n",
            "Iteration: 329440 \t Train: Acc=46%, Loss=0.6960744857788086 \t\t Validation: Acc=46%, Loss=0.6942321062088013\n",
            "Iteration: 329450 \t Train: Acc=52%, Loss=0.6905092000961304 \t\t Validation: Acc=50%, Loss=0.6909893751144409\n",
            "Iteration: 329460 \t Train: Acc=49%, Loss=0.6865969896316528 \t\t Validation: Acc=52%, Loss=0.6936353445053101\n",
            "Iteration: 329470 \t Train: Acc=53%, Loss=0.685305655002594 \t\t Validation: Acc=49%, Loss=0.699612557888031\n",
            "Iteration: 329480 \t Train: Acc=51%, Loss=0.6919531226158142 \t\t Validation: Acc=53%, Loss=0.6892518997192383\n",
            "Iteration: 329490 \t Train: Acc=52%, Loss=0.6908107399940491 \t\t Validation: Acc=49%, Loss=0.7001225352287292\n",
            "Iteration: 329500 \t Train: Acc=50%, Loss=0.6871044039726257 \t\t Validation: Acc=50%, Loss=0.6949993968009949\n",
            "Iteration: 329510 \t Train: Acc=46%, Loss=0.6956872344017029 \t\t Validation: Acc=50%, Loss=0.6945161819458008\n",
            "Iteration: 329520 \t Train: Acc=44%, Loss=0.6937117576599121 \t\t Validation: Acc=53%, Loss=0.6858804225921631\n",
            "Iteration: 329530 \t Train: Acc=48%, Loss=0.6970503330230713 \t\t Validation: Acc=50%, Loss=0.692001223564148\n",
            "Iteration: 329540 \t Train: Acc=52%, Loss=0.6851232647895813 \t\t Validation: Acc=50%, Loss=0.69295334815979\n",
            "Iteration: 329550 \t Train: Acc=53%, Loss=0.68999844789505 \t\t Validation: Acc=52%, Loss=0.6899518966674805\n",
            "Iteration: 329560 \t Train: Acc=46%, Loss=0.694413423538208 \t\t Validation: Acc=46%, Loss=0.6945019960403442\n",
            "Iteration: 329570 \t Train: Acc=50%, Loss=0.6904017925262451 \t\t Validation: Acc=50%, Loss=0.7017819881439209\n",
            "Iteration: 329580 \t Train: Acc=46%, Loss=0.694622814655304 \t\t Validation: Acc=51%, Loss=0.6918415427207947\n",
            "Iteration: 329590 \t Train: Acc=50%, Loss=0.6938145756721497 \t\t Validation: Acc=49%, Loss=0.6954556107521057\n",
            "Iteration: 329600 \t Train: Acc=53%, Loss=0.6858736872673035 \t\t Validation: Acc=50%, Loss=0.6862726807594299\n",
            "Iteration: 329610 \t Train: Acc=50%, Loss=0.7027032971382141 \t\t Validation: Acc=54%, Loss=0.6939910054206848\n",
            "Iteration: 329620 \t Train: Acc=46%, Loss=0.6904020309448242 \t\t Validation: Acc=48%, Loss=0.691942036151886\n",
            "Iteration: 329630 \t Train: Acc=47%, Loss=0.6967211961746216 \t\t Validation: Acc=50%, Loss=0.7011160254478455\n",
            "Iteration: 329640 \t Train: Acc=59%, Loss=0.6838643550872803 \t\t Validation: Acc=53%, Loss=0.6928638219833374\n",
            "Iteration: 329650 \t Train: Acc=56%, Loss=0.6884385347366333 \t\t Validation: Acc=44%, Loss=0.6931034326553345\n",
            "Iteration: 329660 \t Train: Acc=45%, Loss=0.6948803663253784 \t\t Validation: Acc=54%, Loss=0.6953256726264954\n",
            "Iteration: 329670 \t Train: Acc=46%, Loss=0.7012336254119873 \t\t Validation: Acc=46%, Loss=0.6883246898651123\n",
            "Iteration: 329680 \t Train: Acc=50%, Loss=0.696855366230011 \t\t Validation: Acc=55%, Loss=0.6876068711280823\n",
            "Iteration: 329690 \t Train: Acc=49%, Loss=0.6915442943572998 \t\t Validation: Acc=52%, Loss=0.6907044649124146\n",
            "Iteration: 329700 \t Train: Acc=46%, Loss=0.6972641348838806 \t\t Validation: Acc=52%, Loss=0.6908565759658813\n",
            "Iteration: 329710 \t Train: Acc=55%, Loss=0.6934447884559631 \t\t Validation: Acc=50%, Loss=0.6866000890731812\n",
            "Iteration: 329720 \t Train: Acc=53%, Loss=0.6884293556213379 \t\t Validation: Acc=51%, Loss=0.6885904669761658\n",
            "Iteration: 329730 \t Train: Acc=55%, Loss=0.684888482093811 \t\t Validation: Acc=50%, Loss=0.7022423148155212\n",
            "Iteration: 329740 \t Train: Acc=49%, Loss=0.6880127191543579 \t\t Validation: Acc=50%, Loss=0.6881768703460693\n",
            "Iteration: 329750 \t Train: Acc=53%, Loss=0.6913560628890991 \t\t Validation: Acc=45%, Loss=0.6977933049201965\n",
            "Iteration: 329760 \t Train: Acc=50%, Loss=0.6883925199508667 \t\t Validation: Acc=47%, Loss=0.6999064683914185\n",
            "Iteration: 329770 \t Train: Acc=50%, Loss=0.687789797782898 \t\t Validation: Acc=50%, Loss=0.6880733966827393\n",
            "Iteration: 329780 \t Train: Acc=54%, Loss=0.6934847831726074 \t\t Validation: Acc=54%, Loss=0.6878510117530823\n",
            "Iteration: 329790 \t Train: Acc=52%, Loss=0.6959877610206604 \t\t Validation: Acc=48%, Loss=0.6912758350372314\n",
            "Iteration: 329800 \t Train: Acc=53%, Loss=0.6914645433425903 \t\t Validation: Acc=49%, Loss=0.6940392255783081\n",
            "Iteration: 329810 \t Train: Acc=44%, Loss=0.6970868110656738 \t\t Validation: Acc=51%, Loss=0.6973493695259094\n",
            "Iteration: 329820 \t Train: Acc=46%, Loss=0.6973008513450623 \t\t Validation: Acc=46%, Loss=0.695503830909729\n",
            "Iteration: 329830 \t Train: Acc=56%, Loss=0.689261257648468 \t\t Validation: Acc=45%, Loss=0.6979073286056519\n",
            "Iteration: 329840 \t Train: Acc=49%, Loss=0.6935288906097412 \t\t Validation: Acc=51%, Loss=0.6971341371536255\n",
            "Iteration: 329850 \t Train: Acc=46%, Loss=0.6982764601707458 \t\t Validation: Acc=54%, Loss=0.6916066408157349\n",
            "Iteration: 329860 \t Train: Acc=49%, Loss=0.6908559799194336 \t\t Validation: Acc=47%, Loss=0.6928340196609497\n",
            "Iteration: 329870 \t Train: Acc=50%, Loss=0.6925272941589355 \t\t Validation: Acc=50%, Loss=0.6999002695083618\n",
            "Iteration: 329880 \t Train: Acc=46%, Loss=0.700098991394043 \t\t Validation: Acc=50%, Loss=0.6852094531059265\n",
            "Iteration: 329890 \t Train: Acc=48%, Loss=0.6943707466125488 \t\t Validation: Acc=47%, Loss=0.6963374018669128\n",
            "Iteration: 329900 \t Train: Acc=49%, Loss=0.6949453353881836 \t\t Validation: Acc=50%, Loss=0.6863802671432495\n",
            "Iteration: 329910 \t Train: Acc=50%, Loss=0.6940654516220093 \t\t Validation: Acc=50%, Loss=0.6882219910621643\n",
            "Iteration: 329920 \t Train: Acc=50%, Loss=0.6943297982215881 \t\t Validation: Acc=47%, Loss=0.6931796073913574\n",
            "Iteration: 329930 \t Train: Acc=53%, Loss=0.6950470209121704 \t\t Validation: Acc=50%, Loss=0.6995522975921631\n",
            "Iteration: 329940 \t Train: Acc=43%, Loss=0.6937012672424316 \t\t Validation: Acc=47%, Loss=0.6949077844619751\n",
            "Iteration: 329950 \t Train: Acc=50%, Loss=0.6851929426193237 \t\t Validation: Acc=47%, Loss=0.6939782500267029\n",
            "Iteration: 329960 \t Train: Acc=52%, Loss=0.6915872097015381 \t\t Validation: Acc=53%, Loss=0.6916157603263855\n",
            "Iteration: 329970 \t Train: Acc=51%, Loss=0.692252516746521 \t\t Validation: Acc=50%, Loss=0.6954834461212158\n",
            "Iteration: 329980 \t Train: Acc=48%, Loss=0.6902068257331848 \t\t Validation: Acc=46%, Loss=0.6936889886856079\n",
            "Iteration: 329990 \t Train: Acc=56%, Loss=0.686012864112854 \t\t Validation: Acc=48%, Loss=0.6925307512283325\n",
            "Iteration: 330000 \t Train: Acc=44%, Loss=0.694407045841217 \t\t Validation: Acc=50%, Loss=0.6913216710090637\n",
            "Iteration: 330010 \t Train: Acc=57%, Loss=0.6898200511932373 \t\t Validation: Acc=50%, Loss=0.6872839331626892\n",
            "Iteration: 330020 \t Train: Acc=52%, Loss=0.6904316544532776 \t\t Validation: Acc=52%, Loss=0.6930220127105713\n",
            "Iteration: 330030 \t Train: Acc=53%, Loss=0.6918288469314575 \t\t Validation: Acc=46%, Loss=0.6952662467956543\n",
            "Iteration: 330040 \t Train: Acc=49%, Loss=0.6849342584609985 \t\t Validation: Acc=46%, Loss=0.6952087879180908\n",
            "Iteration: 330050 \t Train: Acc=54%, Loss=0.6863841414451599 \t\t Validation: Acc=46%, Loss=0.6970219612121582\n",
            "Iteration: 330060 \t Train: Acc=49%, Loss=0.7001200318336487 \t\t Validation: Acc=50%, Loss=0.6904643774032593\n",
            "Iteration: 330070 \t Train: Acc=52%, Loss=0.6860911250114441 \t\t Validation: Acc=49%, Loss=0.697891354560852\n",
            "Iteration: 330080 \t Train: Acc=46%, Loss=0.6902462244033813 \t\t Validation: Acc=50%, Loss=0.6887560486793518\n",
            "Iteration: 330090 \t Train: Acc=56%, Loss=0.6837033629417419 \t\t Validation: Acc=51%, Loss=0.681583821773529\n",
            "Iteration: 330100 \t Train: Acc=49%, Loss=0.6930643916130066 \t\t Validation: Acc=51%, Loss=0.6944495439529419\n",
            "Iteration: 330110 \t Train: Acc=51%, Loss=0.6963714361190796 \t\t Validation: Acc=53%, Loss=0.6996453404426575\n",
            "Iteration: 330120 \t Train: Acc=48%, Loss=0.6918025016784668 \t\t Validation: Acc=47%, Loss=0.6922406554222107\n",
            "Iteration: 330130 \t Train: Acc=50%, Loss=0.689182698726654 \t\t Validation: Acc=48%, Loss=0.6991880536079407\n",
            "Iteration: 330140 \t Train: Acc=50%, Loss=0.6960440874099731 \t\t Validation: Acc=50%, Loss=0.6902263164520264\n",
            "Iteration: 330150 \t Train: Acc=50%, Loss=0.6924421787261963 \t\t Validation: Acc=52%, Loss=0.6855270862579346\n",
            "Iteration: 330160 \t Train: Acc=52%, Loss=0.7005212903022766 \t\t Validation: Acc=46%, Loss=0.6967219114303589\n",
            "Iteration: 330170 \t Train: Acc=51%, Loss=0.6944435834884644 \t\t Validation: Acc=50%, Loss=0.6929262280464172\n",
            "Iteration: 330180 \t Train: Acc=46%, Loss=0.693961501121521 \t\t Validation: Acc=48%, Loss=0.6938807964324951\n",
            "Iteration: 330190 \t Train: Acc=50%, Loss=0.6892382502555847 \t\t Validation: Acc=53%, Loss=0.686923086643219\n",
            "Iteration: 330200 \t Train: Acc=53%, Loss=0.6981644034385681 \t\t Validation: Acc=50%, Loss=0.6898910999298096\n",
            "Iteration: 330210 \t Train: Acc=48%, Loss=0.6966555118560791 \t\t Validation: Acc=55%, Loss=0.6905760169029236\n",
            "Iteration: 330220 \t Train: Acc=59%, Loss=0.6861364841461182 \t\t Validation: Acc=51%, Loss=0.6908984780311584\n",
            "Iteration: 330230 \t Train: Acc=50%, Loss=0.6968735456466675 \t\t Validation: Acc=50%, Loss=0.6922136545181274\n",
            "Iteration: 330240 \t Train: Acc=57%, Loss=0.6830815672874451 \t\t Validation: Acc=48%, Loss=0.6982343196868896\n",
            "Iteration: 330250 \t Train: Acc=46%, Loss=0.6930816173553467 \t\t Validation: Acc=51%, Loss=0.7001249194145203\n",
            "Iteration: 330260 \t Train: Acc=50%, Loss=0.6931269764900208 \t\t Validation: Acc=52%, Loss=0.6954175233840942\n",
            "Iteration: 330270 \t Train: Acc=50%, Loss=0.6956359148025513 \t\t Validation: Acc=49%, Loss=0.6931466460227966\n",
            "Iteration: 330280 \t Train: Acc=51%, Loss=0.6890653967857361 \t\t Validation: Acc=53%, Loss=0.68296879529953\n",
            "Iteration: 330290 \t Train: Acc=49%, Loss=0.6931120157241821 \t\t Validation: Acc=49%, Loss=0.6930345892906189\n",
            "Iteration: 330300 \t Train: Acc=49%, Loss=0.6919059753417969 \t\t Validation: Acc=48%, Loss=0.692862331867218\n",
            "Iteration: 330310 \t Train: Acc=51%, Loss=0.6923160552978516 \t\t Validation: Acc=49%, Loss=0.7038257122039795\n",
            "Iteration: 330320 \t Train: Acc=51%, Loss=0.6901751160621643 \t\t Validation: Acc=46%, Loss=0.6917992830276489\n",
            "Iteration: 330330 \t Train: Acc=51%, Loss=0.6913654208183289 \t\t Validation: Acc=51%, Loss=0.6999786496162415\n",
            "Iteration: 330340 \t Train: Acc=55%, Loss=0.6864926815032959 \t\t Validation: Acc=53%, Loss=0.690414309501648\n",
            "Iteration: 330350 \t Train: Acc=53%, Loss=0.6891041994094849 \t\t Validation: Acc=50%, Loss=0.6892345547676086\n",
            "Iteration: 330360 \t Train: Acc=46%, Loss=0.693674623966217 \t\t Validation: Acc=48%, Loss=0.6964383125305176\n",
            "Iteration: 330370 \t Train: Acc=55%, Loss=0.6868204474449158 \t\t Validation: Acc=51%, Loss=0.6880241632461548\n",
            "Iteration: 330380 \t Train: Acc=53%, Loss=0.695103108882904 \t\t Validation: Acc=50%, Loss=0.6945857405662537\n",
            "Iteration: 330390 \t Train: Acc=50%, Loss=0.6928071975708008 \t\t Validation: Acc=52%, Loss=0.6946336030960083\n",
            "Iteration: 330400 \t Train: Acc=52%, Loss=0.6907212734222412 \t\t Validation: Acc=50%, Loss=0.6965048909187317\n",
            "Iteration: 330410 \t Train: Acc=51%, Loss=0.6931971907615662 \t\t Validation: Acc=50%, Loss=0.6975719928741455\n",
            "Iteration: 330420 \t Train: Acc=48%, Loss=0.6951628923416138 \t\t Validation: Acc=48%, Loss=0.698638379573822\n",
            "Iteration: 330430 \t Train: Acc=52%, Loss=0.6946644186973572 \t\t Validation: Acc=51%, Loss=0.6881275773048401\n",
            "Iteration: 330440 \t Train: Acc=47%, Loss=0.6911121010780334 \t\t Validation: Acc=49%, Loss=0.6961254477500916\n",
            "Iteration: 330450 \t Train: Acc=51%, Loss=0.6933419704437256 \t\t Validation: Acc=50%, Loss=0.6980467438697815\n",
            "Iteration: 330460 \t Train: Acc=52%, Loss=0.6916123628616333 \t\t Validation: Acc=55%, Loss=0.6861733794212341\n",
            "Iteration: 330470 \t Train: Acc=50%, Loss=0.6971815824508667 \t\t Validation: Acc=53%, Loss=0.6872719526290894\n",
            "Iteration: 330480 \t Train: Acc=54%, Loss=0.6850794553756714 \t\t Validation: Acc=49%, Loss=0.6907656192779541\n",
            "Iteration: 330490 \t Train: Acc=46%, Loss=0.6946321725845337 \t\t Validation: Acc=49%, Loss=0.692730188369751\n",
            "Iteration: 330500 \t Train: Acc=49%, Loss=0.6956872940063477 \t\t Validation: Acc=48%, Loss=0.6978082060813904\n",
            "Iteration: 330510 \t Train: Acc=53%, Loss=0.6855762600898743 \t\t Validation: Acc=54%, Loss=0.6946535110473633\n",
            "Iteration: 330520 \t Train: Acc=53%, Loss=0.6891287565231323 \t\t Validation: Acc=53%, Loss=0.6888054609298706\n",
            "Iteration: 330530 \t Train: Acc=50%, Loss=0.6909999847412109 \t\t Validation: Acc=50%, Loss=0.7003148794174194\n",
            "Iteration: 330540 \t Train: Acc=56%, Loss=0.6882262825965881 \t\t Validation: Acc=50%, Loss=0.6947187781333923\n",
            "Iteration: 330550 \t Train: Acc=48%, Loss=0.697304368019104 \t\t Validation: Acc=49%, Loss=0.6941524147987366\n",
            "Iteration: 330560 \t Train: Acc=49%, Loss=0.6913222670555115 \t\t Validation: Acc=45%, Loss=0.6955291032791138\n",
            "Iteration: 330570 \t Train: Acc=43%, Loss=0.6942106485366821 \t\t Validation: Acc=47%, Loss=0.6952913999557495\n",
            "Iteration: 330580 \t Train: Acc=52%, Loss=0.6988809108734131 \t\t Validation: Acc=55%, Loss=0.6927473545074463\n",
            "Iteration: 330590 \t Train: Acc=49%, Loss=0.6927028298377991 \t\t Validation: Acc=52%, Loss=0.6913962364196777\n",
            "Iteration: 330600 \t Train: Acc=47%, Loss=0.6939067244529724 \t\t Validation: Acc=54%, Loss=0.6925076246261597\n",
            "Iteration: 330610 \t Train: Acc=49%, Loss=0.6926555633544922 \t\t Validation: Acc=46%, Loss=0.6966856122016907\n",
            "Iteration: 330620 \t Train: Acc=50%, Loss=0.6969494223594666 \t\t Validation: Acc=46%, Loss=0.6989760398864746\n",
            "Iteration: 330630 \t Train: Acc=52%, Loss=0.6866424083709717 \t\t Validation: Acc=50%, Loss=0.6987031102180481\n",
            "Iteration: 330640 \t Train: Acc=50%, Loss=0.6971663236618042 \t\t Validation: Acc=51%, Loss=0.6937414407730103\n",
            "Iteration: 330650 \t Train: Acc=50%, Loss=0.692958414554596 \t\t Validation: Acc=49%, Loss=0.6896698474884033\n",
            "Iteration: 330660 \t Train: Acc=49%, Loss=0.69381183385849 \t\t Validation: Acc=50%, Loss=0.6956421732902527\n",
            "Iteration: 330670 \t Train: Acc=47%, Loss=0.6975767612457275 \t\t Validation: Acc=51%, Loss=0.689839780330658\n",
            "Iteration: 330680 \t Train: Acc=49%, Loss=0.6981162428855896 \t\t Validation: Acc=53%, Loss=0.6870850324630737\n",
            "Iteration: 330690 \t Train: Acc=59%, Loss=0.6863034963607788 \t\t Validation: Acc=54%, Loss=0.6821459531784058\n",
            "Iteration: 330700 \t Train: Acc=46%, Loss=0.6920293569564819 \t\t Validation: Acc=51%, Loss=0.6886807680130005\n",
            "Iteration: 330710 \t Train: Acc=48%, Loss=0.6952455639839172 \t\t Validation: Acc=53%, Loss=0.6919924020767212\n",
            "Iteration: 330720 \t Train: Acc=59%, Loss=0.6872576475143433 \t\t Validation: Acc=49%, Loss=0.6950991153717041\n",
            "Iteration: 330730 \t Train: Acc=53%, Loss=0.6833800673484802 \t\t Validation: Acc=52%, Loss=0.6896277666091919\n",
            "Iteration: 330740 \t Train: Acc=53%, Loss=0.6970686316490173 \t\t Validation: Acc=53%, Loss=0.6871463060379028\n",
            "Iteration: 330750 \t Train: Acc=52%, Loss=0.6891803741455078 \t\t Validation: Acc=50%, Loss=0.6930543184280396\n",
            "Iteration: 330760 \t Train: Acc=52%, Loss=0.6870676875114441 \t\t Validation: Acc=44%, Loss=0.6968365907669067\n",
            "Iteration: 330770 \t Train: Acc=57%, Loss=0.6910642385482788 \t\t Validation: Acc=46%, Loss=0.6973485350608826\n",
            "Iteration: 330780 \t Train: Acc=47%, Loss=0.6906746625900269 \t\t Validation: Acc=50%, Loss=0.6937433481216431\n",
            "Iteration: 330790 \t Train: Acc=53%, Loss=0.6873741149902344 \t\t Validation: Acc=52%, Loss=0.6918036937713623\n",
            "Iteration: 330800 \t Train: Acc=52%, Loss=0.6903282403945923 \t\t Validation: Acc=48%, Loss=0.6935704946517944\n",
            "Iteration: 330810 \t Train: Acc=50%, Loss=0.6986369490623474 \t\t Validation: Acc=54%, Loss=0.6869743466377258\n",
            "Iteration: 330820 \t Train: Acc=47%, Loss=0.6999748349189758 \t\t Validation: Acc=50%, Loss=0.6934834718704224\n",
            "Iteration: 330830 \t Train: Acc=50%, Loss=0.6924268007278442 \t\t Validation: Acc=46%, Loss=0.6986935138702393\n",
            "Iteration: 330840 \t Train: Acc=52%, Loss=0.6876775026321411 \t\t Validation: Acc=50%, Loss=0.6993445754051208\n",
            "Iteration: 330850 \t Train: Acc=51%, Loss=0.6903895735740662 \t\t Validation: Acc=52%, Loss=0.6874574422836304\n",
            "Iteration: 330860 \t Train: Acc=53%, Loss=0.6902341842651367 \t\t Validation: Acc=49%, Loss=0.7007743120193481\n",
            "Iteration: 330870 \t Train: Acc=52%, Loss=0.6917651891708374 \t\t Validation: Acc=47%, Loss=0.697594165802002\n",
            "Iteration: 330880 \t Train: Acc=47%, Loss=0.6920201778411865 \t\t Validation: Acc=53%, Loss=0.6894680857658386\n",
            "Iteration: 330890 \t Train: Acc=43%, Loss=0.6924142241477966 \t\t Validation: Acc=50%, Loss=0.6866862773895264\n",
            "Iteration: 330900 \t Train: Acc=55%, Loss=0.6859685182571411 \t\t Validation: Acc=55%, Loss=0.6909714341163635\n",
            "Iteration: 330910 \t Train: Acc=49%, Loss=0.692913830280304 \t\t Validation: Acc=57%, Loss=0.6887313723564148\n",
            "Iteration: 330920 \t Train: Acc=50%, Loss=0.6956326961517334 \t\t Validation: Acc=50%, Loss=0.6845516562461853\n",
            "Iteration: 330930 \t Train: Acc=53%, Loss=0.6931031346321106 \t\t Validation: Acc=53%, Loss=0.6861001253128052\n",
            "Iteration: 330940 \t Train: Acc=50%, Loss=0.6932417750358582 \t\t Validation: Acc=52%, Loss=0.6981737613677979\n",
            "Iteration: 330950 \t Train: Acc=49%, Loss=0.6947338581085205 \t\t Validation: Acc=48%, Loss=0.69196617603302\n",
            "Iteration: 330960 \t Train: Acc=48%, Loss=0.6945599317550659 \t\t Validation: Acc=52%, Loss=0.694518506526947\n",
            "Iteration: 330970 \t Train: Acc=50%, Loss=0.7000182867050171 \t\t Validation: Acc=58%, Loss=0.6861743330955505\n",
            "Iteration: 330980 \t Train: Acc=47%, Loss=0.69693922996521 \t\t Validation: Acc=51%, Loss=0.6935920119285583\n",
            "Iteration: 330990 \t Train: Acc=53%, Loss=0.6841235756874084 \t\t Validation: Acc=48%, Loss=0.6981416940689087\n",
            "Iteration: 331000 \t Train: Acc=48%, Loss=0.6951144933700562 \t\t Validation: Acc=50%, Loss=0.6914199590682983\n",
            "Iteration: 331010 \t Train: Acc=49%, Loss=0.6902847290039062 \t\t Validation: Acc=50%, Loss=0.6951220035552979\n",
            "Iteration: 331020 \t Train: Acc=44%, Loss=0.6990143060684204 \t\t Validation: Acc=53%, Loss=0.6982074975967407\n",
            "Iteration: 331030 \t Train: Acc=50%, Loss=0.6973857283592224 \t\t Validation: Acc=50%, Loss=0.6905006766319275\n",
            "Iteration: 331040 \t Train: Acc=50%, Loss=0.6869121193885803 \t\t Validation: Acc=50%, Loss=0.6993988156318665\n",
            "Iteration: 331050 \t Train: Acc=60%, Loss=0.6883989572525024 \t\t Validation: Acc=50%, Loss=0.6912719011306763\n",
            "Iteration: 331060 \t Train: Acc=49%, Loss=0.6953160166740417 \t\t Validation: Acc=54%, Loss=0.6880627274513245\n",
            "Iteration: 331070 \t Train: Acc=54%, Loss=0.6842038035392761 \t\t Validation: Acc=50%, Loss=0.6956623792648315\n",
            "Iteration: 331080 \t Train: Acc=59%, Loss=0.6852579712867737 \t\t Validation: Acc=53%, Loss=0.6940428018569946\n",
            "Iteration: 331090 \t Train: Acc=49%, Loss=0.6868411302566528 \t\t Validation: Acc=51%, Loss=0.688780665397644\n",
            "Iteration: 331100 \t Train: Acc=50%, Loss=0.693057119846344 \t\t Validation: Acc=55%, Loss=0.6874803304672241\n",
            "Iteration: 331110 \t Train: Acc=45%, Loss=0.6957883834838867 \t\t Validation: Acc=49%, Loss=0.6988885402679443\n",
            "Iteration: 331120 \t Train: Acc=50%, Loss=0.6914409399032593 \t\t Validation: Acc=53%, Loss=0.689428448677063\n",
            "Iteration: 331130 \t Train: Acc=51%, Loss=0.6921457052230835 \t\t Validation: Acc=53%, Loss=0.686399519443512\n",
            "Iteration: 331140 \t Train: Acc=51%, Loss=0.6830810308456421 \t\t Validation: Acc=46%, Loss=0.691331148147583\n",
            "Iteration: 331150 \t Train: Acc=48%, Loss=0.6914905309677124 \t\t Validation: Acc=51%, Loss=0.6913225650787354\n",
            "Iteration: 331160 \t Train: Acc=58%, Loss=0.6824883818626404 \t\t Validation: Acc=49%, Loss=0.6952621340751648\n",
            "Iteration: 331170 \t Train: Acc=55%, Loss=0.6906225085258484 \t\t Validation: Acc=53%, Loss=0.6958538889884949\n",
            "Iteration: 331180 \t Train: Acc=51%, Loss=0.692123293876648 \t\t Validation: Acc=49%, Loss=0.6987523436546326\n",
            "Iteration: 331190 \t Train: Acc=52%, Loss=0.690098226070404 \t\t Validation: Acc=51%, Loss=0.6954488754272461\n",
            "Iteration: 331200 \t Train: Acc=53%, Loss=0.6897486448287964 \t\t Validation: Acc=52%, Loss=0.685305118560791\n",
            "Iteration: 331210 \t Train: Acc=55%, Loss=0.682643711566925 \t\t Validation: Acc=52%, Loss=0.6918022632598877\n",
            "Iteration: 331220 \t Train: Acc=50%, Loss=0.6988620162010193 \t\t Validation: Acc=51%, Loss=0.6976714730262756\n",
            "Iteration: 331230 \t Train: Acc=47%, Loss=0.6947640180587769 \t\t Validation: Acc=50%, Loss=0.6955105662345886\n",
            "Iteration: 331240 \t Train: Acc=52%, Loss=0.6915015578269958 \t\t Validation: Acc=50%, Loss=0.6931537985801697\n",
            "Iteration: 331250 \t Train: Acc=50%, Loss=0.6904211044311523 \t\t Validation: Acc=53%, Loss=0.6869384050369263\n",
            "Iteration: 331260 \t Train: Acc=50%, Loss=0.6958279013633728 \t\t Validation: Acc=47%, Loss=0.6920303106307983\n",
            "Iteration: 331270 \t Train: Acc=50%, Loss=0.6927220225334167 \t\t Validation: Acc=48%, Loss=0.6936806440353394\n",
            "Iteration: 331280 \t Train: Acc=53%, Loss=0.6936192512512207 \t\t Validation: Acc=48%, Loss=0.6976774334907532\n",
            "Iteration: 331290 \t Train: Acc=47%, Loss=0.6955831050872803 \t\t Validation: Acc=46%, Loss=0.695810079574585\n",
            "Iteration: 331300 \t Train: Acc=51%, Loss=0.6904605627059937 \t\t Validation: Acc=45%, Loss=0.6957364678382874\n",
            "Iteration: 331310 \t Train: Acc=50%, Loss=0.6888401508331299 \t\t Validation: Acc=46%, Loss=0.6971069574356079\n",
            "Iteration: 331320 \t Train: Acc=52%, Loss=0.6851202249526978 \t\t Validation: Acc=50%, Loss=0.6931950449943542\n",
            "Iteration: 331330 \t Train: Acc=53%, Loss=0.6856955885887146 \t\t Validation: Acc=51%, Loss=0.6839556694030762\n",
            "Iteration: 331340 \t Train: Acc=52%, Loss=0.6872049570083618 \t\t Validation: Acc=47%, Loss=0.6897974014282227\n",
            "Iteration: 331350 \t Train: Acc=47%, Loss=0.6931512951850891 \t\t Validation: Acc=48%, Loss=0.6938462257385254\n",
            "Iteration: 331360 \t Train: Acc=46%, Loss=0.695334255695343 \t\t Validation: Acc=58%, Loss=0.685717761516571\n",
            "Iteration: 331370 \t Train: Acc=50%, Loss=0.6922196745872498 \t\t Validation: Acc=50%, Loss=0.6891506910324097\n",
            "Iteration: 331380 \t Train: Acc=51%, Loss=0.6921030879020691 \t\t Validation: Acc=50%, Loss=0.6951883435249329\n",
            "Iteration: 331390 \t Train: Acc=53%, Loss=0.6883927583694458 \t\t Validation: Acc=52%, Loss=0.6878069639205933\n",
            "Iteration: 331400 \t Train: Acc=53%, Loss=0.6948985457420349 \t\t Validation: Acc=49%, Loss=0.6906757354736328\n",
            "Iteration: 331410 \t Train: Acc=46%, Loss=0.6920160055160522 \t\t Validation: Acc=52%, Loss=0.6927103400230408\n",
            "Iteration: 331420 \t Train: Acc=51%, Loss=0.6942926645278931 \t\t Validation: Acc=51%, Loss=0.6920019388198853\n",
            "Iteration: 331430 \t Train: Acc=46%, Loss=0.6986659169197083 \t\t Validation: Acc=51%, Loss=0.6902993321418762\n",
            "Iteration: 331440 \t Train: Acc=50%, Loss=0.6910288333892822 \t\t Validation: Acc=49%, Loss=0.691072940826416\n",
            "Iteration: 331450 \t Train: Acc=50%, Loss=0.6926653981208801 \t\t Validation: Acc=49%, Loss=0.697403609752655\n",
            "Iteration: 331460 \t Train: Acc=53%, Loss=0.693109929561615 \t\t Validation: Acc=50%, Loss=0.6946372389793396\n",
            "Iteration: 331470 \t Train: Acc=42%, Loss=0.6951849460601807 \t\t Validation: Acc=49%, Loss=0.6982238292694092\n",
            "Iteration: 331480 \t Train: Acc=51%, Loss=0.6850288510322571 \t\t Validation: Acc=53%, Loss=0.6909188628196716\n",
            "Iteration: 331490 \t Train: Acc=53%, Loss=0.688601016998291 \t\t Validation: Acc=49%, Loss=0.6941894292831421\n",
            "Iteration: 331500 \t Train: Acc=49%, Loss=0.693578839302063 \t\t Validation: Acc=44%, Loss=0.6984512209892273\n",
            "Iteration: 331510 \t Train: Acc=45%, Loss=0.6974980235099792 \t\t Validation: Acc=49%, Loss=0.6943989992141724\n",
            "Iteration: 331520 \t Train: Acc=49%, Loss=0.684260368347168 \t\t Validation: Acc=51%, Loss=0.6905198693275452\n",
            "Iteration: 331530 \t Train: Acc=47%, Loss=0.6928560137748718 \t\t Validation: Acc=50%, Loss=0.6928015947341919\n",
            "Iteration: 331540 \t Train: Acc=46%, Loss=0.6933374404907227 \t\t Validation: Acc=47%, Loss=0.6983225345611572\n",
            "Iteration: 331550 \t Train: Acc=49%, Loss=0.6910847425460815 \t\t Validation: Acc=48%, Loss=0.6967725157737732\n",
            "Iteration: 331560 \t Train: Acc=51%, Loss=0.6941081881523132 \t\t Validation: Acc=52%, Loss=0.6893351674079895\n",
            "Iteration: 331570 \t Train: Acc=43%, Loss=0.6918786764144897 \t\t Validation: Acc=50%, Loss=0.6938585638999939\n",
            "Iteration: 331580 \t Train: Acc=52%, Loss=0.6939882040023804 \t\t Validation: Acc=52%, Loss=0.6919776797294617\n",
            "Iteration: 331590 \t Train: Acc=50%, Loss=0.690733790397644 \t\t Validation: Acc=54%, Loss=0.6871881484985352\n",
            "Iteration: 331600 \t Train: Acc=51%, Loss=0.689100444316864 \t\t Validation: Acc=50%, Loss=0.6912620067596436\n",
            "Iteration: 331610 \t Train: Acc=48%, Loss=0.6902121305465698 \t\t Validation: Acc=52%, Loss=0.6897937059402466\n",
            "Iteration: 331620 \t Train: Acc=53%, Loss=0.6913144588470459 \t\t Validation: Acc=47%, Loss=0.7029537558555603\n",
            "Iteration: 331630 \t Train: Acc=57%, Loss=0.687778115272522 \t\t Validation: Acc=50%, Loss=0.6897299289703369\n",
            "Iteration: 331640 \t Train: Acc=50%, Loss=0.6905360221862793 \t\t Validation: Acc=53%, Loss=0.6914501190185547\n",
            "Iteration: 331650 \t Train: Acc=49%, Loss=0.696144700050354 \t\t Validation: Acc=50%, Loss=0.6884005069732666\n",
            "Iteration: 331660 \t Train: Acc=54%, Loss=0.6878548264503479 \t\t Validation: Acc=46%, Loss=0.6996423602104187\n",
            "Iteration: 331670 \t Train: Acc=55%, Loss=0.6884769201278687 \t\t Validation: Acc=50%, Loss=0.6937276124954224\n",
            "Iteration: 331680 \t Train: Acc=53%, Loss=0.6883789896965027 \t\t Validation: Acc=51%, Loss=0.692414402961731\n",
            "Iteration: 331690 \t Train: Acc=47%, Loss=0.6933064460754395 \t\t Validation: Acc=46%, Loss=0.6969800591468811\n",
            "Iteration: 331700 \t Train: Acc=46%, Loss=0.696724534034729 \t\t Validation: Acc=49%, Loss=0.6986061334609985\n",
            "Iteration: 331710 \t Train: Acc=52%, Loss=0.6917159557342529 \t\t Validation: Acc=52%, Loss=0.695834219455719\n",
            "Iteration: 331720 \t Train: Acc=46%, Loss=0.6989456415176392 \t\t Validation: Acc=50%, Loss=0.6908954381942749\n",
            "Iteration: 331730 \t Train: Acc=42%, Loss=0.6991219520568848 \t\t Validation: Acc=54%, Loss=0.6896327137947083\n",
            "Iteration: 331740 \t Train: Acc=48%, Loss=0.6948007345199585 \t\t Validation: Acc=54%, Loss=0.6865122318267822\n",
            "Iteration: 331750 \t Train: Acc=43%, Loss=0.6840284466743469 \t\t Validation: Acc=55%, Loss=0.6892597079277039\n",
            "Iteration: 331760 \t Train: Acc=55%, Loss=0.6905509233474731 \t\t Validation: Acc=53%, Loss=0.6907849907875061\n",
            "Iteration: 331770 \t Train: Acc=44%, Loss=0.695769190788269 \t\t Validation: Acc=50%, Loss=0.6952569484710693\n",
            "Iteration: 331780 \t Train: Acc=50%, Loss=0.6960528492927551 \t\t Validation: Acc=47%, Loss=0.6959520578384399\n",
            "Iteration: 331790 \t Train: Acc=52%, Loss=0.6922612190246582 \t\t Validation: Acc=60%, Loss=0.6825484037399292\n",
            "Iteration: 331800 \t Train: Acc=48%, Loss=0.6926382780075073 \t\t Validation: Acc=53%, Loss=0.6864281296730042\n",
            "Iteration: 331810 \t Train: Acc=53%, Loss=0.6894080638885498 \t\t Validation: Acc=50%, Loss=0.6935226917266846\n",
            "Iteration: 331820 \t Train: Acc=51%, Loss=0.6911829710006714 \t\t Validation: Acc=50%, Loss=0.6905582547187805\n",
            "Iteration: 331830 \t Train: Acc=49%, Loss=0.6902722716331482 \t\t Validation: Acc=50%, Loss=0.6934866309165955\n",
            "Iteration: 331840 \t Train: Acc=48%, Loss=0.6932346224784851 \t\t Validation: Acc=49%, Loss=0.6917991638183594\n",
            "Iteration: 331850 \t Train: Acc=54%, Loss=0.6899505853652954 \t\t Validation: Acc=44%, Loss=0.6985286474227905\n",
            "Iteration: 331860 \t Train: Acc=53%, Loss=0.6931418180465698 \t\t Validation: Acc=50%, Loss=0.689656674861908\n",
            "Iteration: 331870 \t Train: Acc=52%, Loss=0.6928563117980957 \t\t Validation: Acc=50%, Loss=0.690130352973938\n",
            "Iteration: 331880 \t Train: Acc=53%, Loss=0.6924285888671875 \t\t Validation: Acc=50%, Loss=0.6882084608078003\n",
            "Iteration: 331890 \t Train: Acc=53%, Loss=0.6913145780563354 \t\t Validation: Acc=52%, Loss=0.6929136514663696\n",
            "Iteration: 331900 \t Train: Acc=54%, Loss=0.6965214014053345 \t\t Validation: Acc=49%, Loss=0.6899887919425964\n",
            "Iteration: 331910 \t Train: Acc=53%, Loss=0.6887208223342896 \t\t Validation: Acc=47%, Loss=0.6970892548561096\n",
            "Iteration: 331920 \t Train: Acc=49%, Loss=0.6934553384780884 \t\t Validation: Acc=51%, Loss=0.6928091049194336\n",
            "Iteration: 331930 \t Train: Acc=57%, Loss=0.6840673685073853 \t\t Validation: Acc=50%, Loss=0.691214919090271\n",
            "Iteration: 331940 \t Train: Acc=57%, Loss=0.6827970743179321 \t\t Validation: Acc=47%, Loss=0.6934183835983276\n",
            "Iteration: 331950 \t Train: Acc=50%, Loss=0.6886289715766907 \t\t Validation: Acc=49%, Loss=0.6941236853599548\n",
            "Iteration: 331960 \t Train: Acc=47%, Loss=0.6983364820480347 \t\t Validation: Acc=44%, Loss=0.6953887939453125\n",
            "Iteration: 331970 \t Train: Acc=50%, Loss=0.6971664428710938 \t\t Validation: Acc=50%, Loss=0.6931917667388916\n",
            "Iteration: 331980 \t Train: Acc=55%, Loss=0.6931447982788086 \t\t Validation: Acc=47%, Loss=0.7001774907112122\n",
            "Iteration: 331990 \t Train: Acc=50%, Loss=0.6931846141815186 \t\t Validation: Acc=46%, Loss=0.6953493356704712\n",
            "Iteration: 332000 \t Train: Acc=52%, Loss=0.6929914355278015 \t\t Validation: Acc=51%, Loss=0.6959437131881714\n",
            "Iteration: 332010 \t Train: Acc=53%, Loss=0.6882416009902954 \t\t Validation: Acc=41%, Loss=0.705240786075592\n",
            "Iteration: 332020 \t Train: Acc=50%, Loss=0.6926466822624207 \t\t Validation: Acc=47%, Loss=0.6911925673484802\n",
            "Iteration: 332030 \t Train: Acc=54%, Loss=0.6876844167709351 \t\t Validation: Acc=47%, Loss=0.7012178897857666\n",
            "Iteration: 332040 \t Train: Acc=46%, Loss=0.7005245089530945 \t\t Validation: Acc=51%, Loss=0.6949175000190735\n",
            "Iteration: 332050 \t Train: Acc=52%, Loss=0.6950203776359558 \t\t Validation: Acc=48%, Loss=0.6952071189880371\n",
            "Iteration: 332060 \t Train: Acc=52%, Loss=0.6910569667816162 \t\t Validation: Acc=45%, Loss=0.6971468329429626\n",
            "Iteration: 332070 \t Train: Acc=51%, Loss=0.6930240392684937 \t\t Validation: Acc=57%, Loss=0.6855409741401672\n",
            "Iteration: 332080 \t Train: Acc=50%, Loss=0.6889511346817017 \t\t Validation: Acc=52%, Loss=0.6932295560836792\n",
            "Iteration: 332090 \t Train: Acc=53%, Loss=0.6949484944343567 \t\t Validation: Acc=46%, Loss=0.7002557516098022\n",
            "Iteration: 332100 \t Train: Acc=48%, Loss=0.6955622434616089 \t\t Validation: Acc=51%, Loss=0.6907069087028503\n",
            "Iteration: 332110 \t Train: Acc=52%, Loss=0.6895452737808228 \t\t Validation: Acc=53%, Loss=0.6850631237030029\n",
            "Iteration: 332120 \t Train: Acc=46%, Loss=0.6957178115844727 \t\t Validation: Acc=55%, Loss=0.6848223209381104\n",
            "Iteration: 332130 \t Train: Acc=48%, Loss=0.6918321847915649 \t\t Validation: Acc=49%, Loss=0.6982364654541016\n",
            "Iteration: 332140 \t Train: Acc=51%, Loss=0.6926845908164978 \t\t Validation: Acc=53%, Loss=0.6937608122825623\n",
            "Iteration: 332150 \t Train: Acc=51%, Loss=0.6960774660110474 \t\t Validation: Acc=53%, Loss=0.6902868747711182\n",
            "Iteration: 332160 \t Train: Acc=53%, Loss=0.6843953728675842 \t\t Validation: Acc=49%, Loss=0.691741943359375\n",
            "Iteration: 332170 \t Train: Acc=51%, Loss=0.6925312280654907 \t\t Validation: Acc=51%, Loss=0.6911694407463074\n",
            "Iteration: 332180 \t Train: Acc=47%, Loss=0.6928258538246155 \t\t Validation: Acc=50%, Loss=0.6915723085403442\n",
            "Iteration: 332190 \t Train: Acc=50%, Loss=0.6939581036567688 \t\t Validation: Acc=50%, Loss=0.6899431943893433\n",
            "Iteration: 332200 \t Train: Acc=50%, Loss=0.6925119757652283 \t\t Validation: Acc=53%, Loss=0.6901102066040039\n",
            "Iteration: 332210 \t Train: Acc=53%, Loss=0.6954033970832825 \t\t Validation: Acc=49%, Loss=0.6918739676475525\n",
            "Iteration: 332220 \t Train: Acc=51%, Loss=0.6900726556777954 \t\t Validation: Acc=48%, Loss=0.6944749355316162\n",
            "Iteration: 332230 \t Train: Acc=51%, Loss=0.6936861872673035 \t\t Validation: Acc=50%, Loss=0.6921349763870239\n",
            "Iteration: 332240 \t Train: Acc=54%, Loss=0.6912907361984253 \t\t Validation: Acc=50%, Loss=0.6904259920120239\n",
            "Iteration: 332250 \t Train: Acc=53%, Loss=0.6926679015159607 \t\t Validation: Acc=50%, Loss=0.6924277544021606\n",
            "Iteration: 332260 \t Train: Acc=44%, Loss=0.6982290744781494 \t\t Validation: Acc=48%, Loss=0.696239173412323\n",
            "Iteration: 332270 \t Train: Acc=42%, Loss=0.7014746069908142 \t\t Validation: Acc=49%, Loss=0.6906023621559143\n",
            "Iteration: 332280 \t Train: Acc=46%, Loss=0.695975124835968 \t\t Validation: Acc=54%, Loss=0.6927356719970703\n",
            "Iteration: 332290 \t Train: Acc=52%, Loss=0.6875783801078796 \t\t Validation: Acc=54%, Loss=0.6916323900222778\n",
            "Iteration: 332300 \t Train: Acc=50%, Loss=0.6917997002601624 \t\t Validation: Acc=48%, Loss=0.6944577693939209\n",
            "Iteration: 332310 \t Train: Acc=55%, Loss=0.6924488544464111 \t\t Validation: Acc=48%, Loss=0.696453332901001\n",
            "Iteration: 332320 \t Train: Acc=51%, Loss=0.6917083263397217 \t\t Validation: Acc=44%, Loss=0.7054386138916016\n",
            "Iteration: 332330 \t Train: Acc=55%, Loss=0.6888036727905273 \t\t Validation: Acc=48%, Loss=0.7018557786941528\n",
            "Iteration: 332340 \t Train: Acc=51%, Loss=0.6895893812179565 \t\t Validation: Acc=49%, Loss=0.6933998465538025\n",
            "Iteration: 332350 \t Train: Acc=50%, Loss=0.6914970874786377 \t\t Validation: Acc=50%, Loss=0.690460205078125\n",
            "Iteration: 332360 \t Train: Acc=50%, Loss=0.6940532922744751 \t\t Validation: Acc=53%, Loss=0.6910722255706787\n",
            "Iteration: 332370 \t Train: Acc=49%, Loss=0.6948061585426331 \t\t Validation: Acc=49%, Loss=0.688195526599884\n",
            "Iteration: 332380 \t Train: Acc=46%, Loss=0.6962083578109741 \t\t Validation: Acc=49%, Loss=0.6954593062400818\n",
            "Iteration: 332390 \t Train: Acc=50%, Loss=0.6918321251869202 \t\t Validation: Acc=49%, Loss=0.6972367167472839\n",
            "Iteration: 332400 \t Train: Acc=52%, Loss=0.6921032667160034 \t\t Validation: Acc=57%, Loss=0.6869168281555176\n",
            "Iteration: 332410 \t Train: Acc=53%, Loss=0.6880204677581787 \t\t Validation: Acc=55%, Loss=0.679246187210083\n",
            "Iteration: 332420 \t Train: Acc=49%, Loss=0.7008919715881348 \t\t Validation: Acc=55%, Loss=0.6858325004577637\n",
            "Iteration: 332430 \t Train: Acc=53%, Loss=0.68978351354599 \t\t Validation: Acc=51%, Loss=0.6880084872245789\n",
            "Iteration: 332440 \t Train: Acc=48%, Loss=0.6970980167388916 \t\t Validation: Acc=49%, Loss=0.6951026916503906\n",
            "Iteration: 332450 \t Train: Acc=50%, Loss=0.6919752359390259 \t\t Validation: Acc=46%, Loss=0.7006093859672546\n",
            "Iteration: 332460 \t Train: Acc=48%, Loss=0.6969024538993835 \t\t Validation: Acc=46%, Loss=0.6951379776000977\n",
            "Iteration: 332470 \t Train: Acc=47%, Loss=0.6913331747055054 \t\t Validation: Acc=50%, Loss=0.6895010471343994\n",
            "Iteration: 332480 \t Train: Acc=46%, Loss=0.6948504447937012 \t\t Validation: Acc=51%, Loss=0.6902520656585693\n",
            "Iteration: 332490 \t Train: Acc=51%, Loss=0.6911782622337341 \t\t Validation: Acc=52%, Loss=0.6910935640335083\n",
            "Iteration: 332500 \t Train: Acc=50%, Loss=0.6923157572746277 \t\t Validation: Acc=50%, Loss=0.6971353888511658\n",
            "Iteration: 332510 \t Train: Acc=48%, Loss=0.6952295899391174 \t\t Validation: Acc=48%, Loss=0.6962029933929443\n",
            "Iteration: 332520 \t Train: Acc=51%, Loss=0.693722128868103 \t\t Validation: Acc=49%, Loss=0.6935840845108032\n",
            "Iteration: 332530 \t Train: Acc=56%, Loss=0.6905335783958435 \t\t Validation: Acc=50%, Loss=0.6944127082824707\n",
            "Iteration: 332540 \t Train: Acc=55%, Loss=0.6856473088264465 \t\t Validation: Acc=51%, Loss=0.6907081604003906\n",
            "Iteration: 332550 \t Train: Acc=46%, Loss=0.6889598965644836 \t\t Validation: Acc=52%, Loss=0.6927403807640076\n",
            "Iteration: 332560 \t Train: Acc=59%, Loss=0.684221625328064 \t\t Validation: Acc=45%, Loss=0.7012495398521423\n",
            "Iteration: 332570 \t Train: Acc=51%, Loss=0.6894760131835938 \t\t Validation: Acc=49%, Loss=0.6964036226272583\n",
            "Iteration: 332580 \t Train: Acc=49%, Loss=0.6940891146659851 \t\t Validation: Acc=52%, Loss=0.6913682818412781\n",
            "Iteration: 332590 \t Train: Acc=47%, Loss=0.7011101841926575 \t\t Validation: Acc=51%, Loss=0.688937783241272\n",
            "Iteration: 332600 \t Train: Acc=55%, Loss=0.6859310865402222 \t\t Validation: Acc=53%, Loss=0.686404287815094\n",
            "Iteration: 332610 \t Train: Acc=48%, Loss=0.6883491277694702 \t\t Validation: Acc=50%, Loss=0.6928091049194336\n",
            "Iteration: 332620 \t Train: Acc=57%, Loss=0.6800983548164368 \t\t Validation: Acc=47%, Loss=0.6954094171524048\n",
            "Iteration: 332630 \t Train: Acc=54%, Loss=0.6912716627120972 \t\t Validation: Acc=55%, Loss=0.6858857870101929\n",
            "Iteration: 332640 \t Train: Acc=45%, Loss=0.699509859085083 \t\t Validation: Acc=53%, Loss=0.6879361271858215\n",
            "Iteration: 332650 \t Train: Acc=50%, Loss=0.6903879642486572 \t\t Validation: Acc=52%, Loss=0.6889071464538574\n",
            "Iteration: 332660 \t Train: Acc=53%, Loss=0.6840277910232544 \t\t Validation: Acc=53%, Loss=0.6821975708007812\n",
            "Iteration: 332670 \t Train: Acc=53%, Loss=0.690693736076355 \t\t Validation: Acc=55%, Loss=0.6867163181304932\n",
            "Iteration: 332680 \t Train: Acc=51%, Loss=0.6833029389381409 \t\t Validation: Acc=50%, Loss=0.6933770179748535\n",
            "Iteration: 332690 \t Train: Acc=53%, Loss=0.6885790228843689 \t\t Validation: Acc=57%, Loss=0.6827571392059326\n",
            "Iteration: 332700 \t Train: Acc=54%, Loss=0.6904599666595459 \t\t Validation: Acc=44%, Loss=0.700475811958313\n",
            "Iteration: 332710 \t Train: Acc=54%, Loss=0.6870380640029907 \t\t Validation: Acc=46%, Loss=0.6945143938064575\n",
            "Iteration: 332720 \t Train: Acc=50%, Loss=0.6967029571533203 \t\t Validation: Acc=52%, Loss=0.6863461136817932\n",
            "Iteration: 332730 \t Train: Acc=55%, Loss=0.690676748752594 \t\t Validation: Acc=46%, Loss=0.6980963349342346\n",
            "Iteration: 332740 \t Train: Acc=50%, Loss=0.6982429027557373 \t\t Validation: Acc=56%, Loss=0.6832367181777954\n",
            "Iteration: 332750 \t Train: Acc=49%, Loss=0.6903322339057922 \t\t Validation: Acc=55%, Loss=0.6860788464546204\n",
            "Iteration: 332760 \t Train: Acc=51%, Loss=0.6927427053451538 \t\t Validation: Acc=53%, Loss=0.6896693706512451\n",
            "Iteration: 332770 \t Train: Acc=57%, Loss=0.6865314245223999 \t\t Validation: Acc=49%, Loss=0.6953108310699463\n",
            "Iteration: 332780 \t Train: Acc=52%, Loss=0.6918314099311829 \t\t Validation: Acc=48%, Loss=0.6944371461868286\n",
            "Iteration: 332790 \t Train: Acc=49%, Loss=0.693503737449646 \t\t Validation: Acc=51%, Loss=0.6921452283859253\n",
            "Iteration: 332800 \t Train: Acc=47%, Loss=0.6951629519462585 \t\t Validation: Acc=53%, Loss=0.6900570392608643\n",
            "Iteration: 332810 \t Train: Acc=47%, Loss=0.6921777725219727 \t\t Validation: Acc=50%, Loss=0.6986564993858337\n",
            "Iteration: 332820 \t Train: Acc=48%, Loss=0.6916269063949585 \t\t Validation: Acc=48%, Loss=0.6983699798583984\n",
            "Iteration: 332830 \t Train: Acc=40%, Loss=0.6956226229667664 \t\t Validation: Acc=47%, Loss=0.6942453384399414\n",
            "Iteration: 332840 \t Train: Acc=43%, Loss=0.6949254870414734 \t\t Validation: Acc=48%, Loss=0.6999266147613525\n",
            "Iteration: 332850 \t Train: Acc=45%, Loss=0.6936052441596985 \t\t Validation: Acc=48%, Loss=0.6922495365142822\n",
            "Iteration: 332860 \t Train: Acc=53%, Loss=0.691848874092102 \t\t Validation: Acc=52%, Loss=0.6945639848709106\n",
            "Iteration: 332870 \t Train: Acc=48%, Loss=0.687039852142334 \t\t Validation: Acc=50%, Loss=0.6942099928855896\n",
            "Iteration: 332880 \t Train: Acc=43%, Loss=0.6986148953437805 \t\t Validation: Acc=56%, Loss=0.6871930956840515\n",
            "Iteration: 332890 \t Train: Acc=55%, Loss=0.6836668252944946 \t\t Validation: Acc=51%, Loss=0.6929342150688171\n",
            "Iteration: 332900 \t Train: Acc=45%, Loss=0.6915703415870667 \t\t Validation: Acc=52%, Loss=0.6966767907142639\n",
            "Iteration: 332910 \t Train: Acc=50%, Loss=0.6932803988456726 \t\t Validation: Acc=51%, Loss=0.7030413150787354\n",
            "Iteration: 332920 \t Train: Acc=54%, Loss=0.6902214884757996 \t\t Validation: Acc=49%, Loss=0.6955129504203796\n",
            "Iteration: 332930 \t Train: Acc=55%, Loss=0.6884446740150452 \t\t Validation: Acc=46%, Loss=0.7075592875480652\n",
            "Iteration: 332940 \t Train: Acc=50%, Loss=0.695808470249176 \t\t Validation: Acc=53%, Loss=0.6911131143569946\n",
            "Iteration: 332950 \t Train: Acc=45%, Loss=0.6959192156791687 \t\t Validation: Acc=52%, Loss=0.6939857602119446\n",
            "Iteration: 332960 \t Train: Acc=57%, Loss=0.6832173466682434 \t\t Validation: Acc=55%, Loss=0.687720775604248\n",
            "Iteration: 332970 \t Train: Acc=50%, Loss=0.6947610974311829 \t\t Validation: Acc=50%, Loss=0.6999788284301758\n",
            "Iteration: 332980 \t Train: Acc=50%, Loss=0.6932803392410278 \t\t Validation: Acc=53%, Loss=0.6805776357650757\n",
            "Iteration: 332990 \t Train: Acc=46%, Loss=0.6864023804664612 \t\t Validation: Acc=46%, Loss=0.7047915458679199\n",
            "Iteration: 333000 \t Train: Acc=50%, Loss=0.7005245089530945 \t\t Validation: Acc=51%, Loss=0.6975483894348145\n",
            "Iteration: 333010 \t Train: Acc=55%, Loss=0.691338300704956 \t\t Validation: Acc=53%, Loss=0.6894164085388184\n",
            "Iteration: 333020 \t Train: Acc=45%, Loss=0.6995816230773926 \t\t Validation: Acc=50%, Loss=0.6936753988265991\n",
            "Iteration: 333030 \t Train: Acc=53%, Loss=0.6951359510421753 \t\t Validation: Acc=48%, Loss=0.6948421597480774\n",
            "Iteration: 333040 \t Train: Acc=57%, Loss=0.6895186901092529 \t\t Validation: Acc=47%, Loss=0.7005530595779419\n",
            "Iteration: 333050 \t Train: Acc=51%, Loss=0.6856158375740051 \t\t Validation: Acc=49%, Loss=0.6967096924781799\n",
            "Iteration: 333060 \t Train: Acc=51%, Loss=0.6865027546882629 \t\t Validation: Acc=46%, Loss=0.6986688375473022\n",
            "Iteration: 333070 \t Train: Acc=49%, Loss=0.6858705282211304 \t\t Validation: Acc=50%, Loss=0.690555214881897\n",
            "Iteration: 333080 \t Train: Acc=50%, Loss=0.6966595649719238 \t\t Validation: Acc=51%, Loss=0.6914517879486084\n",
            "Iteration: 333090 \t Train: Acc=54%, Loss=0.6908355951309204 \t\t Validation: Acc=50%, Loss=0.6979780793190002\n",
            "Iteration: 333100 \t Train: Acc=47%, Loss=0.6909803152084351 \t\t Validation: Acc=50%, Loss=0.6947886347770691\n",
            "Iteration: 333110 \t Train: Acc=49%, Loss=0.7002385258674622 \t\t Validation: Acc=54%, Loss=0.6861045360565186\n",
            "Iteration: 333120 \t Train: Acc=49%, Loss=0.6890078783035278 \t\t Validation: Acc=48%, Loss=0.6966643929481506\n",
            "Iteration: 333130 \t Train: Acc=52%, Loss=0.6844350695610046 \t\t Validation: Acc=54%, Loss=0.6924274563789368\n",
            "Iteration: 333140 \t Train: Acc=45%, Loss=0.7049583792686462 \t\t Validation: Acc=51%, Loss=0.7068917751312256\n",
            "Iteration: 333150 \t Train: Acc=50%, Loss=0.6945707201957703 \t\t Validation: Acc=46%, Loss=0.6940801739692688\n",
            "Iteration: 333160 \t Train: Acc=51%, Loss=0.6927443146705627 \t\t Validation: Acc=53%, Loss=0.6973878145217896\n",
            "Iteration: 333170 \t Train: Acc=43%, Loss=0.6984120607376099 \t\t Validation: Acc=50%, Loss=0.6941330432891846\n",
            "Iteration: 333180 \t Train: Acc=53%, Loss=0.6912038922309875 \t\t Validation: Acc=46%, Loss=0.6978033781051636\n",
            "Iteration: 333190 \t Train: Acc=54%, Loss=0.6917338371276855 \t\t Validation: Acc=52%, Loss=0.6864953637123108\n",
            "Iteration: 333200 \t Train: Acc=50%, Loss=0.6981053948402405 \t\t Validation: Acc=53%, Loss=0.6909809112548828\n",
            "Iteration: 333210 \t Train: Acc=50%, Loss=0.6959012746810913 \t\t Validation: Acc=49%, Loss=0.6987137794494629\n",
            "Iteration: 333220 \t Train: Acc=48%, Loss=0.6934934854507446 \t\t Validation: Acc=48%, Loss=0.6942316293716431\n",
            "Iteration: 333230 \t Train: Acc=48%, Loss=0.6923449039459229 \t\t Validation: Acc=53%, Loss=0.6916327476501465\n",
            "Iteration: 333240 \t Train: Acc=57%, Loss=0.6890147924423218 \t\t Validation: Acc=52%, Loss=0.6876107454299927\n",
            "Iteration: 333250 \t Train: Acc=51%, Loss=0.6868594884872437 \t\t Validation: Acc=46%, Loss=0.7001189589500427\n",
            "Iteration: 333260 \t Train: Acc=52%, Loss=0.6916757822036743 \t\t Validation: Acc=46%, Loss=0.6964923143386841\n",
            "Iteration: 333270 \t Train: Acc=50%, Loss=0.6942417025566101 \t\t Validation: Acc=50%, Loss=0.689967930316925\n",
            "Iteration: 333280 \t Train: Acc=54%, Loss=0.6889669895172119 \t\t Validation: Acc=48%, Loss=0.698049783706665\n",
            "Iteration: 333290 \t Train: Acc=54%, Loss=0.6897005438804626 \t\t Validation: Acc=50%, Loss=0.6907864212989807\n",
            "Iteration: 333300 \t Train: Acc=50%, Loss=0.6928783059120178 \t\t Validation: Acc=53%, Loss=0.6937107443809509\n",
            "Iteration: 333310 \t Train: Acc=47%, Loss=0.6923149824142456 \t\t Validation: Acc=53%, Loss=0.6888384222984314\n",
            "Iteration: 333320 \t Train: Acc=51%, Loss=0.6920019388198853 \t\t Validation: Acc=53%, Loss=0.6918333172798157\n",
            "Iteration: 333330 \t Train: Acc=53%, Loss=0.6868959665298462 \t\t Validation: Acc=50%, Loss=0.6907974481582642\n",
            "Iteration: 333340 \t Train: Acc=50%, Loss=0.6909851431846619 \t\t Validation: Acc=51%, Loss=0.6896384954452515\n",
            "Iteration: 333350 \t Train: Acc=54%, Loss=0.6870555877685547 \t\t Validation: Acc=51%, Loss=0.6870659589767456\n",
            "Iteration: 333360 \t Train: Acc=57%, Loss=0.6856377720832825 \t\t Validation: Acc=52%, Loss=0.686128556728363\n",
            "Iteration: 333370 \t Train: Acc=53%, Loss=0.688993513584137 \t\t Validation: Acc=50%, Loss=0.6953126788139343\n",
            "Iteration: 333380 \t Train: Acc=50%, Loss=0.6902816891670227 \t\t Validation: Acc=52%, Loss=0.6984155774116516\n",
            "Iteration: 333390 \t Train: Acc=50%, Loss=0.6918758153915405 \t\t Validation: Acc=50%, Loss=0.6972783803939819\n",
            "Iteration: 333400 \t Train: Acc=54%, Loss=0.6878588795661926 \t\t Validation: Acc=44%, Loss=0.7021481990814209\n",
            "Iteration: 333410 \t Train: Acc=47%, Loss=0.6950857639312744 \t\t Validation: Acc=47%, Loss=0.6923453211784363\n",
            "Iteration: 333420 \t Train: Acc=50%, Loss=0.6920438408851624 \t\t Validation: Acc=47%, Loss=0.693039059638977\n",
            "Iteration: 333430 \t Train: Acc=53%, Loss=0.6902108788490295 \t\t Validation: Acc=57%, Loss=0.6860729455947876\n",
            "Iteration: 333440 \t Train: Acc=46%, Loss=0.695699155330658 \t\t Validation: Acc=49%, Loss=0.6950441598892212\n",
            "Iteration: 333450 \t Train: Acc=48%, Loss=0.6891927719116211 \t\t Validation: Acc=52%, Loss=0.6925836205482483\n",
            "Iteration: 333460 \t Train: Acc=49%, Loss=0.6904218792915344 \t\t Validation: Acc=49%, Loss=0.6975138783454895\n",
            "Iteration: 333470 \t Train: Acc=50%, Loss=0.691338300704956 \t\t Validation: Acc=44%, Loss=0.6983904838562012\n",
            "Iteration: 333480 \t Train: Acc=52%, Loss=0.6837135553359985 \t\t Validation: Acc=51%, Loss=0.6888531446456909\n",
            "Iteration: 333490 \t Train: Acc=52%, Loss=0.6936348080635071 \t\t Validation: Acc=52%, Loss=0.6971387267112732\n",
            "Iteration: 333500 \t Train: Acc=54%, Loss=0.6912485361099243 \t\t Validation: Acc=50%, Loss=0.6938173174858093\n",
            "Iteration: 333510 \t Train: Acc=53%, Loss=0.6885274648666382 \t\t Validation: Acc=56%, Loss=0.6841131448745728\n",
            "Iteration: 333520 \t Train: Acc=54%, Loss=0.6890419721603394 \t\t Validation: Acc=44%, Loss=0.696107029914856\n",
            "Iteration: 333530 \t Train: Acc=47%, Loss=0.6967642307281494 \t\t Validation: Acc=49%, Loss=0.6989094614982605\n",
            "Iteration: 333540 \t Train: Acc=54%, Loss=0.6898159384727478 \t\t Validation: Acc=53%, Loss=0.6922991275787354\n",
            "Iteration: 333550 \t Train: Acc=46%, Loss=0.6964839100837708 \t\t Validation: Acc=50%, Loss=0.7010718584060669\n",
            "Iteration: 333560 \t Train: Acc=51%, Loss=0.6857649683952332 \t\t Validation: Acc=52%, Loss=0.6902987957000732\n",
            "Iteration: 333570 \t Train: Acc=49%, Loss=0.6918910145759583 \t\t Validation: Acc=50%, Loss=0.6960481405258179\n",
            "Iteration: 333580 \t Train: Acc=50%, Loss=0.6922385692596436 \t\t Validation: Acc=49%, Loss=0.6964479088783264\n",
            "Iteration: 333590 \t Train: Acc=53%, Loss=0.6882935166358948 \t\t Validation: Acc=53%, Loss=0.6930338144302368\n",
            "Iteration: 333600 \t Train: Acc=48%, Loss=0.695334792137146 \t\t Validation: Acc=47%, Loss=0.697224497795105\n",
            "Iteration: 333610 \t Train: Acc=46%, Loss=0.6973565220832825 \t\t Validation: Acc=51%, Loss=0.6927869915962219\n",
            "Iteration: 333620 \t Train: Acc=53%, Loss=0.6887722611427307 \t\t Validation: Acc=48%, Loss=0.6977075338363647\n",
            "Iteration: 333630 \t Train: Acc=50%, Loss=0.693362832069397 \t\t Validation: Acc=51%, Loss=0.6893423199653625\n",
            "Iteration: 333640 \t Train: Acc=49%, Loss=0.6930271983146667 \t\t Validation: Acc=51%, Loss=0.6879007816314697\n",
            "Iteration: 333650 \t Train: Acc=51%, Loss=0.6927828192710876 \t\t Validation: Acc=49%, Loss=0.6941233277320862\n",
            "Iteration: 333660 \t Train: Acc=50%, Loss=0.6933917999267578 \t\t Validation: Acc=53%, Loss=0.6923655271530151\n",
            "Iteration: 333670 \t Train: Acc=53%, Loss=0.6890190243721008 \t\t Validation: Acc=50%, Loss=0.6930285692214966\n",
            "Iteration: 333680 \t Train: Acc=50%, Loss=0.6935874223709106 \t\t Validation: Acc=50%, Loss=0.6939105987548828\n",
            "Iteration: 333690 \t Train: Acc=47%, Loss=0.7014604210853577 \t\t Validation: Acc=47%, Loss=0.6937441825866699\n",
            "Iteration: 333700 \t Train: Acc=45%, Loss=0.6956283450126648 \t\t Validation: Acc=53%, Loss=0.6819764375686646\n",
            "Iteration: 333710 \t Train: Acc=50%, Loss=0.6942387223243713 \t\t Validation: Acc=54%, Loss=0.6837431788444519\n",
            "Iteration: 333720 \t Train: Acc=47%, Loss=0.6943208575248718 \t\t Validation: Acc=50%, Loss=0.6948933005332947\n",
            "Iteration: 333730 \t Train: Acc=52%, Loss=0.6971486806869507 \t\t Validation: Acc=52%, Loss=0.6927019953727722\n",
            "Iteration: 333740 \t Train: Acc=56%, Loss=0.6907891631126404 \t\t Validation: Acc=49%, Loss=0.6966469883918762\n",
            "Iteration: 333750 \t Train: Acc=46%, Loss=0.6971728205680847 \t\t Validation: Acc=50%, Loss=0.689479649066925\n",
            "Iteration: 333760 \t Train: Acc=42%, Loss=0.6977607011795044 \t\t Validation: Acc=52%, Loss=0.6926075220108032\n",
            "Iteration: 333770 \t Train: Acc=50%, Loss=0.6922785043716431 \t\t Validation: Acc=51%, Loss=0.6938698291778564\n",
            "Iteration: 333780 \t Train: Acc=46%, Loss=0.6975914239883423 \t\t Validation: Acc=53%, Loss=0.6939051151275635\n",
            "Iteration: 333790 \t Train: Acc=53%, Loss=0.6903330683708191 \t\t Validation: Acc=50%, Loss=0.6958359479904175\n",
            "Iteration: 333800 \t Train: Acc=51%, Loss=0.687406599521637 \t\t Validation: Acc=50%, Loss=0.6925561428070068\n",
            "Iteration: 333810 \t Train: Acc=45%, Loss=0.7034521698951721 \t\t Validation: Acc=54%, Loss=0.6877346634864807\n",
            "Iteration: 333820 \t Train: Acc=48%, Loss=0.694817304611206 \t\t Validation: Acc=56%, Loss=0.6912072896957397\n",
            "Iteration: 333830 \t Train: Acc=50%, Loss=0.6900510787963867 \t\t Validation: Acc=48%, Loss=0.6899926066398621\n",
            "Iteration: 333840 \t Train: Acc=54%, Loss=0.6903976798057556 \t\t Validation: Acc=51%, Loss=0.6862260699272156\n",
            "Iteration: 333850 \t Train: Acc=50%, Loss=0.6909242868423462 \t\t Validation: Acc=50%, Loss=0.6911246180534363\n",
            "Iteration: 333860 \t Train: Acc=50%, Loss=0.6895395517349243 \t\t Validation: Acc=50%, Loss=0.6897147297859192\n",
            "Iteration: 333870 \t Train: Acc=46%, Loss=0.6926628947257996 \t\t Validation: Acc=49%, Loss=0.6928331255912781\n",
            "Iteration: 333880 \t Train: Acc=50%, Loss=0.6899034976959229 \t\t Validation: Acc=46%, Loss=0.6964532136917114\n",
            "Iteration: 333890 \t Train: Acc=52%, Loss=0.6919693946838379 \t\t Validation: Acc=50%, Loss=0.6948332786560059\n",
            "Iteration: 333900 \t Train: Acc=56%, Loss=0.6843037009239197 \t\t Validation: Acc=50%, Loss=0.6895975470542908\n",
            "Iteration: 333910 \t Train: Acc=46%, Loss=0.6975430250167847 \t\t Validation: Acc=53%, Loss=0.6906163692474365\n",
            "Iteration: 333920 \t Train: Acc=49%, Loss=0.6958508491516113 \t\t Validation: Acc=45%, Loss=0.6987810730934143\n",
            "Iteration: 333930 \t Train: Acc=50%, Loss=0.6895168423652649 \t\t Validation: Acc=51%, Loss=0.6932240724563599\n",
            "Iteration: 333940 \t Train: Acc=46%, Loss=0.6983420252799988 \t\t Validation: Acc=48%, Loss=0.6939306259155273\n",
            "Iteration: 333950 \t Train: Acc=50%, Loss=0.6880228519439697 \t\t Validation: Acc=48%, Loss=0.6964248418807983\n",
            "Iteration: 333960 \t Train: Acc=55%, Loss=0.6888056397438049 \t\t Validation: Acc=49%, Loss=0.6974779367446899\n",
            "Iteration: 333970 \t Train: Acc=46%, Loss=0.6899182796478271 \t\t Validation: Acc=50%, Loss=0.6916940212249756\n",
            "Iteration: 333980 \t Train: Acc=53%, Loss=0.6894826889038086 \t\t Validation: Acc=49%, Loss=0.692168653011322\n",
            "Iteration: 333990 \t Train: Acc=53%, Loss=0.6898316144943237 \t\t Validation: Acc=52%, Loss=0.6889361143112183\n",
            "Iteration: 334000 \t Train: Acc=48%, Loss=0.6942743062973022 \t\t Validation: Acc=50%, Loss=0.6885161399841309\n",
            "Iteration: 334010 \t Train: Acc=50%, Loss=0.684830904006958 \t\t Validation: Acc=50%, Loss=0.6960718631744385\n",
            "Iteration: 334020 \t Train: Acc=49%, Loss=0.6984994411468506 \t\t Validation: Acc=53%, Loss=0.6910617351531982\n",
            "Iteration: 334030 \t Train: Acc=53%, Loss=0.6877891421318054 \t\t Validation: Acc=50%, Loss=0.6965762972831726\n",
            "Iteration: 334040 \t Train: Acc=47%, Loss=0.6920617818832397 \t\t Validation: Acc=50%, Loss=0.6932176947593689\n",
            "Iteration: 334050 \t Train: Acc=52%, Loss=0.6892722249031067 \t\t Validation: Acc=47%, Loss=0.6965593099594116\n",
            "Iteration: 334060 \t Train: Acc=50%, Loss=0.6861656904220581 \t\t Validation: Acc=50%, Loss=0.6927721500396729\n",
            "Iteration: 334070 \t Train: Acc=55%, Loss=0.6880255341529846 \t\t Validation: Acc=50%, Loss=0.6953228116035461\n",
            "Iteration: 334080 \t Train: Acc=52%, Loss=0.692231297492981 \t\t Validation: Acc=51%, Loss=0.6896939873695374\n",
            "Iteration: 334090 \t Train: Acc=50%, Loss=0.6919045448303223 \t\t Validation: Acc=50%, Loss=0.6916830539703369\n",
            "Iteration: 334100 \t Train: Acc=53%, Loss=0.6868770122528076 \t\t Validation: Acc=53%, Loss=0.6890409588813782\n",
            "Iteration: 334110 \t Train: Acc=46%, Loss=0.6890339851379395 \t\t Validation: Acc=50%, Loss=0.6913197636604309\n",
            "Iteration: 334120 \t Train: Acc=48%, Loss=0.6869443655014038 \t\t Validation: Acc=53%, Loss=0.6893189549446106\n",
            "Iteration: 334130 \t Train: Acc=48%, Loss=0.6921271681785583 \t\t Validation: Acc=50%, Loss=0.6915390491485596\n",
            "Iteration: 334140 \t Train: Acc=50%, Loss=0.6888214349746704 \t\t Validation: Acc=47%, Loss=0.696861743927002\n",
            "Iteration: 334150 \t Train: Acc=51%, Loss=0.6821774840354919 \t\t Validation: Acc=51%, Loss=0.690492570400238\n",
            "Iteration: 334160 \t Train: Acc=53%, Loss=0.6834251880645752 \t\t Validation: Acc=48%, Loss=0.6931476593017578\n",
            "Iteration: 334170 \t Train: Acc=50%, Loss=0.6946420669555664 \t\t Validation: Acc=50%, Loss=0.6892872452735901\n",
            "Iteration: 334180 \t Train: Acc=51%, Loss=0.6905524730682373 \t\t Validation: Acc=49%, Loss=0.6972588300704956\n",
            "Iteration: 334190 \t Train: Acc=48%, Loss=0.6889501810073853 \t\t Validation: Acc=49%, Loss=0.6913007497787476\n",
            "Iteration: 334200 \t Train: Acc=51%, Loss=0.6881504058837891 \t\t Validation: Acc=49%, Loss=0.6909385323524475\n",
            "Iteration: 334210 \t Train: Acc=51%, Loss=0.6934123039245605 \t\t Validation: Acc=52%, Loss=0.6949422359466553\n",
            "Iteration: 334220 \t Train: Acc=53%, Loss=0.6943517923355103 \t\t Validation: Acc=52%, Loss=0.6974513530731201\n",
            "Iteration: 334230 \t Train: Acc=42%, Loss=0.6970555186271667 \t\t Validation: Acc=46%, Loss=0.6928243041038513\n",
            "Iteration: 334240 \t Train: Acc=52%, Loss=0.6914410591125488 \t\t Validation: Acc=52%, Loss=0.6898057460784912\n",
            "Iteration: 334250 \t Train: Acc=50%, Loss=0.6959310173988342 \t\t Validation: Acc=53%, Loss=0.6922799348831177\n",
            "Iteration: 334260 \t Train: Acc=55%, Loss=0.6916950345039368 \t\t Validation: Acc=54%, Loss=0.6919525861740112\n",
            "Iteration: 334270 \t Train: Acc=46%, Loss=0.6951522827148438 \t\t Validation: Acc=49%, Loss=0.6908246874809265\n",
            "Iteration: 334280 \t Train: Acc=54%, Loss=0.6887708902359009 \t\t Validation: Acc=50%, Loss=0.6948010325431824\n",
            "Iteration: 334290 \t Train: Acc=46%, Loss=0.6943336725234985 \t\t Validation: Acc=55%, Loss=0.6907029151916504\n",
            "Iteration: 334300 \t Train: Acc=54%, Loss=0.6915764808654785 \t\t Validation: Acc=51%, Loss=0.687340259552002\n",
            "Iteration: 334310 \t Train: Acc=50%, Loss=0.6945591568946838 \t\t Validation: Acc=51%, Loss=0.6900291442871094\n",
            "Iteration: 334320 \t Train: Acc=48%, Loss=0.6949246525764465 \t\t Validation: Acc=51%, Loss=0.6890852451324463\n",
            "Iteration: 334330 \t Train: Acc=54%, Loss=0.6895885467529297 \t\t Validation: Acc=50%, Loss=0.6897609233856201\n",
            "Iteration: 334340 \t Train: Acc=49%, Loss=0.6920573711395264 \t\t Validation: Acc=47%, Loss=0.6963079571723938\n",
            "Iteration: 334350 \t Train: Acc=50%, Loss=0.6918041110038757 \t\t Validation: Acc=49%, Loss=0.6952628493309021\n",
            "Iteration: 334360 \t Train: Acc=48%, Loss=0.6913660168647766 \t\t Validation: Acc=49%, Loss=0.693208634853363\n",
            "Iteration: 334370 \t Train: Acc=52%, Loss=0.6951054334640503 \t\t Validation: Acc=52%, Loss=0.6971195936203003\n",
            "Iteration: 334380 \t Train: Acc=50%, Loss=0.6916178464889526 \t\t Validation: Acc=50%, Loss=0.6906113028526306\n",
            "Iteration: 334390 \t Train: Acc=50%, Loss=0.690154492855072 \t\t Validation: Acc=47%, Loss=0.6914223432540894\n",
            "Iteration: 334400 \t Train: Acc=60%, Loss=0.6833716034889221 \t\t Validation: Acc=52%, Loss=0.6848349571228027\n",
            "Iteration: 334410 \t Train: Acc=52%, Loss=0.6898547410964966 \t\t Validation: Acc=53%, Loss=0.6901611089706421\n",
            "Iteration: 334420 \t Train: Acc=56%, Loss=0.6868470907211304 \t\t Validation: Acc=53%, Loss=0.6932209134101868\n",
            "Iteration: 334430 \t Train: Acc=54%, Loss=0.6912171244621277 \t\t Validation: Acc=52%, Loss=0.6898285150527954\n",
            "Iteration: 334440 \t Train: Acc=50%, Loss=0.6967899203300476 \t\t Validation: Acc=52%, Loss=0.6866310834884644\n",
            "Iteration: 334450 \t Train: Acc=47%, Loss=0.6989589929580688 \t\t Validation: Acc=49%, Loss=0.6946589350700378\n",
            "Iteration: 334460 \t Train: Acc=48%, Loss=0.6870295405387878 \t\t Validation: Acc=50%, Loss=0.6905989646911621\n",
            "Iteration: 334470 \t Train: Acc=52%, Loss=0.6885794997215271 \t\t Validation: Acc=53%, Loss=0.6849691271781921\n",
            "Iteration: 334480 \t Train: Acc=48%, Loss=0.6936135292053223 \t\t Validation: Acc=48%, Loss=0.6957392692565918\n",
            "Iteration: 334490 \t Train: Acc=52%, Loss=0.6901028752326965 \t\t Validation: Acc=50%, Loss=0.6880266070365906\n",
            "Iteration: 334500 \t Train: Acc=53%, Loss=0.6998546719551086 \t\t Validation: Acc=52%, Loss=0.6913439035415649\n",
            "Iteration: 334510 \t Train: Acc=48%, Loss=0.6904518604278564 \t\t Validation: Acc=50%, Loss=0.6890615224838257\n",
            "Iteration: 334520 \t Train: Acc=53%, Loss=0.692605197429657 \t\t Validation: Acc=52%, Loss=0.6990113258361816\n",
            "Iteration: 334530 \t Train: Acc=50%, Loss=0.6946728825569153 \t\t Validation: Acc=51%, Loss=0.6884341239929199\n",
            "Iteration: 334540 \t Train: Acc=51%, Loss=0.6943994164466858 \t\t Validation: Acc=48%, Loss=0.695363461971283\n",
            "Iteration: 334550 \t Train: Acc=46%, Loss=0.6995092034339905 \t\t Validation: Acc=52%, Loss=0.6846110820770264\n",
            "Iteration: 334560 \t Train: Acc=49%, Loss=0.6927085518836975 \t\t Validation: Acc=47%, Loss=0.6926958560943604\n",
            "Iteration: 334570 \t Train: Acc=46%, Loss=0.6930301189422607 \t\t Validation: Acc=49%, Loss=0.6959989666938782\n",
            "Iteration: 334580 \t Train: Acc=53%, Loss=0.6885004639625549 \t\t Validation: Acc=53%, Loss=0.6887413859367371\n",
            "Iteration: 334590 \t Train: Acc=50%, Loss=0.6894420385360718 \t\t Validation: Acc=46%, Loss=0.695048451423645\n",
            "Iteration: 334600 \t Train: Acc=50%, Loss=0.6899509429931641 \t\t Validation: Acc=46%, Loss=0.6959419846534729\n",
            "Iteration: 334610 \t Train: Acc=46%, Loss=0.6923123002052307 \t\t Validation: Acc=53%, Loss=0.6965944766998291\n",
            "Iteration: 334620 \t Train: Acc=46%, Loss=0.6876199841499329 \t\t Validation: Acc=47%, Loss=0.6987606883049011\n",
            "Iteration: 334630 \t Train: Acc=46%, Loss=0.6873228549957275 \t\t Validation: Acc=50%, Loss=0.6918642520904541\n",
            "Iteration: 334640 \t Train: Acc=50%, Loss=0.6900111436843872 \t\t Validation: Acc=51%, Loss=0.6936889886856079\n",
            "Iteration: 334650 \t Train: Acc=51%, Loss=0.692997395992279 \t\t Validation: Acc=47%, Loss=0.6967970728874207\n",
            "Iteration: 334660 \t Train: Acc=55%, Loss=0.6846342086791992 \t\t Validation: Acc=48%, Loss=0.6935217976570129\n",
            "Iteration: 334670 \t Train: Acc=50%, Loss=0.6963433623313904 \t\t Validation: Acc=51%, Loss=0.6952464580535889\n",
            "Iteration: 334680 \t Train: Acc=53%, Loss=0.6886218786239624 \t\t Validation: Acc=49%, Loss=0.6998911499977112\n",
            "Iteration: 334690 \t Train: Acc=51%, Loss=0.6909513473510742 \t\t Validation: Acc=50%, Loss=0.6887571811676025\n",
            "Iteration: 334700 \t Train: Acc=47%, Loss=0.6925931572914124 \t\t Validation: Acc=50%, Loss=0.6877373456954956\n",
            "Iteration: 334710 \t Train: Acc=51%, Loss=0.691129207611084 \t\t Validation: Acc=53%, Loss=0.685523509979248\n",
            "Iteration: 334720 \t Train: Acc=52%, Loss=0.699657678604126 \t\t Validation: Acc=47%, Loss=0.6944918632507324\n",
            "Iteration: 334730 \t Train: Acc=53%, Loss=0.6928130984306335 \t\t Validation: Acc=50%, Loss=0.6950740814208984\n",
            "Iteration: 334740 \t Train: Acc=53%, Loss=0.6838676929473877 \t\t Validation: Acc=49%, Loss=0.6904585957527161\n",
            "Iteration: 334750 \t Train: Acc=50%, Loss=0.6936697959899902 \t\t Validation: Acc=50%, Loss=0.6896989345550537\n",
            "Iteration: 334760 \t Train: Acc=50%, Loss=0.6919639110565186 \t\t Validation: Acc=56%, Loss=0.6830581426620483\n",
            "Iteration: 334770 \t Train: Acc=51%, Loss=0.6929138898849487 \t\t Validation: Acc=50%, Loss=0.6956533789634705\n",
            "Iteration: 334780 \t Train: Acc=51%, Loss=0.6932377815246582 \t\t Validation: Acc=50%, Loss=0.6912677884101868\n",
            "Iteration: 334790 \t Train: Acc=41%, Loss=0.6895872950553894 \t\t Validation: Acc=53%, Loss=0.6860182881355286\n",
            "Iteration: 334800 \t Train: Acc=45%, Loss=0.6941409111022949 \t\t Validation: Acc=53%, Loss=0.6835917830467224\n",
            "Iteration: 334810 \t Train: Acc=46%, Loss=0.6926096081733704 \t\t Validation: Acc=49%, Loss=0.6953085660934448\n",
            "Iteration: 334820 \t Train: Acc=59%, Loss=0.682482123374939 \t\t Validation: Acc=50%, Loss=0.6922340393066406\n",
            "Iteration: 334830 \t Train: Acc=52%, Loss=0.6876301169395447 \t\t Validation: Acc=48%, Loss=0.6943965554237366\n",
            "Iteration: 334840 \t Train: Acc=52%, Loss=0.6890634298324585 \t\t Validation: Acc=49%, Loss=0.696692943572998\n",
            "Iteration: 334850 \t Train: Acc=49%, Loss=0.6904001235961914 \t\t Validation: Acc=50%, Loss=0.699184775352478\n",
            "Iteration: 334860 \t Train: Acc=46%, Loss=0.6914743185043335 \t\t Validation: Acc=51%, Loss=0.6873652338981628\n",
            "Iteration: 334870 \t Train: Acc=48%, Loss=0.6917409896850586 \t\t Validation: Acc=51%, Loss=0.6867785453796387\n",
            "Iteration: 334880 \t Train: Acc=57%, Loss=0.6923600435256958 \t\t Validation: Acc=47%, Loss=0.6975551843643188\n",
            "Iteration: 334890 \t Train: Acc=52%, Loss=0.6955833435058594 \t\t Validation: Acc=51%, Loss=0.6937609314918518\n",
            "Iteration: 334900 \t Train: Acc=53%, Loss=0.6888436675071716 \t\t Validation: Acc=48%, Loss=0.6960600018501282\n",
            "Iteration: 334910 \t Train: Acc=49%, Loss=0.6965548992156982 \t\t Validation: Acc=50%, Loss=0.6928790211677551\n",
            "Iteration: 334920 \t Train: Acc=46%, Loss=0.6949589252471924 \t\t Validation: Acc=53%, Loss=0.6888286471366882\n",
            "Iteration: 334930 \t Train: Acc=46%, Loss=0.6979036927223206 \t\t Validation: Acc=49%, Loss=0.6917452216148376\n",
            "Iteration: 334940 \t Train: Acc=50%, Loss=0.6885530352592468 \t\t Validation: Acc=53%, Loss=0.6883322596549988\n",
            "Iteration: 334950 \t Train: Acc=52%, Loss=0.6898457407951355 \t\t Validation: Acc=52%, Loss=0.6910550594329834\n",
            "Iteration: 334960 \t Train: Acc=50%, Loss=0.6919934153556824 \t\t Validation: Acc=50%, Loss=0.6930978298187256\n",
            "Iteration: 334970 \t Train: Acc=47%, Loss=0.6929934024810791 \t\t Validation: Acc=51%, Loss=0.6902538537979126\n",
            "Iteration: 334980 \t Train: Acc=53%, Loss=0.6907401084899902 \t\t Validation: Acc=49%, Loss=0.6968719959259033\n",
            "Iteration: 334990 \t Train: Acc=57%, Loss=0.690680205821991 \t\t Validation: Acc=51%, Loss=0.6917254328727722\n",
            "Iteration: 335000 \t Train: Acc=49%, Loss=0.6973395347595215 \t\t Validation: Acc=57%, Loss=0.6873714327812195\n",
            "Iteration: 335010 \t Train: Acc=51%, Loss=0.6915093660354614 \t\t Validation: Acc=46%, Loss=0.6961421966552734\n",
            "Iteration: 335020 \t Train: Acc=52%, Loss=0.691065788269043 \t\t Validation: Acc=49%, Loss=0.6937733888626099\n",
            "Iteration: 335030 \t Train: Acc=48%, Loss=0.6928712725639343 \t\t Validation: Acc=47%, Loss=0.6982158422470093\n",
            "Iteration: 335040 \t Train: Acc=59%, Loss=0.6873043775558472 \t\t Validation: Acc=49%, Loss=0.6936140060424805\n",
            "Iteration: 335050 \t Train: Acc=48%, Loss=0.6920391321182251 \t\t Validation: Acc=50%, Loss=0.6950713396072388\n",
            "Iteration: 335060 \t Train: Acc=51%, Loss=0.6853212714195251 \t\t Validation: Acc=50%, Loss=0.6920397281646729\n",
            "Iteration: 335070 \t Train: Acc=52%, Loss=0.6968498229980469 \t\t Validation: Acc=51%, Loss=0.6915064454078674\n",
            "Iteration: 335080 \t Train: Acc=53%, Loss=0.6891186237335205 \t\t Validation: Acc=50%, Loss=0.692441463470459\n",
            "Iteration: 335090 \t Train: Acc=50%, Loss=0.6861961483955383 \t\t Validation: Acc=51%, Loss=0.6916401386260986\n",
            "Iteration: 335100 \t Train: Acc=48%, Loss=0.6896367073059082 \t\t Validation: Acc=48%, Loss=0.6936013698577881\n",
            "Iteration: 335110 \t Train: Acc=50%, Loss=0.6921077370643616 \t\t Validation: Acc=52%, Loss=0.691458523273468\n",
            "Iteration: 335120 \t Train: Acc=44%, Loss=0.6910548210144043 \t\t Validation: Acc=53%, Loss=0.692181408405304\n",
            "Iteration: 335130 \t Train: Acc=50%, Loss=0.689591109752655 \t\t Validation: Acc=50%, Loss=0.6942115426063538\n",
            "Iteration: 335140 \t Train: Acc=51%, Loss=0.6871302723884583 \t\t Validation: Acc=50%, Loss=0.6933411955833435\n",
            "Iteration: 335150 \t Train: Acc=52%, Loss=0.6914354562759399 \t\t Validation: Acc=50%, Loss=0.6921409964561462\n",
            "Iteration: 335160 \t Train: Acc=54%, Loss=0.6894012093544006 \t\t Validation: Acc=51%, Loss=0.6919616460800171\n",
            "Iteration: 335170 \t Train: Acc=50%, Loss=0.6966477036476135 \t\t Validation: Acc=50%, Loss=0.6997775435447693\n",
            "Iteration: 335180 \t Train: Acc=51%, Loss=0.6853592991828918 \t\t Validation: Acc=56%, Loss=0.6869274973869324\n",
            "Iteration: 335190 \t Train: Acc=52%, Loss=0.6967772841453552 \t\t Validation: Acc=46%, Loss=0.6954596638679504\n",
            "Iteration: 335200 \t Train: Acc=55%, Loss=0.6889504194259644 \t\t Validation: Acc=53%, Loss=0.6891706585884094\n",
            "Iteration: 335210 \t Train: Acc=56%, Loss=0.6874637007713318 \t\t Validation: Acc=47%, Loss=0.6967527270317078\n",
            "Iteration: 335220 \t Train: Acc=53%, Loss=0.7010815739631653 \t\t Validation: Acc=50%, Loss=0.692143440246582\n",
            "Iteration: 335230 \t Train: Acc=52%, Loss=0.693483829498291 \t\t Validation: Acc=50%, Loss=0.6914433836936951\n",
            "Iteration: 335240 \t Train: Acc=51%, Loss=0.6878242492675781 \t\t Validation: Acc=51%, Loss=0.6895274519920349\n",
            "Iteration: 335250 \t Train: Acc=53%, Loss=0.686202883720398 \t\t Validation: Acc=46%, Loss=0.6968706846237183\n",
            "Iteration: 335260 \t Train: Acc=47%, Loss=0.6932697296142578 \t\t Validation: Acc=54%, Loss=0.6881570816040039\n",
            "Iteration: 335270 \t Train: Acc=48%, Loss=0.6908543109893799 \t\t Validation: Acc=49%, Loss=0.6933407783508301\n",
            "Iteration: 335280 \t Train: Acc=52%, Loss=0.688959002494812 \t\t Validation: Acc=48%, Loss=0.6957677602767944\n",
            "Iteration: 335290 \t Train: Acc=53%, Loss=0.6836884021759033 \t\t Validation: Acc=55%, Loss=0.6856085658073425\n",
            "Iteration: 335300 \t Train: Acc=56%, Loss=0.6826173663139343 \t\t Validation: Acc=48%, Loss=0.6937928199768066\n",
            "Iteration: 335310 \t Train: Acc=53%, Loss=0.6872411370277405 \t\t Validation: Acc=51%, Loss=0.6888635158538818\n",
            "Iteration: 335320 \t Train: Acc=51%, Loss=0.690974235534668 \t\t Validation: Acc=50%, Loss=0.7001328468322754\n",
            "Iteration: 335330 \t Train: Acc=51%, Loss=0.6896712779998779 \t\t Validation: Acc=48%, Loss=0.700112521648407\n",
            "Iteration: 335340 \t Train: Acc=53%, Loss=0.6864901185035706 \t\t Validation: Acc=48%, Loss=0.6954088807106018\n",
            "Iteration: 335350 \t Train: Acc=51%, Loss=0.6851509213447571 \t\t Validation: Acc=45%, Loss=0.7000432014465332\n",
            "Iteration: 335360 \t Train: Acc=53%, Loss=0.6939229965209961 \t\t Validation: Acc=53%, Loss=0.6902976036071777\n",
            "It's been too long since we last saved the model. Saving...\n",
            "Iteration: 335370 \t Train: Acc=53%, Loss=0.690738320350647 \t\t Validation: Acc=45%, Loss=0.6993293762207031\n",
            "Iteration: 335380 \t Train: Acc=49%, Loss=0.6904604434967041 \t\t Validation: Acc=54%, Loss=0.6891419887542725\n",
            "Iteration: 335390 \t Train: Acc=54%, Loss=0.6873443126678467 \t\t Validation: Acc=44%, Loss=0.7002487182617188\n",
            "Iteration: 335400 \t Train: Acc=56%, Loss=0.6890427470207214 \t\t Validation: Acc=47%, Loss=0.6951587200164795\n",
            "Iteration: 335410 \t Train: Acc=58%, Loss=0.6859666109085083 \t\t Validation: Acc=52%, Loss=0.6911048293113708\n",
            "Iteration: 335420 \t Train: Acc=53%, Loss=0.6916135549545288 \t\t Validation: Acc=52%, Loss=0.6884661912918091\n",
            "Iteration: 335430 \t Train: Acc=50%, Loss=0.700147807598114 \t\t Validation: Acc=48%, Loss=0.6925292611122131\n",
            "Iteration: 335440 \t Train: Acc=53%, Loss=0.6858280897140503 \t\t Validation: Acc=50%, Loss=0.6919971704483032\n",
            "Iteration: 335450 \t Train: Acc=49%, Loss=0.6931243538856506 \t\t Validation: Acc=57%, Loss=0.6863256096839905\n",
            "Iteration: 335460 \t Train: Acc=55%, Loss=0.6861498355865479 \t\t Validation: Acc=48%, Loss=0.6966440081596375\n",
            "Iteration: 335470 \t Train: Acc=51%, Loss=0.6917310953140259 \t\t Validation: Acc=54%, Loss=0.689948320388794\n",
            "Iteration: 335480 \t Train: Acc=44%, Loss=0.7017642259597778 \t\t Validation: Acc=48%, Loss=0.6977055668830872\n",
            "Iteration: 335490 \t Train: Acc=48%, Loss=0.6976161599159241 \t\t Validation: Acc=48%, Loss=0.6972509026527405\n",
            "Iteration: 335500 \t Train: Acc=58%, Loss=0.6865723729133606 \t\t Validation: Acc=54%, Loss=0.6912904381752014\n",
            "Iteration: 335510 \t Train: Acc=49%, Loss=0.7007387280464172 \t\t Validation: Acc=48%, Loss=0.6886844635009766\n",
            "Iteration: 335520 \t Train: Acc=53%, Loss=0.6930306553840637 \t\t Validation: Acc=55%, Loss=0.6909076571464539\n",
            "Iteration: 335530 \t Train: Acc=57%, Loss=0.688628613948822 \t\t Validation: Acc=50%, Loss=0.6895103454589844\n",
            "Iteration: 335540 \t Train: Acc=51%, Loss=0.6907827258110046 \t\t Validation: Acc=53%, Loss=0.6896865367889404\n",
            "Iteration: 335550 \t Train: Acc=46%, Loss=0.6922422647476196 \t\t Validation: Acc=48%, Loss=0.6946146488189697\n",
            "Iteration: 335560 \t Train: Acc=51%, Loss=0.6912028789520264 \t\t Validation: Acc=49%, Loss=0.6967051029205322\n",
            "Iteration: 335570 \t Train: Acc=42%, Loss=0.7000091075897217 \t\t Validation: Acc=49%, Loss=0.6917446255683899\n",
            "Iteration: 335580 \t Train: Acc=55%, Loss=0.6898248791694641 \t\t Validation: Acc=53%, Loss=0.6900568008422852\n",
            "Iteration: 335590 \t Train: Acc=44%, Loss=0.6897324323654175 \t\t Validation: Acc=50%, Loss=0.6954748034477234\n",
            "Iteration: 335600 \t Train: Acc=52%, Loss=0.6916995644569397 \t\t Validation: Acc=48%, Loss=0.6968507766723633\n",
            "Iteration: 335610 \t Train: Acc=51%, Loss=0.6928287744522095 \t\t Validation: Acc=50%, Loss=0.6912745237350464\n",
            "Iteration: 335620 \t Train: Acc=56%, Loss=0.6928342580795288 \t\t Validation: Acc=53%, Loss=0.6911943554878235\n",
            "Iteration: 335630 \t Train: Acc=56%, Loss=0.6904212236404419 \t\t Validation: Acc=48%, Loss=0.6947929263114929\n",
            "Iteration: 335640 \t Train: Acc=48%, Loss=0.700594425201416 \t\t Validation: Acc=50%, Loss=0.6901063919067383\n",
            "Iteration: 335650 \t Train: Acc=55%, Loss=0.6848608255386353 \t\t Validation: Acc=52%, Loss=0.6909568905830383\n",
            "Iteration: 335660 \t Train: Acc=50%, Loss=0.6935470104217529 \t\t Validation: Acc=51%, Loss=0.6923090219497681\n",
            "Iteration: 335670 \t Train: Acc=51%, Loss=0.6940379738807678 \t\t Validation: Acc=50%, Loss=0.6926881074905396\n",
            "Iteration: 335680 \t Train: Acc=48%, Loss=0.6960737109184265 \t\t Validation: Acc=46%, Loss=0.6989265084266663\n",
            "Iteration: 335690 \t Train: Acc=50%, Loss=0.6895692348480225 \t\t Validation: Acc=53%, Loss=0.6920009255409241\n",
            "Iteration: 335700 \t Train: Acc=46%, Loss=0.6890814304351807 \t\t Validation: Acc=50%, Loss=0.6899136900901794\n",
            "Iteration: 335710 \t Train: Acc=49%, Loss=0.7047473788261414 \t\t Validation: Acc=53%, Loss=0.6899136304855347\n",
            "Iteration: 335720 \t Train: Acc=53%, Loss=0.6873059272766113 \t\t Validation: Acc=51%, Loss=0.6931089162826538\n",
            "Iteration: 335730 \t Train: Acc=50%, Loss=0.6885033845901489 \t\t Validation: Acc=50%, Loss=0.6914209127426147\n",
            "Iteration: 335740 \t Train: Acc=46%, Loss=0.690832793712616 \t\t Validation: Acc=47%, Loss=0.6951856017112732\n",
            "Iteration: 335750 \t Train: Acc=42%, Loss=0.7078244090080261 \t\t Validation: Acc=55%, Loss=0.6826280355453491\n",
            "Iteration: 335760 \t Train: Acc=50%, Loss=0.6928584575653076 \t\t Validation: Acc=46%, Loss=0.6973766088485718\n",
            "Iteration: 335770 \t Train: Acc=47%, Loss=0.6884441375732422 \t\t Validation: Acc=44%, Loss=0.6959275007247925\n",
            "Iteration: 335780 \t Train: Acc=46%, Loss=0.6955880522727966 \t\t Validation: Acc=54%, Loss=0.6857532262802124\n",
            "Iteration: 335790 \t Train: Acc=46%, Loss=0.7022417783737183 \t\t Validation: Acc=52%, Loss=0.6912323832511902\n",
            "Iteration: 335800 \t Train: Acc=53%, Loss=0.6857165098190308 \t\t Validation: Acc=51%, Loss=0.6911389231681824\n",
            "Iteration: 335810 \t Train: Acc=50%, Loss=0.6905746459960938 \t\t Validation: Acc=47%, Loss=0.697380781173706\n",
            "Iteration: 335820 \t Train: Acc=52%, Loss=0.6893720626831055 \t\t Validation: Acc=55%, Loss=0.6906526684761047\n",
            "Iteration: 335830 \t Train: Acc=52%, Loss=0.6924228072166443 \t\t Validation: Acc=53%, Loss=0.6913238167762756\n",
            "Iteration: 335840 \t Train: Acc=48%, Loss=0.6875418424606323 \t\t Validation: Acc=53%, Loss=0.6922803521156311\n",
            "Iteration: 335850 \t Train: Acc=53%, Loss=0.6905983090400696 \t\t Validation: Acc=50%, Loss=0.6942685842514038\n",
            "Iteration: 335860 \t Train: Acc=51%, Loss=0.690265953540802 \t\t Validation: Acc=44%, Loss=0.6937756538391113\n",
            "Iteration: 335870 \t Train: Acc=46%, Loss=0.6895390748977661 \t\t Validation: Acc=48%, Loss=0.6932260990142822\n",
            "Iteration: 335880 \t Train: Acc=52%, Loss=0.6919806003570557 \t\t Validation: Acc=48%, Loss=0.6966145038604736\n",
            "Iteration: 335890 \t Train: Acc=50%, Loss=0.693344235420227 \t\t Validation: Acc=47%, Loss=0.6964974403381348\n",
            "Iteration: 335900 \t Train: Acc=48%, Loss=0.6944764256477356 \t\t Validation: Acc=49%, Loss=0.6945794820785522\n",
            "Iteration: 335910 \t Train: Acc=52%, Loss=0.6943662166595459 \t\t Validation: Acc=52%, Loss=0.692341685295105\n",
            "Iteration: 335920 \t Train: Acc=56%, Loss=0.6915706992149353 \t\t Validation: Acc=55%, Loss=0.6886974573135376\n",
            "Iteration: 335930 \t Train: Acc=51%, Loss=0.6928613185882568 \t\t Validation: Acc=48%, Loss=0.6936080455780029\n",
            "Iteration: 335940 \t Train: Acc=54%, Loss=0.6890386343002319 \t\t Validation: Acc=58%, Loss=0.6837877035140991\n",
            "Iteration: 335950 \t Train: Acc=53%, Loss=0.6880742907524109 \t\t Validation: Acc=51%, Loss=0.6921592354774475\n",
            "Iteration: 335960 \t Train: Acc=52%, Loss=0.6890292763710022 \t\t Validation: Acc=50%, Loss=0.6954036355018616\n",
            "Iteration: 335970 \t Train: Acc=46%, Loss=0.6963648796081543 \t\t Validation: Acc=50%, Loss=0.6913196444511414\n",
            "Iteration: 335980 \t Train: Acc=58%, Loss=0.6794476509094238 \t\t Validation: Acc=50%, Loss=0.6915086507797241\n",
            "Iteration: 335990 \t Train: Acc=49%, Loss=0.6932698488235474 \t\t Validation: Acc=52%, Loss=0.6917538642883301\n",
            "Iteration: 336000 \t Train: Acc=50%, Loss=0.6912670135498047 \t\t Validation: Acc=52%, Loss=0.6893765926361084\n",
            "Iteration: 336010 \t Train: Acc=51%, Loss=0.6897222995758057 \t\t Validation: Acc=53%, Loss=0.6911172270774841\n",
            "Iteration: 336020 \t Train: Acc=48%, Loss=0.6968212127685547 \t\t Validation: Acc=53%, Loss=0.6876968741416931\n",
            "Iteration: 336030 \t Train: Acc=48%, Loss=0.7005173563957214 \t\t Validation: Acc=51%, Loss=0.6928856372833252\n",
            "Iteration: 336040 \t Train: Acc=49%, Loss=0.6920706629753113 \t\t Validation: Acc=52%, Loss=0.6899768114089966\n",
            "Iteration: 336050 \t Train: Acc=48%, Loss=0.6915909051895142 \t\t Validation: Acc=50%, Loss=0.6919621825218201\n",
            "Iteration: 336060 \t Train: Acc=49%, Loss=0.6963084936141968 \t\t Validation: Acc=48%, Loss=0.6947250962257385\n",
            "Iteration: 336070 \t Train: Acc=52%, Loss=0.6928362250328064 \t\t Validation: Acc=48%, Loss=0.6900461912155151\n",
            "Iteration: 336080 \t Train: Acc=48%, Loss=0.6915649771690369 \t\t Validation: Acc=49%, Loss=0.6934006810188293\n",
            "Iteration: 336090 \t Train: Acc=52%, Loss=0.6910997033119202 \t\t Validation: Acc=57%, Loss=0.6891750693321228\n",
            "Iteration: 336100 \t Train: Acc=53%, Loss=0.6939889192581177 \t\t Validation: Acc=52%, Loss=0.6914416551589966\n",
            "Iteration: 336110 \t Train: Acc=50%, Loss=0.69516921043396 \t\t Validation: Acc=53%, Loss=0.687966525554657\n",
            "Iteration: 336120 \t Train: Acc=54%, Loss=0.6861205101013184 \t\t Validation: Acc=58%, Loss=0.6849609613418579\n",
            "Iteration: 336130 \t Train: Acc=53%, Loss=0.68895423412323 \t\t Validation: Acc=49%, Loss=0.6959775686264038\n",
            "Iteration: 336140 \t Train: Acc=57%, Loss=0.6854972839355469 \t\t Validation: Acc=48%, Loss=0.6939806342124939\n",
            "Iteration: 336150 \t Train: Acc=57%, Loss=0.6839606761932373 \t\t Validation: Acc=53%, Loss=0.6874524354934692\n",
            "Iteration: 336160 \t Train: Acc=52%, Loss=0.6868413090705872 \t\t Validation: Acc=51%, Loss=0.6930717825889587\n",
            "Iteration: 336170 \t Train: Acc=52%, Loss=0.6910437941551208 \t\t Validation: Acc=55%, Loss=0.6890137195587158\n",
            "Iteration: 336180 \t Train: Acc=53%, Loss=0.6834889650344849 \t\t Validation: Acc=49%, Loss=0.6944182515144348\n",
            "Iteration: 336190 \t Train: Acc=53%, Loss=0.6893115043640137 \t\t Validation: Acc=47%, Loss=0.6987504959106445\n",
            "Iteration: 336200 \t Train: Acc=53%, Loss=0.6876211166381836 \t\t Validation: Acc=50%, Loss=0.6960941553115845\n",
            "Iteration: 336210 \t Train: Acc=47%, Loss=0.6957679986953735 \t\t Validation: Acc=52%, Loss=0.686983048915863\n",
            "Iteration: 336220 \t Train: Acc=50%, Loss=0.690970778465271 \t\t Validation: Acc=53%, Loss=0.6848052740097046\n",
            "Iteration: 336230 \t Train: Acc=47%, Loss=0.692762553691864 \t\t Validation: Acc=46%, Loss=0.6969350576400757\n",
            "Iteration: 336240 \t Train: Acc=53%, Loss=0.6894509196281433 \t\t Validation: Acc=51%, Loss=0.6946015954017639\n",
            "Iteration: 336250 \t Train: Acc=46%, Loss=0.6986033320426941 \t\t Validation: Acc=50%, Loss=0.6926974654197693\n",
            "Iteration: 336260 \t Train: Acc=53%, Loss=0.6922301650047302 \t\t Validation: Acc=47%, Loss=0.6927348375320435\n",
            "Iteration: 336270 \t Train: Acc=51%, Loss=0.6927219033241272 \t\t Validation: Acc=49%, Loss=0.6945715546607971\n",
            "Iteration: 336280 \t Train: Acc=46%, Loss=0.6913173794746399 \t\t Validation: Acc=49%, Loss=0.6963551640510559\n",
            "Iteration: 336290 \t Train: Acc=52%, Loss=0.6896152496337891 \t\t Validation: Acc=48%, Loss=0.6958807110786438\n",
            "Iteration: 336300 \t Train: Acc=51%, Loss=0.6850320100784302 \t\t Validation: Acc=54%, Loss=0.6872087717056274\n",
            "Iteration: 336310 \t Train: Acc=53%, Loss=0.6884767413139343 \t\t Validation: Acc=56%, Loss=0.6944767236709595\n",
            "Iteration: 336320 \t Train: Acc=47%, Loss=0.6914297342300415 \t\t Validation: Acc=51%, Loss=0.6917953491210938\n",
            "Iteration: 336330 \t Train: Acc=53%, Loss=0.6867530345916748 \t\t Validation: Acc=44%, Loss=0.7000930309295654\n",
            "Iteration: 336340 \t Train: Acc=50%, Loss=0.6902832984924316 \t\t Validation: Acc=47%, Loss=0.6949999928474426\n",
            "Iteration: 336350 \t Train: Acc=46%, Loss=0.6992375254631042 \t\t Validation: Acc=53%, Loss=0.6893373727798462\n",
            "Iteration: 336360 \t Train: Acc=50%, Loss=0.6948844194412231 \t\t Validation: Acc=54%, Loss=0.6874692440032959\n",
            "Iteration: 336370 \t Train: Acc=50%, Loss=0.6947497725486755 \t\t Validation: Acc=51%, Loss=0.6909626722335815\n",
            "Iteration: 336380 \t Train: Acc=51%, Loss=0.6916723847389221 \t\t Validation: Acc=52%, Loss=0.6924558877944946\n",
            "Iteration: 336390 \t Train: Acc=46%, Loss=0.6953957676887512 \t\t Validation: Acc=51%, Loss=0.6944456696510315\n",
            "Iteration: 336400 \t Train: Acc=51%, Loss=0.6905601620674133 \t\t Validation: Acc=54%, Loss=0.6897776126861572\n",
            "Iteration: 336410 \t Train: Acc=48%, Loss=0.6882146000862122 \t\t Validation: Acc=53%, Loss=0.6933417916297913\n",
            "Iteration: 336420 \t Train: Acc=50%, Loss=0.7038007974624634 \t\t Validation: Acc=53%, Loss=0.6914510130882263\n",
            "Iteration: 336430 \t Train: Acc=52%, Loss=0.6903945207595825 \t\t Validation: Acc=53%, Loss=0.6929277181625366\n",
            "Iteration: 336440 \t Train: Acc=46%, Loss=0.6939683556556702 \t\t Validation: Acc=52%, Loss=0.690746009349823\n",
            "Iteration: 336450 \t Train: Acc=54%, Loss=0.6861013770103455 \t\t Validation: Acc=53%, Loss=0.693187952041626\n",
            "Iteration: 336460 \t Train: Acc=53%, Loss=0.6910316944122314 \t\t Validation: Acc=53%, Loss=0.6947208642959595\n",
            "Iteration: 336470 \t Train: Acc=52%, Loss=0.6881203055381775 \t\t Validation: Acc=54%, Loss=0.6884152889251709\n",
            "Iteration: 336480 \t Train: Acc=50%, Loss=0.6967872381210327 \t\t Validation: Acc=46%, Loss=0.6947132349014282\n",
            "Iteration: 336490 \t Train: Acc=53%, Loss=0.6911723017692566 \t\t Validation: Acc=54%, Loss=0.6915575861930847\n",
            "Iteration: 336500 \t Train: Acc=53%, Loss=0.689765989780426 \t\t Validation: Acc=50%, Loss=0.694185197353363\n",
            "Iteration: 336510 \t Train: Acc=52%, Loss=0.6927527785301208 \t\t Validation: Acc=57%, Loss=0.6870723962783813\n",
            "Iteration: 336520 \t Train: Acc=49%, Loss=0.6901691555976868 \t\t Validation: Acc=49%, Loss=0.6929008364677429\n",
            "Iteration: 336530 \t Train: Acc=50%, Loss=0.6934642791748047 \t\t Validation: Acc=55%, Loss=0.6917762160301208\n",
            "Iteration: 336540 \t Train: Acc=52%, Loss=0.6921464204788208 \t\t Validation: Acc=53%, Loss=0.6927102208137512\n",
            "Iteration: 336550 \t Train: Acc=50%, Loss=0.6912106275558472 \t\t Validation: Acc=53%, Loss=0.692547619342804\n",
            "Iteration: 336560 \t Train: Acc=58%, Loss=0.6818207502365112 \t\t Validation: Acc=50%, Loss=0.691944420337677\n",
            "Iteration: 336570 \t Train: Acc=54%, Loss=0.6906877756118774 \t\t Validation: Acc=51%, Loss=0.6934005618095398\n",
            "Iteration: 336580 \t Train: Acc=46%, Loss=0.6928344964981079 \t\t Validation: Acc=56%, Loss=0.6896020770072937\n",
            "Iteration: 336590 \t Train: Acc=53%, Loss=0.6891315579414368 \t\t Validation: Acc=53%, Loss=0.6887490749359131\n",
            "Iteration: 336600 \t Train: Acc=58%, Loss=0.6912941336631775 \t\t Validation: Acc=53%, Loss=0.6888121366500854\n",
            "Iteration: 336610 \t Train: Acc=49%, Loss=0.6932162046432495 \t\t Validation: Acc=55%, Loss=0.6855073571205139\n",
            "Iteration: 336620 \t Train: Acc=52%, Loss=0.6948105096817017 \t\t Validation: Acc=56%, Loss=0.6880842447280884\n",
            "Iteration: 336630 \t Train: Acc=52%, Loss=0.6887319087982178 \t\t Validation: Acc=57%, Loss=0.6863043904304504\n",
            "Iteration: 336640 \t Train: Acc=50%, Loss=0.69099360704422 \t\t Validation: Acc=53%, Loss=0.6901558637619019\n",
            "Iteration: 336650 \t Train: Acc=45%, Loss=0.6943495273590088 \t\t Validation: Acc=53%, Loss=0.691531777381897\n",
            "Iteration: 336660 \t Train: Acc=50%, Loss=0.6891345977783203 \t\t Validation: Acc=50%, Loss=0.6934314966201782\n",
            "Iteration: 336670 \t Train: Acc=54%, Loss=0.6881847381591797 \t\t Validation: Acc=51%, Loss=0.691352367401123\n",
            "Iteration: 336680 \t Train: Acc=47%, Loss=0.7021128535270691 \t\t Validation: Acc=50%, Loss=0.693942129611969\n",
            "Iteration: 336690 \t Train: Acc=43%, Loss=0.7069520950317383 \t\t Validation: Acc=50%, Loss=0.694855809211731\n",
            "Iteration: 336700 \t Train: Acc=52%, Loss=0.6949824690818787 \t\t Validation: Acc=53%, Loss=0.6861841082572937\n",
            "Iteration: 336710 \t Train: Acc=50%, Loss=0.6903773546218872 \t\t Validation: Acc=54%, Loss=0.6890631914138794\n",
            "Iteration: 336720 \t Train: Acc=50%, Loss=0.7021985650062561 \t\t Validation: Acc=50%, Loss=0.6906272172927856\n",
            "Iteration: 336730 \t Train: Acc=51%, Loss=0.6916347146034241 \t\t Validation: Acc=49%, Loss=0.6912879943847656\n",
            "Iteration: 336740 \t Train: Acc=46%, Loss=0.6950136423110962 \t\t Validation: Acc=52%, Loss=0.691666305065155\n",
            "Iteration: 336750 \t Train: Acc=51%, Loss=0.6895482540130615 \t\t Validation: Acc=54%, Loss=0.6878095269203186\n",
            "Iteration: 336760 \t Train: Acc=53%, Loss=0.6868035197257996 \t\t Validation: Acc=50%, Loss=0.6917009353637695\n",
            "Iteration: 336770 \t Train: Acc=50%, Loss=0.6858288645744324 \t\t Validation: Acc=53%, Loss=0.6875100135803223\n",
            "Iteration: 336780 \t Train: Acc=48%, Loss=0.6932076811790466 \t\t Validation: Acc=47%, Loss=0.6969600319862366\n",
            "Iteration: 336790 \t Train: Acc=49%, Loss=0.6970066428184509 \t\t Validation: Acc=51%, Loss=0.6896467208862305\n",
            "Iteration: 336800 \t Train: Acc=47%, Loss=0.693206787109375 \t\t Validation: Acc=50%, Loss=0.6907399892807007\n",
            "Iteration: 336810 \t Train: Acc=51%, Loss=0.6933258175849915 \t\t Validation: Acc=48%, Loss=0.694933295249939\n",
            "Iteration: 336820 \t Train: Acc=49%, Loss=0.6890551447868347 \t\t Validation: Acc=47%, Loss=0.6950762271881104\n",
            "Iteration: 336830 \t Train: Acc=50%, Loss=0.6966478228569031 \t\t Validation: Acc=47%, Loss=0.6907788515090942\n",
            "Iteration: 336840 \t Train: Acc=53%, Loss=0.6945894956588745 \t\t Validation: Acc=49%, Loss=0.6923137307167053\n",
            "Iteration: 336850 \t Train: Acc=50%, Loss=0.690925121307373 \t\t Validation: Acc=48%, Loss=0.692539393901825\n",
            "Iteration: 336860 \t Train: Acc=57%, Loss=0.6882800459861755 \t\t Validation: Acc=55%, Loss=0.6869307160377502\n",
            "Iteration: 336870 \t Train: Acc=50%, Loss=0.6963760256767273 \t\t Validation: Acc=47%, Loss=0.6962725520133972\n",
            "Iteration: 336880 \t Train: Acc=46%, Loss=0.6974853277206421 \t\t Validation: Acc=53%, Loss=0.6902497410774231\n",
            "Iteration: 336890 \t Train: Acc=52%, Loss=0.692337155342102 \t\t Validation: Acc=46%, Loss=0.6956139802932739\n",
            "Iteration: 336900 \t Train: Acc=52%, Loss=0.6966166496276855 \t\t Validation: Acc=52%, Loss=0.6909501552581787\n",
            "Iteration: 336910 \t Train: Acc=49%, Loss=0.6904563903808594 \t\t Validation: Acc=50%, Loss=0.6919130086898804\n",
            "Iteration: 336920 \t Train: Acc=54%, Loss=0.696236789226532 \t\t Validation: Acc=53%, Loss=0.6920061707496643\n",
            "Iteration: 336930 \t Train: Acc=52%, Loss=0.6921961903572083 \t\t Validation: Acc=51%, Loss=0.6901779770851135\n",
            "Iteration: 336940 \t Train: Acc=48%, Loss=0.6903074383735657 \t\t Validation: Acc=49%, Loss=0.697818398475647\n",
            "Iteration: 336950 \t Train: Acc=50%, Loss=0.6980278491973877 \t\t Validation: Acc=49%, Loss=0.6932465434074402\n",
            "Iteration: 336960 \t Train: Acc=53%, Loss=0.6909582614898682 \t\t Validation: Acc=46%, Loss=0.6992608904838562\n",
            "Iteration: 336970 \t Train: Acc=43%, Loss=0.6988051533699036 \t\t Validation: Acc=50%, Loss=0.6950240731239319\n",
            "Iteration: 336980 \t Train: Acc=49%, Loss=0.6909171938896179 \t\t Validation: Acc=51%, Loss=0.6912651062011719\n",
            "Iteration: 336990 \t Train: Acc=47%, Loss=0.6944316029548645 \t\t Validation: Acc=50%, Loss=0.6945846080780029\n",
            "Iteration: 337000 \t Train: Acc=50%, Loss=0.6923683285713196 \t\t Validation: Acc=49%, Loss=0.6937887072563171\n",
            "Iteration: 337010 \t Train: Acc=50%, Loss=0.696033775806427 \t\t Validation: Acc=48%, Loss=0.6937297582626343\n",
            "Iteration: 337020 \t Train: Acc=46%, Loss=0.6986382007598877 \t\t Validation: Acc=50%, Loss=0.6921080350875854\n",
            "Iteration: 337030 \t Train: Acc=46%, Loss=0.6982343196868896 \t\t Validation: Acc=46%, Loss=0.6946126222610474\n",
            "Iteration: 337040 \t Train: Acc=57%, Loss=0.6853989958763123 \t\t Validation: Acc=47%, Loss=0.6908454298973083\n",
            "Iteration: 337050 \t Train: Acc=47%, Loss=0.6961747407913208 \t\t Validation: Acc=52%, Loss=0.6928713321685791\n",
            "Iteration: 337060 \t Train: Acc=53%, Loss=0.6887255907058716 \t\t Validation: Acc=50%, Loss=0.6931488513946533\n",
            "Iteration: 337070 \t Train: Acc=49%, Loss=0.6925745010375977 \t\t Validation: Acc=53%, Loss=0.6894460320472717\n",
            "Iteration: 337080 \t Train: Acc=50%, Loss=0.6908479928970337 \t\t Validation: Acc=50%, Loss=0.6884272694587708\n",
            "Iteration: 337090 \t Train: Acc=53%, Loss=0.6889170408248901 \t\t Validation: Acc=54%, Loss=0.6892728209495544\n",
            "Iteration: 337100 \t Train: Acc=50%, Loss=0.6911764144897461 \t\t Validation: Acc=53%, Loss=0.6904366612434387\n",
            "Iteration: 337110 \t Train: Acc=50%, Loss=0.6876857876777649 \t\t Validation: Acc=52%, Loss=0.6909948587417603\n",
            "Iteration: 337120 \t Train: Acc=50%, Loss=0.6926465630531311 \t\t Validation: Acc=59%, Loss=0.6892747282981873\n",
            "Iteration: 337130 \t Train: Acc=50%, Loss=0.6969435811042786 \t\t Validation: Acc=49%, Loss=0.6954509019851685\n",
            "Iteration: 337140 \t Train: Acc=46%, Loss=0.6937991380691528 \t\t Validation: Acc=55%, Loss=0.6925562620162964\n",
            "Iteration: 337150 \t Train: Acc=48%, Loss=0.693737268447876 \t\t Validation: Acc=46%, Loss=0.6951109170913696\n",
            "Iteration: 337160 \t Train: Acc=47%, Loss=0.6940010786056519 \t\t Validation: Acc=49%, Loss=0.6925622224807739\n",
            "Iteration: 337170 \t Train: Acc=54%, Loss=0.6853266358375549 \t\t Validation: Acc=51%, Loss=0.6949949264526367\n",
            "Iteration: 337180 \t Train: Acc=56%, Loss=0.6972782611846924 \t\t Validation: Acc=50%, Loss=0.6935108304023743\n",
            "Iteration: 337190 \t Train: Acc=47%, Loss=0.6970995664596558 \t\t Validation: Acc=51%, Loss=0.6917262077331543\n",
            "Iteration: 337200 \t Train: Acc=46%, Loss=0.699113130569458 \t\t Validation: Acc=51%, Loss=0.6923049688339233\n",
            "Iteration: 337210 \t Train: Acc=52%, Loss=0.686145544052124 \t\t Validation: Acc=50%, Loss=0.6937170028686523\n",
            "Iteration: 337220 \t Train: Acc=50%, Loss=0.686242938041687 \t\t Validation: Acc=49%, Loss=0.6957634687423706\n",
            "Iteration: 337230 \t Train: Acc=50%, Loss=0.6900824308395386 \t\t Validation: Acc=50%, Loss=0.6927011609077454\n",
            "Iteration: 337240 \t Train: Acc=52%, Loss=0.6868827939033508 \t\t Validation: Acc=50%, Loss=0.6942527294158936\n",
            "Iteration: 337250 \t Train: Acc=52%, Loss=0.688432514667511 \t\t Validation: Acc=56%, Loss=0.686805009841919\n",
            "Iteration: 337260 \t Train: Acc=54%, Loss=0.6846415400505066 \t\t Validation: Acc=50%, Loss=0.69281405210495\n",
            "Iteration: 337270 \t Train: Acc=51%, Loss=0.6955790519714355 \t\t Validation: Acc=53%, Loss=0.688538134098053\n",
            "Iteration: 337280 \t Train: Acc=52%, Loss=0.6890928745269775 \t\t Validation: Acc=46%, Loss=0.6945161819458008\n",
            "Iteration: 337290 \t Train: Acc=45%, Loss=0.6994709968566895 \t\t Validation: Acc=53%, Loss=0.6879623532295227\n",
            "Iteration: 337300 \t Train: Acc=55%, Loss=0.6843394041061401 \t\t Validation: Acc=50%, Loss=0.6886024475097656\n",
            "Iteration: 337310 \t Train: Acc=46%, Loss=0.6933754682540894 \t\t Validation: Acc=46%, Loss=0.7027134299278259\n",
            "Iteration: 337320 \t Train: Acc=47%, Loss=0.7002350091934204 \t\t Validation: Acc=47%, Loss=0.6901521682739258\n",
            "Iteration: 337330 \t Train: Acc=55%, Loss=0.6912144422531128 \t\t Validation: Acc=51%, Loss=0.6959301829338074\n",
            "Iteration: 337340 \t Train: Acc=49%, Loss=0.6952371001243591 \t\t Validation: Acc=50%, Loss=0.6953275203704834\n",
            "Iteration: 337350 \t Train: Acc=57%, Loss=0.6873634457588196 \t\t Validation: Acc=51%, Loss=0.6951266527175903\n",
            "Iteration: 337360 \t Train: Acc=50%, Loss=0.6967130303382874 \t\t Validation: Acc=50%, Loss=0.6938135623931885\n",
            "Iteration: 337370 \t Train: Acc=54%, Loss=0.6870684027671814 \t\t Validation: Acc=50%, Loss=0.690723717212677\n",
            "Iteration: 337380 \t Train: Acc=53%, Loss=0.6919640302658081 \t\t Validation: Acc=50%, Loss=0.6895576119422913\n",
            "Iteration: 337390 \t Train: Acc=53%, Loss=0.6962145566940308 \t\t Validation: Acc=51%, Loss=0.6931118965148926\n",
            "Iteration: 337400 \t Train: Acc=57%, Loss=0.6850585341453552 \t\t Validation: Acc=50%, Loss=0.6918876767158508\n",
            "Iteration: 337410 \t Train: Acc=46%, Loss=0.6910707950592041 \t\t Validation: Acc=47%, Loss=0.696657657623291\n",
            "Iteration: 337420 \t Train: Acc=50%, Loss=0.6930359601974487 \t\t Validation: Acc=50%, Loss=0.6980615854263306\n",
            "Iteration: 337430 \t Train: Acc=54%, Loss=0.6897975206375122 \t\t Validation: Acc=53%, Loss=0.692071259021759\n",
            "Iteration: 337440 \t Train: Acc=54%, Loss=0.6928088665008545 \t\t Validation: Acc=46%, Loss=0.7002155780792236\n",
            "Iteration: 337450 \t Train: Acc=53%, Loss=0.6933178901672363 \t\t Validation: Acc=49%, Loss=0.6919866800308228\n",
            "Iteration: 337460 \t Train: Acc=51%, Loss=0.6890319585800171 \t\t Validation: Acc=48%, Loss=0.6934972405433655\n",
            "Iteration: 337470 \t Train: Acc=55%, Loss=0.6864712238311768 \t\t Validation: Acc=47%, Loss=0.6957851648330688\n",
            "Iteration: 337480 \t Train: Acc=52%, Loss=0.6911439299583435 \t\t Validation: Acc=50%, Loss=0.6869606971740723\n",
            "Iteration: 337490 \t Train: Acc=49%, Loss=0.6927429437637329 \t\t Validation: Acc=52%, Loss=0.6900250911712646\n",
            "Iteration: 337500 \t Train: Acc=46%, Loss=0.6966086030006409 \t\t Validation: Acc=46%, Loss=0.6952312588691711\n",
            "Iteration: 337510 \t Train: Acc=40%, Loss=0.6965766549110413 \t\t Validation: Acc=48%, Loss=0.6994479298591614\n",
            "Iteration: 337520 \t Train: Acc=46%, Loss=0.6939006447792053 \t\t Validation: Acc=53%, Loss=0.6896932721138\n",
            "Iteration: 337530 \t Train: Acc=46%, Loss=0.697263240814209 \t\t Validation: Acc=50%, Loss=0.695453405380249\n",
            "Iteration: 337540 \t Train: Acc=51%, Loss=0.6902849078178406 \t\t Validation: Acc=47%, Loss=0.699431300163269\n",
            "Iteration: 337550 \t Train: Acc=47%, Loss=0.6923800706863403 \t\t Validation: Acc=46%, Loss=0.6936759948730469\n",
            "Iteration: 337560 \t Train: Acc=54%, Loss=0.6906654834747314 \t\t Validation: Acc=50%, Loss=0.6970119476318359\n",
            "Iteration: 337570 \t Train: Acc=44%, Loss=0.6904677152633667 \t\t Validation: Acc=48%, Loss=0.6993467807769775\n",
            "Iteration: 337580 \t Train: Acc=55%, Loss=0.6903328895568848 \t\t Validation: Acc=56%, Loss=0.6884152889251709\n",
            "Iteration: 337590 \t Train: Acc=50%, Loss=0.6915364861488342 \t\t Validation: Acc=53%, Loss=0.6903458833694458\n",
            "Iteration: 337600 \t Train: Acc=56%, Loss=0.6885277032852173 \t\t Validation: Acc=50%, Loss=0.6873298287391663\n",
            "Iteration: 337610 \t Train: Acc=51%, Loss=0.698166012763977 \t\t Validation: Acc=48%, Loss=0.6940082311630249\n",
            "Iteration: 337620 \t Train: Acc=46%, Loss=0.6960412859916687 \t\t Validation: Acc=46%, Loss=0.7017946243286133\n",
            "Iteration: 337630 \t Train: Acc=53%, Loss=0.693094789981842 \t\t Validation: Acc=48%, Loss=0.6949812769889832\n",
            "Iteration: 337640 \t Train: Acc=51%, Loss=0.6966530084609985 \t\t Validation: Acc=53%, Loss=0.6915264129638672\n",
            "Iteration: 337650 \t Train: Acc=47%, Loss=0.6939531564712524 \t\t Validation: Acc=53%, Loss=0.6896857023239136\n",
            "Iteration: 337660 \t Train: Acc=50%, Loss=0.6878486275672913 \t\t Validation: Acc=51%, Loss=0.6966748833656311\n",
            "Iteration: 337670 \t Train: Acc=46%, Loss=0.7039185762405396 \t\t Validation: Acc=52%, Loss=0.692229151725769\n",
            "Iteration: 337680 \t Train: Acc=56%, Loss=0.7005690932273865 \t\t Validation: Acc=52%, Loss=0.687584400177002\n",
            "Iteration: 337690 \t Train: Acc=47%, Loss=0.7045313715934753 \t\t Validation: Acc=49%, Loss=0.6926003098487854\n",
            "Iteration: 337700 \t Train: Acc=46%, Loss=0.6989673972129822 \t\t Validation: Acc=53%, Loss=0.6897652745246887\n",
            "Iteration: 337710 \t Train: Acc=53%, Loss=0.6831761002540588 \t\t Validation: Acc=51%, Loss=0.6919929385185242\n",
            "Iteration: 337720 \t Train: Acc=54%, Loss=0.6909186244010925 \t\t Validation: Acc=51%, Loss=0.6901530623435974\n",
            "Iteration: 337730 \t Train: Acc=47%, Loss=0.6941922903060913 \t\t Validation: Acc=49%, Loss=0.6986478567123413\n",
            "Iteration: 337740 \t Train: Acc=44%, Loss=0.7004568576812744 \t\t Validation: Acc=50%, Loss=0.6887915730476379\n",
            "Iteration: 337750 \t Train: Acc=53%, Loss=0.6916634440422058 \t\t Validation: Acc=49%, Loss=0.6966095566749573\n",
            "Iteration: 337760 \t Train: Acc=48%, Loss=0.6968791484832764 \t\t Validation: Acc=52%, Loss=0.6952252388000488\n",
            "Iteration: 337770 \t Train: Acc=48%, Loss=0.6959160566329956 \t\t Validation: Acc=47%, Loss=0.6941303610801697\n",
            "Iteration: 337780 \t Train: Acc=53%, Loss=0.6894891262054443 \t\t Validation: Acc=46%, Loss=0.6898680925369263\n",
            "Iteration: 337790 \t Train: Acc=57%, Loss=0.6835415959358215 \t\t Validation: Acc=51%, Loss=0.6906546950340271\n",
            "Iteration: 337800 \t Train: Acc=54%, Loss=0.6862428188323975 \t\t Validation: Acc=46%, Loss=0.6967636346817017\n",
            "Iteration: 337810 \t Train: Acc=55%, Loss=0.6872949600219727 \t\t Validation: Acc=56%, Loss=0.6884782910346985\n",
            "Iteration: 337820 \t Train: Acc=50%, Loss=0.6878135204315186 \t\t Validation: Acc=54%, Loss=0.6893810629844666\n",
            "Iteration: 337830 \t Train: Acc=44%, Loss=0.6970240473747253 \t\t Validation: Acc=44%, Loss=0.7004333734512329\n",
            "Iteration: 337840 \t Train: Acc=54%, Loss=0.6922216415405273 \t\t Validation: Acc=51%, Loss=0.6881141662597656\n",
            "Iteration: 337850 \t Train: Acc=46%, Loss=0.6933380365371704 \t\t Validation: Acc=47%, Loss=0.697137713432312\n",
            "Iteration: 337860 \t Train: Acc=56%, Loss=0.6837557554244995 \t\t Validation: Acc=48%, Loss=0.6951844692230225\n",
            "Iteration: 337870 \t Train: Acc=49%, Loss=0.7001941800117493 \t\t Validation: Acc=53%, Loss=0.689590573310852\n",
            "Iteration: 337880 \t Train: Acc=53%, Loss=0.6888481378555298 \t\t Validation: Acc=53%, Loss=0.6900656819343567\n",
            "Iteration: 337890 \t Train: Acc=56%, Loss=0.685606837272644 \t\t Validation: Acc=56%, Loss=0.6857576370239258\n",
            "Iteration: 337900 \t Train: Acc=51%, Loss=0.6911209225654602 \t\t Validation: Acc=50%, Loss=0.6939927339553833\n",
            "Iteration: 337910 \t Train: Acc=50%, Loss=0.6911495327949524 \t\t Validation: Acc=53%, Loss=0.6879159212112427\n",
            "Iteration: 337920 \t Train: Acc=49%, Loss=0.6926100254058838 \t\t Validation: Acc=54%, Loss=0.6864017248153687\n",
            "Iteration: 337930 \t Train: Acc=50%, Loss=0.6927639245986938 \t\t Validation: Acc=50%, Loss=0.6913172602653503\n",
            "Iteration: 337940 \t Train: Acc=56%, Loss=0.6877381801605225 \t\t Validation: Acc=53%, Loss=0.687239408493042\n",
            "Iteration: 337950 \t Train: Acc=46%, Loss=0.6949890851974487 \t\t Validation: Acc=44%, Loss=0.6972759962081909\n",
            "Iteration: 337960 \t Train: Acc=49%, Loss=0.6875702142715454 \t\t Validation: Acc=51%, Loss=0.6948310732841492\n",
            "Iteration: 337970 \t Train: Acc=48%, Loss=0.693343997001648 \t\t Validation: Acc=53%, Loss=0.6858159899711609\n",
            "Iteration: 337980 \t Train: Acc=53%, Loss=0.6901534795761108 \t\t Validation: Acc=51%, Loss=0.6942979097366333\n",
            "Iteration: 337990 \t Train: Acc=52%, Loss=0.688142716884613 \t\t Validation: Acc=50%, Loss=0.6891019344329834\n",
            "Iteration: 338000 \t Train: Acc=41%, Loss=0.6998773217201233 \t\t Validation: Acc=50%, Loss=0.6997532248497009\n",
            "Iteration: 338010 \t Train: Acc=46%, Loss=0.6931639909744263 \t\t Validation: Acc=48%, Loss=0.6943491101264954\n",
            "Iteration: 338020 \t Train: Acc=57%, Loss=0.6882781982421875 \t\t Validation: Acc=53%, Loss=0.686431348323822\n",
            "Iteration: 338030 \t Train: Acc=48%, Loss=0.6945573091506958 \t\t Validation: Acc=46%, Loss=0.6973301768302917\n",
            "Iteration: 338040 \t Train: Acc=57%, Loss=0.6865487098693848 \t\t Validation: Acc=50%, Loss=0.6924704909324646\n",
            "Iteration: 338050 \t Train: Acc=49%, Loss=0.6911976337432861 \t\t Validation: Acc=50%, Loss=0.6932399272918701\n",
            "Iteration: 338060 \t Train: Acc=52%, Loss=0.6920102834701538 \t\t Validation: Acc=54%, Loss=0.692540168762207\n",
            "Iteration: 338070 \t Train: Acc=41%, Loss=0.6969069242477417 \t\t Validation: Acc=46%, Loss=0.696241557598114\n",
            "Iteration: 338080 \t Train: Acc=47%, Loss=0.6956853270530701 \t\t Validation: Acc=52%, Loss=0.6884847283363342\n",
            "Iteration: 338090 \t Train: Acc=50%, Loss=0.6868191957473755 \t\t Validation: Acc=50%, Loss=0.6927595138549805\n",
            "Iteration: 338100 \t Train: Acc=53%, Loss=0.6890491843223572 \t\t Validation: Acc=51%, Loss=0.6939463019371033\n",
            "Iteration: 338110 \t Train: Acc=47%, Loss=0.696052610874176 \t\t Validation: Acc=51%, Loss=0.6958031058311462\n",
            "Iteration: 338120 \t Train: Acc=48%, Loss=0.6912397742271423 \t\t Validation: Acc=53%, Loss=0.6914512515068054\n",
            "Iteration: 338130 \t Train: Acc=50%, Loss=0.6877065300941467 \t\t Validation: Acc=51%, Loss=0.690811038017273\n",
            "Iteration: 338140 \t Train: Acc=50%, Loss=0.6866054534912109 \t\t Validation: Acc=48%, Loss=0.6928715109825134\n",
            "Iteration: 338150 \t Train: Acc=51%, Loss=0.6923840045928955 \t\t Validation: Acc=48%, Loss=0.6898974776268005\n",
            "Iteration: 338160 \t Train: Acc=50%, Loss=0.6921616792678833 \t\t Validation: Acc=53%, Loss=0.6894199848175049\n",
            "Iteration: 338170 \t Train: Acc=50%, Loss=0.6936970949172974 \t\t Validation: Acc=53%, Loss=0.6900692582130432\n",
            "Iteration: 338180 \t Train: Acc=53%, Loss=0.6850315928459167 \t\t Validation: Acc=46%, Loss=0.6964375376701355\n",
            "Iteration: 338190 \t Train: Acc=53%, Loss=0.6930469870567322 \t\t Validation: Acc=51%, Loss=0.6951752305030823\n",
            "Iteration: 338200 \t Train: Acc=54%, Loss=0.6848632097244263 \t\t Validation: Acc=52%, Loss=0.6926167011260986\n",
            "Iteration: 338210 \t Train: Acc=52%, Loss=0.6904689073562622 \t\t Validation: Acc=45%, Loss=0.6936200857162476\n",
            "Iteration: 338220 \t Train: Acc=51%, Loss=0.6910780668258667 \t\t Validation: Acc=48%, Loss=0.6942328810691833\n",
            "Iteration: 338230 \t Train: Acc=51%, Loss=0.6941554546356201 \t\t Validation: Acc=53%, Loss=0.6890677213668823\n",
            "Iteration: 338240 \t Train: Acc=51%, Loss=0.6932805180549622 \t\t Validation: Acc=53%, Loss=0.687240719795227\n",
            "Iteration: 338250 \t Train: Acc=53%, Loss=0.6879552006721497 \t\t Validation: Acc=49%, Loss=0.6970727443695068\n",
            "Iteration: 338260 \t Train: Acc=53%, Loss=0.6892489194869995 \t\t Validation: Acc=48%, Loss=0.6944694519042969\n",
            "Iteration: 338270 \t Train: Acc=49%, Loss=0.6977393627166748 \t\t Validation: Acc=52%, Loss=0.6905727982521057\n",
            "Iteration: 338280 \t Train: Acc=55%, Loss=0.6896796822547913 \t\t Validation: Acc=50%, Loss=0.693474292755127\n",
            "Iteration: 338290 \t Train: Acc=57%, Loss=0.68769371509552 \t\t Validation: Acc=51%, Loss=0.6939388513565063\n",
            "Iteration: 338300 \t Train: Acc=50%, Loss=0.6964083909988403 \t\t Validation: Acc=52%, Loss=0.688782274723053\n",
            "Iteration: 338310 \t Train: Acc=52%, Loss=0.6893405914306641 \t\t Validation: Acc=46%, Loss=0.6941014528274536\n",
            "Iteration: 338320 \t Train: Acc=47%, Loss=0.6928225159645081 \t\t Validation: Acc=51%, Loss=0.6938554644584656\n",
            "Iteration: 338330 \t Train: Acc=50%, Loss=0.6900516748428345 \t\t Validation: Acc=52%, Loss=0.6857786774635315\n",
            "Iteration: 338340 \t Train: Acc=56%, Loss=0.6931061744689941 \t\t Validation: Acc=48%, Loss=0.6939051151275635\n",
            "Iteration: 338350 \t Train: Acc=49%, Loss=0.6935033798217773 \t\t Validation: Acc=50%, Loss=0.6948598623275757\n",
            "Iteration: 338360 \t Train: Acc=52%, Loss=0.685960054397583 \t\t Validation: Acc=51%, Loss=0.6908518075942993\n",
            "Iteration: 338370 \t Train: Acc=56%, Loss=0.6861302852630615 \t\t Validation: Acc=50%, Loss=0.6897459626197815\n",
            "Iteration: 338380 \t Train: Acc=53%, Loss=0.6896665692329407 \t\t Validation: Acc=51%, Loss=0.6915179491043091\n",
            "Iteration: 338390 \t Train: Acc=53%, Loss=0.6861433386802673 \t\t Validation: Acc=53%, Loss=0.6961901187896729\n",
            "Iteration: 338400 \t Train: Acc=52%, Loss=0.6963880062103271 \t\t Validation: Acc=51%, Loss=0.6963866949081421\n",
            "Iteration: 338410 \t Train: Acc=50%, Loss=0.6933867335319519 \t\t Validation: Acc=49%, Loss=0.6934148073196411\n",
            "Iteration: 338420 \t Train: Acc=51%, Loss=0.6889483332633972 \t\t Validation: Acc=53%, Loss=0.6931418180465698\n",
            "Iteration: 338430 \t Train: Acc=52%, Loss=0.6931815147399902 \t\t Validation: Acc=57%, Loss=0.6848688125610352\n",
            "Iteration: 338440 \t Train: Acc=53%, Loss=0.6896712779998779 \t\t Validation: Acc=49%, Loss=0.6939502954483032\n",
            "Iteration: 338450 \t Train: Acc=53%, Loss=0.6904047131538391 \t\t Validation: Acc=49%, Loss=0.6955392360687256\n",
            "Iteration: 338460 \t Train: Acc=53%, Loss=0.6917455792427063 \t\t Validation: Acc=57%, Loss=0.6892868280410767\n",
            "Iteration: 338470 \t Train: Acc=50%, Loss=0.693915843963623 \t\t Validation: Acc=50%, Loss=0.6944565773010254\n",
            "Iteration: 338480 \t Train: Acc=50%, Loss=0.6968061327934265 \t\t Validation: Acc=53%, Loss=0.6936846971511841\n",
            "Iteration: 338490 \t Train: Acc=44%, Loss=0.6950042247772217 \t\t Validation: Acc=53%, Loss=0.6846405863761902\n",
            "Iteration: 338500 \t Train: Acc=54%, Loss=0.6874518394470215 \t\t Validation: Acc=49%, Loss=0.6947929263114929\n",
            "Iteration: 338510 \t Train: Acc=49%, Loss=0.6923151016235352 \t\t Validation: Acc=48%, Loss=0.6958284378051758\n",
            "Iteration: 338520 \t Train: Acc=53%, Loss=0.6829653978347778 \t\t Validation: Acc=56%, Loss=0.6887229084968567\n",
            "Iteration: 338530 \t Train: Acc=54%, Loss=0.6883036494255066 \t\t Validation: Acc=46%, Loss=0.6943632364273071\n",
            "Iteration: 338540 \t Train: Acc=52%, Loss=0.6893835067749023 \t\t Validation: Acc=49%, Loss=0.6923532485961914\n",
            "Iteration: 338550 \t Train: Acc=48%, Loss=0.6948080658912659 \t\t Validation: Acc=48%, Loss=0.6954219937324524\n",
            "Iteration: 338560 \t Train: Acc=64%, Loss=0.6833170652389526 \t\t Validation: Acc=46%, Loss=0.6942623853683472\n",
            "Iteration: 338570 \t Train: Acc=49%, Loss=0.6963548064231873 \t\t Validation: Acc=46%, Loss=0.6978960633277893\n",
            "Iteration: 338580 \t Train: Acc=50%, Loss=0.6906548142433167 \t\t Validation: Acc=51%, Loss=0.695305347442627\n",
            "Iteration: 338590 \t Train: Acc=52%, Loss=0.6984320878982544 \t\t Validation: Acc=51%, Loss=0.6923769116401672\n",
            "Iteration: 338600 \t Train: Acc=49%, Loss=0.6927465796470642 \t\t Validation: Acc=48%, Loss=0.6967939138412476\n",
            "Iteration: 338610 \t Train: Acc=50%, Loss=0.6958670616149902 \t\t Validation: Acc=46%, Loss=0.6954547762870789\n",
            "Iteration: 338620 \t Train: Acc=50%, Loss=0.6954649686813354 \t\t Validation: Acc=48%, Loss=0.6923877596855164\n",
            "Iteration: 338630 \t Train: Acc=53%, Loss=0.6913801431655884 \t\t Validation: Acc=50%, Loss=0.690955638885498\n",
            "Iteration: 338640 \t Train: Acc=54%, Loss=0.6895405054092407 \t\t Validation: Acc=50%, Loss=0.699265718460083\n",
            "Iteration: 338650 \t Train: Acc=50%, Loss=0.6954684257507324 \t\t Validation: Acc=50%, Loss=0.6886043548583984\n",
            "Iteration: 338660 \t Train: Acc=53%, Loss=0.683223307132721 \t\t Validation: Acc=53%, Loss=0.6864131093025208\n",
            "Iteration: 338670 \t Train: Acc=56%, Loss=0.685102641582489 \t\t Validation: Acc=51%, Loss=0.6921529769897461\n",
            "Iteration: 338680 \t Train: Acc=51%, Loss=0.6925880312919617 \t\t Validation: Acc=49%, Loss=0.6947453022003174\n",
            "Iteration: 338690 \t Train: Acc=49%, Loss=0.6945047378540039 \t\t Validation: Acc=49%, Loss=0.6901897192001343\n",
            "Iteration: 338700 \t Train: Acc=48%, Loss=0.6994022727012634 \t\t Validation: Acc=51%, Loss=0.696406900882721\n",
            "Iteration: 338710 \t Train: Acc=50%, Loss=0.6919565796852112 \t\t Validation: Acc=48%, Loss=0.6953160762786865\n",
            "Iteration: 338720 \t Train: Acc=49%, Loss=0.6981867551803589 \t\t Validation: Acc=50%, Loss=0.6907373666763306\n",
            "Iteration: 338730 \t Train: Acc=53%, Loss=0.6900744438171387 \t\t Validation: Acc=50%, Loss=0.6915121674537659\n",
            "Iteration: 338740 \t Train: Acc=49%, Loss=0.6963996887207031 \t\t Validation: Acc=49%, Loss=0.694039523601532\n",
            "Iteration: 338750 \t Train: Acc=52%, Loss=0.6983881592750549 \t\t Validation: Acc=49%, Loss=0.6931025981903076\n",
            "Iteration: 338760 \t Train: Acc=51%, Loss=0.6962486505508423 \t\t Validation: Acc=50%, Loss=0.6907504200935364\n",
            "Iteration: 338770 \t Train: Acc=54%, Loss=0.6882975101470947 \t\t Validation: Acc=49%, Loss=0.692971408367157\n",
            "Iteration: 338780 \t Train: Acc=52%, Loss=0.6909143924713135 \t\t Validation: Acc=46%, Loss=0.6959901452064514\n",
            "Iteration: 338790 \t Train: Acc=50%, Loss=0.6906828880310059 \t\t Validation: Acc=53%, Loss=0.6906945109367371\n",
            "Iteration: 338800 \t Train: Acc=49%, Loss=0.6952012777328491 \t\t Validation: Acc=50%, Loss=0.6906116008758545\n",
            "Iteration: 338810 \t Train: Acc=55%, Loss=0.6883077621459961 \t\t Validation: Acc=54%, Loss=0.6844081282615662\n",
            "Iteration: 338820 \t Train: Acc=53%, Loss=0.6895771026611328 \t\t Validation: Acc=46%, Loss=0.6947463750839233\n",
            "Iteration: 338830 \t Train: Acc=57%, Loss=0.6903261542320251 \t\t Validation: Acc=51%, Loss=0.6900191903114319\n",
            "Iteration: 338840 \t Train: Acc=57%, Loss=0.691255509853363 \t\t Validation: Acc=50%, Loss=0.6937366724014282\n",
            "Iteration: 338850 \t Train: Acc=49%, Loss=0.6958307027816772 \t\t Validation: Acc=50%, Loss=0.6891338229179382\n",
            "Iteration: 338860 \t Train: Acc=53%, Loss=0.6876192092895508 \t\t Validation: Acc=49%, Loss=0.691351056098938\n",
            "Iteration: 338870 \t Train: Acc=52%, Loss=0.6892194747924805 \t\t Validation: Acc=50%, Loss=0.6926323175430298\n",
            "Iteration: 338880 \t Train: Acc=50%, Loss=0.6934460401535034 \t\t Validation: Acc=51%, Loss=0.6930397748947144\n",
            "Iteration: 338890 \t Train: Acc=57%, Loss=0.691307783126831 \t\t Validation: Acc=53%, Loss=0.6891173124313354\n",
            "Iteration: 338900 \t Train: Acc=51%, Loss=0.688170313835144 \t\t Validation: Acc=51%, Loss=0.690659761428833\n",
            "Iteration: 338910 \t Train: Acc=42%, Loss=0.6945078372955322 \t\t Validation: Acc=50%, Loss=0.6902989745140076\n",
            "Iteration: 338920 \t Train: Acc=46%, Loss=0.6957623958587646 \t\t Validation: Acc=52%, Loss=0.6935684084892273\n",
            "Iteration: 338930 \t Train: Acc=50%, Loss=0.6971299648284912 \t\t Validation: Acc=55%, Loss=0.686733067035675\n",
            "Iteration: 338940 \t Train: Acc=50%, Loss=0.6931049823760986 \t\t Validation: Acc=48%, Loss=0.6926952004432678\n",
            "Iteration: 338950 \t Train: Acc=49%, Loss=0.690598726272583 \t\t Validation: Acc=48%, Loss=0.6928091049194336\n",
            "Iteration: 338960 \t Train: Acc=48%, Loss=0.6893243789672852 \t\t Validation: Acc=53%, Loss=0.6907070875167847\n",
            "Iteration: 338970 \t Train: Acc=43%, Loss=0.6962876319885254 \t\t Validation: Acc=46%, Loss=0.6933258175849915\n",
            "Iteration: 338980 \t Train: Acc=44%, Loss=0.7019213438034058 \t\t Validation: Acc=49%, Loss=0.6938384175300598\n",
            "Iteration: 338990 \t Train: Acc=51%, Loss=0.6894810199737549 \t\t Validation: Acc=47%, Loss=0.6931678056716919\n",
            "Iteration: 339000 \t Train: Acc=46%, Loss=0.6982210874557495 \t\t Validation: Acc=53%, Loss=0.6910318732261658\n",
            "Iteration: 339010 \t Train: Acc=50%, Loss=0.6897407174110413 \t\t Validation: Acc=44%, Loss=0.694911003112793\n",
            "Iteration: 339020 \t Train: Acc=46%, Loss=0.6951922178268433 \t\t Validation: Acc=50%, Loss=0.6921818256378174\n",
            "Iteration: 339030 \t Train: Acc=53%, Loss=0.6840779185295105 \t\t Validation: Acc=54%, Loss=0.6887403726577759\n",
            "Iteration: 339040 \t Train: Acc=52%, Loss=0.6959289908409119 \t\t Validation: Acc=52%, Loss=0.689425528049469\n",
            "Iteration: 339050 \t Train: Acc=50%, Loss=0.6875838041305542 \t\t Validation: Acc=48%, Loss=0.6937915086746216\n",
            "Iteration: 339060 \t Train: Acc=52%, Loss=0.6911576986312866 \t\t Validation: Acc=53%, Loss=0.6879264116287231\n",
            "Iteration: 339070 \t Train: Acc=48%, Loss=0.6975923776626587 \t\t Validation: Acc=52%, Loss=0.6896219253540039\n",
            "Iteration: 339080 \t Train: Acc=54%, Loss=0.6879571676254272 \t\t Validation: Acc=50%, Loss=0.6896098852157593\n",
            "Iteration: 339090 \t Train: Acc=50%, Loss=0.695801317691803 \t\t Validation: Acc=54%, Loss=0.688332736492157\n",
            "Iteration: 339100 \t Train: Acc=53%, Loss=0.6905757784843445 \t\t Validation: Acc=50%, Loss=0.6941161155700684\n",
            "Iteration: 339110 \t Train: Acc=50%, Loss=0.6908381581306458 \t\t Validation: Acc=47%, Loss=0.6957040429115295\n",
            "Iteration: 339120 \t Train: Acc=49%, Loss=0.6938519477844238 \t\t Validation: Acc=50%, Loss=0.6940659880638123\n",
            "Iteration: 339130 \t Train: Acc=53%, Loss=0.6846281290054321 \t\t Validation: Acc=50%, Loss=0.6928524971008301\n",
            "Iteration: 339140 \t Train: Acc=54%, Loss=0.6935105323791504 \t\t Validation: Acc=51%, Loss=0.6918065547943115\n",
            "Iteration: 339150 \t Train: Acc=50%, Loss=0.6913986206054688 \t\t Validation: Acc=54%, Loss=0.6879936456680298\n",
            "Iteration: 339160 \t Train: Acc=51%, Loss=0.6881062984466553 \t\t Validation: Acc=46%, Loss=0.6986173391342163\n",
            "Iteration: 339170 \t Train: Acc=46%, Loss=0.6950557231903076 \t\t Validation: Acc=53%, Loss=0.6830127239227295\n",
            "Iteration: 339180 \t Train: Acc=54%, Loss=0.6897366642951965 \t\t Validation: Acc=50%, Loss=0.6952996253967285\n",
            "Iteration: 339190 \t Train: Acc=48%, Loss=0.6917145848274231 \t\t Validation: Acc=46%, Loss=0.6988869309425354\n",
            "Iteration: 339200 \t Train: Acc=51%, Loss=0.6870132684707642 \t\t Validation: Acc=46%, Loss=0.6929892301559448\n",
            "Iteration: 339210 \t Train: Acc=53%, Loss=0.6921403408050537 \t\t Validation: Acc=55%, Loss=0.6906852722167969\n",
            "Iteration: 339220 \t Train: Acc=48%, Loss=0.6988492012023926 \t\t Validation: Acc=44%, Loss=0.6979816555976868\n",
            "Iteration: 339230 \t Train: Acc=52%, Loss=0.6922009587287903 \t\t Validation: Acc=50%, Loss=0.6899452805519104\n",
            "Iteration: 339240 \t Train: Acc=53%, Loss=0.6873652338981628 \t\t Validation: Acc=54%, Loss=0.687729001045227\n",
            "Iteration: 339250 \t Train: Acc=55%, Loss=0.6895708441734314 \t\t Validation: Acc=51%, Loss=0.690265417098999\n",
            "Iteration: 339260 \t Train: Acc=49%, Loss=0.6988143920898438 \t\t Validation: Acc=49%, Loss=0.692493200302124\n",
            "Iteration: 339270 \t Train: Acc=56%, Loss=0.6933450102806091 \t\t Validation: Acc=55%, Loss=0.689591646194458\n",
            "Iteration: 339280 \t Train: Acc=51%, Loss=0.6891950368881226 \t\t Validation: Acc=49%, Loss=0.6903498768806458\n",
            "Iteration: 339290 \t Train: Acc=53%, Loss=0.6968457698822021 \t\t Validation: Acc=53%, Loss=0.6945968866348267\n",
            "Iteration: 339300 \t Train: Acc=53%, Loss=0.6924766898155212 \t\t Validation: Acc=49%, Loss=0.691448986530304\n",
            "Iteration: 339310 \t Train: Acc=47%, Loss=0.6936594247817993 \t\t Validation: Acc=49%, Loss=0.6946341395378113\n",
            "Iteration: 339320 \t Train: Acc=57%, Loss=0.6865078806877136 \t\t Validation: Acc=49%, Loss=0.6900807619094849\n",
            "Iteration: 339330 \t Train: Acc=50%, Loss=0.6915708780288696 \t\t Validation: Acc=55%, Loss=0.6886885166168213\n",
            "Iteration: 339340 \t Train: Acc=53%, Loss=0.6875944137573242 \t\t Validation: Acc=53%, Loss=0.6899769306182861\n",
            "Iteration: 339350 \t Train: Acc=46%, Loss=0.6939200162887573 \t\t Validation: Acc=41%, Loss=0.7003353238105774\n",
            "Iteration: 339360 \t Train: Acc=53%, Loss=0.6889700889587402 \t\t Validation: Acc=51%, Loss=0.6963385343551636\n",
            "Iteration: 339370 \t Train: Acc=50%, Loss=0.688774824142456 \t\t Validation: Acc=52%, Loss=0.6913433074951172\n",
            "Iteration: 339380 \t Train: Acc=53%, Loss=0.6885954141616821 \t\t Validation: Acc=53%, Loss=0.6875747442245483\n",
            "Iteration: 339390 \t Train: Acc=55%, Loss=0.6914293169975281 \t\t Validation: Acc=53%, Loss=0.692249059677124\n",
            "Iteration: 339400 \t Train: Acc=51%, Loss=0.6923366785049438 \t\t Validation: Acc=50%, Loss=0.691947877407074\n",
            "Iteration: 339410 \t Train: Acc=57%, Loss=0.6939767003059387 \t\t Validation: Acc=48%, Loss=0.6952444314956665\n",
            "Iteration: 339420 \t Train: Acc=49%, Loss=0.6976283192634583 \t\t Validation: Acc=50%, Loss=0.6905142664909363\n",
            "Iteration: 339430 \t Train: Acc=56%, Loss=0.6948593854904175 \t\t Validation: Acc=51%, Loss=0.6906961798667908\n",
            "Iteration: 339440 \t Train: Acc=57%, Loss=0.6846993565559387 \t\t Validation: Acc=54%, Loss=0.6905695199966431\n",
            "Iteration: 339450 \t Train: Acc=53%, Loss=0.6937270760536194 \t\t Validation: Acc=52%, Loss=0.6909807920455933\n",
            "Iteration: 339460 \t Train: Acc=48%, Loss=0.6921721696853638 \t\t Validation: Acc=48%, Loss=0.6937863230705261\n",
            "Iteration: 339470 \t Train: Acc=50%, Loss=0.6932118535041809 \t\t Validation: Acc=46%, Loss=0.6934394836425781\n",
            "Iteration: 339480 \t Train: Acc=52%, Loss=0.6872246861457825 \t\t Validation: Acc=55%, Loss=0.6887781620025635\n",
            "Iteration: 339490 \t Train: Acc=45%, Loss=0.6996386647224426 \t\t Validation: Acc=49%, Loss=0.6909997463226318\n",
            "Iteration: 339500 \t Train: Acc=53%, Loss=0.6932457685470581 \t\t Validation: Acc=50%, Loss=0.693613588809967\n",
            "Iteration: 339510 \t Train: Acc=54%, Loss=0.6933797001838684 \t\t Validation: Acc=56%, Loss=0.6910293102264404\n",
            "Iteration: 339520 \t Train: Acc=50%, Loss=0.6953757405281067 \t\t Validation: Acc=51%, Loss=0.6929739117622375\n",
            "Iteration: 339530 \t Train: Acc=50%, Loss=0.6909012794494629 \t\t Validation: Acc=52%, Loss=0.6925052404403687\n",
            "Iteration: 339540 \t Train: Acc=48%, Loss=0.6960902810096741 \t\t Validation: Acc=46%, Loss=0.696344792842865\n",
            "Iteration: 339550 \t Train: Acc=51%, Loss=0.6897822618484497 \t\t Validation: Acc=53%, Loss=0.6913757920265198\n",
            "Iteration: 339560 \t Train: Acc=55%, Loss=0.6792870759963989 \t\t Validation: Acc=50%, Loss=0.6930561065673828\n",
            "Iteration: 339570 \t Train: Acc=53%, Loss=0.693446159362793 \t\t Validation: Acc=50%, Loss=0.6920365691184998\n",
            "Iteration: 339580 \t Train: Acc=48%, Loss=0.6962605714797974 \t\t Validation: Acc=52%, Loss=0.6930619478225708\n",
            "Iteration: 339590 \t Train: Acc=48%, Loss=0.684892475605011 \t\t Validation: Acc=52%, Loss=0.6901453137397766\n",
            "Iteration: 339600 \t Train: Acc=53%, Loss=0.6888424754142761 \t\t Validation: Acc=48%, Loss=0.6941042542457581\n",
            "Iteration: 339610 \t Train: Acc=46%, Loss=0.6937562227249146 \t\t Validation: Acc=48%, Loss=0.6962903141975403\n",
            "Iteration: 339620 \t Train: Acc=49%, Loss=0.699212908744812 \t\t Validation: Acc=47%, Loss=0.6926748156547546\n",
            "Iteration: 339630 \t Train: Acc=50%, Loss=0.6883898377418518 \t\t Validation: Acc=52%, Loss=0.6901120543479919\n",
            "Iteration: 339640 \t Train: Acc=50%, Loss=0.6942547559738159 \t\t Validation: Acc=50%, Loss=0.692283034324646\n",
            "Iteration: 339650 \t Train: Acc=53%, Loss=0.6851177215576172 \t\t Validation: Acc=50%, Loss=0.6899861693382263\n",
            "Iteration: 339660 \t Train: Acc=54%, Loss=0.6860129833221436 \t\t Validation: Acc=53%, Loss=0.690613865852356\n",
            "Iteration: 339670 \t Train: Acc=46%, Loss=0.6943491697311401 \t\t Validation: Acc=49%, Loss=0.6922712326049805\n",
            "Iteration: 339680 \t Train: Acc=50%, Loss=0.6929117441177368 \t\t Validation: Acc=48%, Loss=0.6951758861541748\n",
            "Iteration: 339690 \t Train: Acc=50%, Loss=0.696919858455658 \t\t Validation: Acc=50%, Loss=0.6905556917190552\n",
            "Iteration: 339700 \t Train: Acc=52%, Loss=0.6890056729316711 \t\t Validation: Acc=52%, Loss=0.691303551197052\n",
            "Iteration: 339710 \t Train: Acc=51%, Loss=0.6976877450942993 \t\t Validation: Acc=51%, Loss=0.690252423286438\n",
            "Iteration: 339720 \t Train: Acc=48%, Loss=0.693374514579773 \t\t Validation: Acc=49%, Loss=0.693712055683136\n",
            "Iteration: 339730 \t Train: Acc=50%, Loss=0.6883929967880249 \t\t Validation: Acc=50%, Loss=0.6917746067047119\n",
            "Iteration: 339740 \t Train: Acc=53%, Loss=0.6950547695159912 \t\t Validation: Acc=48%, Loss=0.6954507827758789\n",
            "Iteration: 339750 \t Train: Acc=52%, Loss=0.6896207332611084 \t\t Validation: Acc=47%, Loss=0.6951261758804321\n",
            "Iteration: 339760 \t Train: Acc=46%, Loss=0.6926023960113525 \t\t Validation: Acc=53%, Loss=0.6892080307006836\n",
            "Iteration: 339770 \t Train: Acc=48%, Loss=0.6870752573013306 \t\t Validation: Acc=48%, Loss=0.6958678960800171\n",
            "Iteration: 339780 \t Train: Acc=57%, Loss=0.6859044432640076 \t\t Validation: Acc=46%, Loss=0.6953111886978149\n",
            "Iteration: 339790 \t Train: Acc=50%, Loss=0.6878805160522461 \t\t Validation: Acc=50%, Loss=0.6954531073570251\n",
            "Iteration: 339800 \t Train: Acc=50%, Loss=0.6906116008758545 \t\t Validation: Acc=52%, Loss=0.6986294984817505\n",
            "Iteration: 339810 \t Train: Acc=50%, Loss=0.690550684928894 \t\t Validation: Acc=47%, Loss=0.6919913291931152\n",
            "Iteration: 339820 \t Train: Acc=52%, Loss=0.6877164244651794 \t\t Validation: Acc=50%, Loss=0.6904447078704834\n",
            "Iteration: 339830 \t Train: Acc=50%, Loss=0.6919113397598267 \t\t Validation: Acc=46%, Loss=0.6962153315544128\n",
            "Iteration: 339840 \t Train: Acc=55%, Loss=0.6821646094322205 \t\t Validation: Acc=53%, Loss=0.6894927620887756\n",
            "Iteration: 339850 \t Train: Acc=51%, Loss=0.6964502334594727 \t\t Validation: Acc=52%, Loss=0.6886565089225769\n",
            "Iteration: 339860 \t Train: Acc=54%, Loss=0.6907825469970703 \t\t Validation: Acc=49%, Loss=0.6920075416564941\n",
            "Iteration: 339870 \t Train: Acc=53%, Loss=0.6882542371749878 \t\t Validation: Acc=50%, Loss=0.6967655420303345\n",
            "Iteration: 339880 \t Train: Acc=53%, Loss=0.6855518817901611 \t\t Validation: Acc=55%, Loss=0.6913340091705322\n",
            "Iteration: 339890 \t Train: Acc=48%, Loss=0.6922311186790466 \t\t Validation: Acc=53%, Loss=0.6891839504241943\n",
            "Iteration: 339900 \t Train: Acc=47%, Loss=0.6900767087936401 \t\t Validation: Acc=49%, Loss=0.6954556107521057\n",
            "Iteration: 339910 \t Train: Acc=46%, Loss=0.6884656548500061 \t\t Validation: Acc=50%, Loss=0.6962897777557373\n",
            "Iteration: 339920 \t Train: Acc=50%, Loss=0.6908895969390869 \t\t Validation: Acc=50%, Loss=0.6913253664970398\n",
            "Iteration: 339930 \t Train: Acc=55%, Loss=0.6917218565940857 \t\t Validation: Acc=50%, Loss=0.6934725642204285\n",
            "Iteration: 339940 \t Train: Acc=45%, Loss=0.7049661874771118 \t\t Validation: Acc=47%, Loss=0.7044210433959961\n",
            "Iteration: 339950 \t Train: Acc=46%, Loss=0.6879980564117432 \t\t Validation: Acc=45%, Loss=0.6972554922103882\n",
            "Iteration: 339960 \t Train: Acc=50%, Loss=0.6901795864105225 \t\t Validation: Acc=48%, Loss=0.6955057382583618\n",
            "Iteration: 339970 \t Train: Acc=52%, Loss=0.6898878812789917 \t\t Validation: Acc=49%, Loss=0.686134397983551\n",
            "Iteration: 339980 \t Train: Acc=50%, Loss=0.6892461776733398 \t\t Validation: Acc=53%, Loss=0.6917737126350403\n",
            "Iteration: 339990 \t Train: Acc=50%, Loss=0.700598418712616 \t\t Validation: Acc=52%, Loss=0.6938160061836243\n",
            "Iteration: 340000 \t Train: Acc=46%, Loss=0.6922302842140198 \t\t Validation: Acc=46%, Loss=0.6951961517333984\n",
            "Iteration: 340010 \t Train: Acc=49%, Loss=0.6873512268066406 \t\t Validation: Acc=49%, Loss=0.6902797222137451\n",
            "Iteration: 340020 \t Train: Acc=48%, Loss=0.697661817073822 \t\t Validation: Acc=49%, Loss=0.693684995174408\n",
            "Iteration: 340030 \t Train: Acc=51%, Loss=0.6888309717178345 \t\t Validation: Acc=50%, Loss=0.6958556771278381\n",
            "Iteration: 340040 \t Train: Acc=49%, Loss=0.690054714679718 \t\t Validation: Acc=50%, Loss=0.6928226947784424\n",
            "Iteration: 340050 \t Train: Acc=47%, Loss=0.692905068397522 \t\t Validation: Acc=48%, Loss=0.690034806728363\n",
            "Iteration: 340060 \t Train: Acc=55%, Loss=0.689159631729126 \t\t Validation: Acc=50%, Loss=0.691805362701416\n",
            "Iteration: 340070 \t Train: Acc=50%, Loss=0.6891553401947021 \t\t Validation: Acc=52%, Loss=0.6944683194160461\n",
            "Iteration: 340080 \t Train: Acc=54%, Loss=0.6892940402030945 \t\t Validation: Acc=50%, Loss=0.6934804916381836\n",
            "Iteration: 340090 \t Train: Acc=52%, Loss=0.6901612281799316 \t\t Validation: Acc=47%, Loss=0.6971569061279297\n",
            "Iteration: 340100 \t Train: Acc=46%, Loss=0.6992775201797485 \t\t Validation: Acc=48%, Loss=0.6943283081054688\n",
            "Iteration: 340110 \t Train: Acc=51%, Loss=0.6922765374183655 \t\t Validation: Acc=49%, Loss=0.6930526494979858\n",
            "Iteration: 340120 \t Train: Acc=46%, Loss=0.6896068453788757 \t\t Validation: Acc=52%, Loss=0.6877579092979431\n",
            "Iteration: 340130 \t Train: Acc=53%, Loss=0.688326895236969 \t\t Validation: Acc=55%, Loss=0.6829971075057983\n",
            "Iteration: 340140 \t Train: Acc=53%, Loss=0.6888816952705383 \t\t Validation: Acc=50%, Loss=0.6930168867111206\n",
            "Iteration: 340150 \t Train: Acc=50%, Loss=0.6909105777740479 \t\t Validation: Acc=50%, Loss=0.693162739276886\n",
            "Iteration: 340160 \t Train: Acc=55%, Loss=0.6921322345733643 \t\t Validation: Acc=53%, Loss=0.6922840476036072\n",
            "Iteration: 340170 \t Train: Acc=50%, Loss=0.6952908635139465 \t\t Validation: Acc=52%, Loss=0.6935873031616211\n",
            "Iteration: 340180 \t Train: Acc=51%, Loss=0.7011170983314514 \t\t Validation: Acc=50%, Loss=0.692145586013794\n",
            "Iteration: 340190 \t Train: Acc=48%, Loss=0.6920835375785828 \t\t Validation: Acc=50%, Loss=0.6906017065048218\n",
            "Iteration: 340200 \t Train: Acc=53%, Loss=0.6894453763961792 \t\t Validation: Acc=50%, Loss=0.6870184540748596\n",
            "Iteration: 340210 \t Train: Acc=47%, Loss=0.6952990293502808 \t\t Validation: Acc=50%, Loss=0.694177508354187\n",
            "Iteration: 340220 \t Train: Acc=56%, Loss=0.6893235445022583 \t\t Validation: Acc=46%, Loss=0.6959038972854614\n",
            "Iteration: 340230 \t Train: Acc=50%, Loss=0.6894311308860779 \t\t Validation: Acc=46%, Loss=0.6984429955482483\n",
            "Iteration: 340240 \t Train: Acc=50%, Loss=0.6984654068946838 \t\t Validation: Acc=52%, Loss=0.6891356110572815\n",
            "Iteration: 340250 \t Train: Acc=50%, Loss=0.6959181427955627 \t\t Validation: Acc=53%, Loss=0.6878668069839478\n",
            "Iteration: 340260 \t Train: Acc=48%, Loss=0.6928138732910156 \t\t Validation: Acc=54%, Loss=0.6895251274108887\n",
            "Iteration: 340270 \t Train: Acc=51%, Loss=0.693276584148407 \t\t Validation: Acc=50%, Loss=0.6902947425842285\n",
            "Iteration: 340280 \t Train: Acc=53%, Loss=0.6908290982246399 \t\t Validation: Acc=52%, Loss=0.6924800872802734\n",
            "Iteration: 340290 \t Train: Acc=53%, Loss=0.69019615650177 \t\t Validation: Acc=51%, Loss=0.6893054842948914\n",
            "Iteration: 340300 \t Train: Acc=50%, Loss=0.6916153430938721 \t\t Validation: Acc=48%, Loss=0.6965397000312805\n",
            "Iteration: 340310 \t Train: Acc=53%, Loss=0.6907699704170227 \t\t Validation: Acc=50%, Loss=0.694461464881897\n",
            "Iteration: 340320 \t Train: Acc=49%, Loss=0.6943377256393433 \t\t Validation: Acc=50%, Loss=0.6922397613525391\n",
            "Iteration: 340330 \t Train: Acc=51%, Loss=0.6932730674743652 \t\t Validation: Acc=53%, Loss=0.6899958848953247\n",
            "Iteration: 340340 \t Train: Acc=48%, Loss=0.6922023892402649 \t\t Validation: Acc=46%, Loss=0.6948038339614868\n",
            "Iteration: 340350 \t Train: Acc=48%, Loss=0.691709041595459 \t\t Validation: Acc=54%, Loss=0.6889023184776306\n",
            "Iteration: 340360 \t Train: Acc=52%, Loss=0.6940109729766846 \t\t Validation: Acc=53%, Loss=0.6936222314834595\n",
            "Iteration: 340370 \t Train: Acc=55%, Loss=0.6819596290588379 \t\t Validation: Acc=49%, Loss=0.69239342212677\n",
            "Iteration: 340380 \t Train: Acc=53%, Loss=0.6936181783676147 \t\t Validation: Acc=51%, Loss=0.6911118030548096\n",
            "Iteration: 340390 \t Train: Acc=57%, Loss=0.6859384775161743 \t\t Validation: Acc=50%, Loss=0.6945396661758423\n",
            "Iteration: 340400 \t Train: Acc=52%, Loss=0.6877347826957703 \t\t Validation: Acc=55%, Loss=0.6870654821395874\n",
            "Iteration: 340410 \t Train: Acc=57%, Loss=0.6865701675415039 \t\t Validation: Acc=52%, Loss=0.6920161247253418\n",
            "Iteration: 340420 \t Train: Acc=50%, Loss=0.6901767253875732 \t\t Validation: Acc=56%, Loss=0.684878408908844\n",
            "Iteration: 340430 \t Train: Acc=47%, Loss=0.6926548480987549 \t\t Validation: Acc=50%, Loss=0.6941958665847778\n",
            "Iteration: 340440 \t Train: Acc=44%, Loss=0.6996803283691406 \t\t Validation: Acc=48%, Loss=0.6905154585838318\n",
            "Iteration: 340450 \t Train: Acc=56%, Loss=0.6870806217193604 \t\t Validation: Acc=57%, Loss=0.6865208148956299\n",
            "Iteration: 340460 \t Train: Acc=53%, Loss=0.6838963031768799 \t\t Validation: Acc=51%, Loss=0.6941774487495422\n",
            "Iteration: 340470 \t Train: Acc=47%, Loss=0.6971641778945923 \t\t Validation: Acc=45%, Loss=0.6900954246520996\n",
            "Iteration: 340480 \t Train: Acc=50%, Loss=0.6974060535430908 \t\t Validation: Acc=49%, Loss=0.6949233412742615\n",
            "Iteration: 340490 \t Train: Acc=54%, Loss=0.6873064637184143 \t\t Validation: Acc=49%, Loss=0.6900134086608887\n",
            "Iteration: 340500 \t Train: Acc=53%, Loss=0.6917865872383118 \t\t Validation: Acc=50%, Loss=0.6909381747245789\n",
            "Iteration: 340510 \t Train: Acc=44%, Loss=0.6998557448387146 \t\t Validation: Acc=53%, Loss=0.6936788558959961\n",
            "Iteration: 340520 \t Train: Acc=50%, Loss=0.6912509202957153 \t\t Validation: Acc=54%, Loss=0.6899960041046143\n",
            "Iteration: 340530 \t Train: Acc=49%, Loss=0.6956666111946106 \t\t Validation: Acc=50%, Loss=0.6917072534561157\n",
            "Iteration: 340540 \t Train: Acc=51%, Loss=0.6878761053085327 \t\t Validation: Acc=53%, Loss=0.6914998888969421\n",
            "Iteration: 340550 \t Train: Acc=62%, Loss=0.6831716299057007 \t\t Validation: Acc=50%, Loss=0.6954061985015869\n",
            "Iteration: 340560 \t Train: Acc=52%, Loss=0.6909302473068237 \t\t Validation: Acc=54%, Loss=0.6904057860374451\n",
            "Iteration: 340570 \t Train: Acc=50%, Loss=0.6925677061080933 \t\t Validation: Acc=50%, Loss=0.6874189972877502\n",
            "Iteration: 340580 \t Train: Acc=45%, Loss=0.6932379007339478 \t\t Validation: Acc=49%, Loss=0.6933686137199402\n",
            "Iteration: 340590 \t Train: Acc=53%, Loss=0.6898655891418457 \t\t Validation: Acc=53%, Loss=0.6924718022346497\n",
            "Iteration: 340600 \t Train: Acc=53%, Loss=0.6843976974487305 \t\t Validation: Acc=55%, Loss=0.6915032863616943\n",
            "Iteration: 340610 \t Train: Acc=50%, Loss=0.6978479623794556 \t\t Validation: Acc=53%, Loss=0.6903669834136963\n",
            "Iteration: 340620 \t Train: Acc=56%, Loss=0.6911802887916565 \t\t Validation: Acc=53%, Loss=0.689760684967041\n",
            "Iteration: 340630 \t Train: Acc=48%, Loss=0.690947949886322 \t\t Validation: Acc=50%, Loss=0.6921030282974243\n",
            "Iteration: 340640 \t Train: Acc=45%, Loss=0.6957842111587524 \t\t Validation: Acc=48%, Loss=0.6934296488761902\n",
            "Iteration: 340650 \t Train: Acc=56%, Loss=0.6890008449554443 \t\t Validation: Acc=53%, Loss=0.6876081228256226\n",
            "Iteration: 340660 \t Train: Acc=48%, Loss=0.6925601959228516 \t\t Validation: Acc=57%, Loss=0.6857849359512329\n",
            "Iteration: 340670 \t Train: Acc=52%, Loss=0.6910787224769592 \t\t Validation: Acc=50%, Loss=0.6938234567642212\n",
            "Iteration: 340680 \t Train: Acc=53%, Loss=0.6910493969917297 \t\t Validation: Acc=50%, Loss=0.6951234340667725\n",
            "Iteration: 340690 \t Train: Acc=50%, Loss=0.6949568390846252 \t\t Validation: Acc=50%, Loss=0.6941224336624146\n",
            "Iteration: 340700 \t Train: Acc=53%, Loss=0.6924319267272949 \t\t Validation: Acc=53%, Loss=0.6923201084136963\n",
            "Iteration: 340710 \t Train: Acc=45%, Loss=0.6918195486068726 \t\t Validation: Acc=54%, Loss=0.6855713725090027\n",
            "Iteration: 340720 \t Train: Acc=50%, Loss=0.6877132654190063 \t\t Validation: Acc=51%, Loss=0.6921412944793701\n",
            "Iteration: 340730 \t Train: Acc=48%, Loss=0.6976920366287231 \t\t Validation: Acc=52%, Loss=0.698512852191925\n",
            "Iteration: 340740 \t Train: Acc=48%, Loss=0.6920280456542969 \t\t Validation: Acc=53%, Loss=0.6846609115600586\n",
            "Iteration: 340750 \t Train: Acc=46%, Loss=0.6906228065490723 \t\t Validation: Acc=47%, Loss=0.6885434985160828\n",
            "Iteration: 340760 \t Train: Acc=50%, Loss=0.6958309412002563 \t\t Validation: Acc=52%, Loss=0.6879518032073975\n",
            "Iteration: 340770 \t Train: Acc=50%, Loss=0.6933857202529907 \t\t Validation: Acc=48%, Loss=0.6980583071708679\n",
            "Iteration: 340780 \t Train: Acc=51%, Loss=0.6886019110679626 \t\t Validation: Acc=46%, Loss=0.6916271448135376\n",
            "Iteration: 340790 \t Train: Acc=51%, Loss=0.6900998950004578 \t\t Validation: Acc=48%, Loss=0.692646861076355\n",
            "Iteration: 340800 \t Train: Acc=55%, Loss=0.6903222799301147 \t\t Validation: Acc=51%, Loss=0.6871682405471802\n",
            "Iteration: 340810 \t Train: Acc=46%, Loss=0.6973908543586731 \t\t Validation: Acc=55%, Loss=0.6861788034439087\n",
            "Iteration: 340820 \t Train: Acc=55%, Loss=0.6883794665336609 \t\t Validation: Acc=53%, Loss=0.6900283694267273\n",
            "Iteration: 340830 \t Train: Acc=53%, Loss=0.6813285946846008 \t\t Validation: Acc=50%, Loss=0.6896980404853821\n",
            "Iteration: 340840 \t Train: Acc=50%, Loss=0.6939842104911804 \t\t Validation: Acc=48%, Loss=0.6919036507606506\n",
            "Iteration: 340850 \t Train: Acc=50%, Loss=0.6997863054275513 \t\t Validation: Acc=49%, Loss=0.6967297792434692\n",
            "Iteration: 340860 \t Train: Acc=50%, Loss=0.6895647048950195 \t\t Validation: Acc=50%, Loss=0.6945517659187317\n",
            "Iteration: 340870 \t Train: Acc=49%, Loss=0.6993480920791626 \t\t Validation: Acc=57%, Loss=0.6931540966033936\n",
            "Iteration: 340880 \t Train: Acc=47%, Loss=0.6925649642944336 \t\t Validation: Acc=51%, Loss=0.6917127966880798\n",
            "Iteration: 340890 \t Train: Acc=46%, Loss=0.700443685054779 \t\t Validation: Acc=51%, Loss=0.6939963102340698\n",
            "Iteration: 340900 \t Train: Acc=48%, Loss=0.6933042407035828 \t\t Validation: Acc=53%, Loss=0.6890038847923279\n",
            "Iteration: 340910 \t Train: Acc=48%, Loss=0.6965203881263733 \t\t Validation: Acc=45%, Loss=0.6940356492996216\n",
            "Iteration: 340920 \t Train: Acc=53%, Loss=0.6940548419952393 \t\t Validation: Acc=50%, Loss=0.6949903964996338\n",
            "Iteration: 340930 \t Train: Acc=50%, Loss=0.6951952576637268 \t\t Validation: Acc=51%, Loss=0.6918150186538696\n",
            "Iteration: 340940 \t Train: Acc=57%, Loss=0.6912967562675476 \t\t Validation: Acc=50%, Loss=0.6946693658828735\n",
            "Iteration: 340950 \t Train: Acc=51%, Loss=0.6930180788040161 \t\t Validation: Acc=53%, Loss=0.6912921667098999\n",
            "Iteration: 340960 \t Train: Acc=45%, Loss=0.6937209963798523 \t\t Validation: Acc=50%, Loss=0.692510724067688\n",
            "Iteration: 340970 \t Train: Acc=56%, Loss=0.6864492297172546 \t\t Validation: Acc=52%, Loss=0.6936821937561035\n",
            "Iteration: 340980 \t Train: Acc=46%, Loss=0.6995245218276978 \t\t Validation: Acc=52%, Loss=0.6936703324317932\n",
            "Iteration: 340990 \t Train: Acc=51%, Loss=0.6904781460762024 \t\t Validation: Acc=50%, Loss=0.6960796117782593\n",
            "Iteration: 341000 \t Train: Acc=50%, Loss=0.6951270699501038 \t\t Validation: Acc=56%, Loss=0.6863953471183777\n",
            "Iteration: 341010 \t Train: Acc=58%, Loss=0.6881961822509766 \t\t Validation: Acc=46%, Loss=0.7002226114273071\n",
            "Iteration: 341020 \t Train: Acc=58%, Loss=0.6844635605812073 \t\t Validation: Acc=49%, Loss=0.6923327445983887\n",
            "Iteration: 341030 \t Train: Acc=46%, Loss=0.6968990564346313 \t\t Validation: Acc=49%, Loss=0.6962441802024841\n",
            "Iteration: 341040 \t Train: Acc=48%, Loss=0.6903679370880127 \t\t Validation: Acc=57%, Loss=0.6847564578056335\n",
            "Iteration: 341050 \t Train: Acc=51%, Loss=0.6931564211845398 \t\t Validation: Acc=52%, Loss=0.6895197033882141\n",
            "Iteration: 341060 \t Train: Acc=50%, Loss=0.693894624710083 \t\t Validation: Acc=47%, Loss=0.6967523694038391\n",
            "Iteration: 341070 \t Train: Acc=56%, Loss=0.686913251876831 \t\t Validation: Acc=53%, Loss=0.6896532773971558\n",
            "Iteration: 341080 \t Train: Acc=49%, Loss=0.6882452964782715 \t\t Validation: Acc=46%, Loss=0.6973949670791626\n",
            "Iteration: 341090 \t Train: Acc=54%, Loss=0.6914312243461609 \t\t Validation: Acc=51%, Loss=0.6869755387306213\n",
            "Iteration: 341100 \t Train: Acc=50%, Loss=0.6952570676803589 \t\t Validation: Acc=56%, Loss=0.6910511255264282\n",
            "Iteration: 341110 \t Train: Acc=44%, Loss=0.6964502334594727 \t\t Validation: Acc=50%, Loss=0.6928224563598633\n",
            "Iteration: 341120 \t Train: Acc=57%, Loss=0.6799480319023132 \t\t Validation: Acc=49%, Loss=0.6911063194274902\n",
            "Iteration: 341130 \t Train: Acc=51%, Loss=0.6903932690620422 \t\t Validation: Acc=50%, Loss=0.6967235803604126\n",
            "Iteration: 341140 \t Train: Acc=49%, Loss=0.6933732032775879 \t\t Validation: Acc=50%, Loss=0.6931817531585693\n",
            "Iteration: 341150 \t Train: Acc=47%, Loss=0.6935775279998779 \t\t Validation: Acc=48%, Loss=0.6933003067970276\n",
            "Iteration: 341160 \t Train: Acc=51%, Loss=0.6871116757392883 \t\t Validation: Acc=52%, Loss=0.6915183067321777\n",
            "Iteration: 341170 \t Train: Acc=50%, Loss=0.6949470639228821 \t\t Validation: Acc=48%, Loss=0.6980507969856262\n",
            "Iteration: 341180 \t Train: Acc=51%, Loss=0.698255181312561 \t\t Validation: Acc=47%, Loss=0.6940531134605408\n",
            "Iteration: 341190 \t Train: Acc=49%, Loss=0.6896591782569885 \t\t Validation: Acc=52%, Loss=0.69277423620224\n",
            "Iteration: 341200 \t Train: Acc=50%, Loss=0.7031155824661255 \t\t Validation: Acc=52%, Loss=0.6881067752838135\n",
            "Iteration: 341210 \t Train: Acc=46%, Loss=0.6976385116577148 \t\t Validation: Acc=52%, Loss=0.690177857875824\n",
            "Iteration: 341220 \t Train: Acc=52%, Loss=0.6889055371284485 \t\t Validation: Acc=46%, Loss=0.6926658153533936\n",
            "Iteration: 341230 \t Train: Acc=53%, Loss=0.6920640468597412 \t\t Validation: Acc=47%, Loss=0.698850154876709\n",
            "Iteration: 341240 \t Train: Acc=53%, Loss=0.6923948526382446 \t\t Validation: Acc=53%, Loss=0.6885124444961548\n",
            "Iteration: 341250 \t Train: Acc=48%, Loss=0.6953507661819458 \t\t Validation: Acc=51%, Loss=0.6902564764022827\n",
            "Iteration: 341260 \t Train: Acc=52%, Loss=0.6847455501556396 \t\t Validation: Acc=52%, Loss=0.6921857595443726\n",
            "Iteration: 341270 \t Train: Acc=51%, Loss=0.6902930736541748 \t\t Validation: Acc=50%, Loss=0.6921738386154175\n",
            "Iteration: 341280 \t Train: Acc=53%, Loss=0.685577929019928 \t\t Validation: Acc=55%, Loss=0.6896169781684875\n",
            "Iteration: 341290 \t Train: Acc=50%, Loss=0.6922306418418884 \t\t Validation: Acc=54%, Loss=0.6886171698570251\n",
            "Iteration: 341300 \t Train: Acc=52%, Loss=0.6867766380310059 \t\t Validation: Acc=50%, Loss=0.692591667175293\n",
            "Iteration: 341310 \t Train: Acc=53%, Loss=0.6903280019760132 \t\t Validation: Acc=52%, Loss=0.6937764883041382\n",
            "Iteration: 341320 \t Train: Acc=49%, Loss=0.6956568360328674 \t\t Validation: Acc=49%, Loss=0.6963950991630554\n",
            "Iteration: 341330 \t Train: Acc=50%, Loss=0.7025793790817261 \t\t Validation: Acc=50%, Loss=0.6899149417877197\n",
            "Iteration: 341340 \t Train: Acc=46%, Loss=0.6958605647087097 \t\t Validation: Acc=50%, Loss=0.6943473815917969\n",
            "Iteration: 341350 \t Train: Acc=56%, Loss=0.6890224814414978 \t\t Validation: Acc=54%, Loss=0.6871281862258911\n",
            "Iteration: 341360 \t Train: Acc=47%, Loss=0.6962675452232361 \t\t Validation: Acc=51%, Loss=0.6954264044761658\n",
            "Iteration: 341370 \t Train: Acc=50%, Loss=0.6918236017227173 \t\t Validation: Acc=45%, Loss=0.6921084523200989\n",
            "Iteration: 341380 \t Train: Acc=47%, Loss=0.6947768330574036 \t\t Validation: Acc=44%, Loss=0.690721333026886\n",
            "Iteration: 341390 \t Train: Acc=50%, Loss=0.6952897310256958 \t\t Validation: Acc=51%, Loss=0.6925932168960571\n",
            "Iteration: 341400 \t Train: Acc=50%, Loss=0.6963382363319397 \t\t Validation: Acc=56%, Loss=0.6918492317199707\n",
            "Iteration: 341410 \t Train: Acc=48%, Loss=0.6914453506469727 \t\t Validation: Acc=52%, Loss=0.6927635669708252\n",
            "Iteration: 341420 \t Train: Acc=53%, Loss=0.6913790702819824 \t\t Validation: Acc=51%, Loss=0.6929454207420349\n",
            "Iteration: 341430 \t Train: Acc=61%, Loss=0.6847347617149353 \t\t Validation: Acc=49%, Loss=0.6937079429626465\n",
            "Iteration: 341440 \t Train: Acc=53%, Loss=0.6921065449714661 \t\t Validation: Acc=53%, Loss=0.6927173733711243\n",
            "Iteration: 341450 \t Train: Acc=55%, Loss=0.6844567656517029 \t\t Validation: Acc=51%, Loss=0.6947100162506104\n",
            "Iteration: 341460 \t Train: Acc=46%, Loss=0.6911880373954773 \t\t Validation: Acc=51%, Loss=0.6919508576393127\n",
            "Iteration: 341470 \t Train: Acc=50%, Loss=0.6879773736000061 \t\t Validation: Acc=53%, Loss=0.6920744776725769\n",
            "Iteration: 341480 \t Train: Acc=50%, Loss=0.6977275609970093 \t\t Validation: Acc=53%, Loss=0.6923469305038452\n",
            "Iteration: 341490 \t Train: Acc=51%, Loss=0.6963911652565002 \t\t Validation: Acc=49%, Loss=0.6911048889160156\n",
            "Iteration: 341500 \t Train: Acc=53%, Loss=0.6896967887878418 \t\t Validation: Acc=53%, Loss=0.6928563117980957\n",
            "Iteration: 341510 \t Train: Acc=51%, Loss=0.6925041675567627 \t\t Validation: Acc=51%, Loss=0.6882103085517883\n",
            "Iteration: 341520 \t Train: Acc=50%, Loss=0.6935643553733826 \t\t Validation: Acc=53%, Loss=0.6915099620819092\n",
            "Iteration: 341530 \t Train: Acc=50%, Loss=0.696762204170227 \t\t Validation: Acc=46%, Loss=0.6946110129356384\n",
            "Iteration: 341540 \t Train: Acc=51%, Loss=0.6943296194076538 \t\t Validation: Acc=52%, Loss=0.6890268325805664\n",
            "Iteration: 341550 \t Train: Acc=48%, Loss=0.6916003823280334 \t\t Validation: Acc=50%, Loss=0.6952011585235596\n",
            "Iteration: 341560 \t Train: Acc=59%, Loss=0.6842852830886841 \t\t Validation: Acc=48%, Loss=0.695402979850769\n",
            "Iteration: 341570 \t Train: Acc=50%, Loss=0.6912654042243958 \t\t Validation: Acc=54%, Loss=0.6915631294250488\n",
            "Iteration: 341580 \t Train: Acc=49%, Loss=0.694625973701477 \t\t Validation: Acc=49%, Loss=0.697688102722168\n",
            "Iteration: 341590 \t Train: Acc=53%, Loss=0.6939565539360046 \t\t Validation: Acc=49%, Loss=0.6936987638473511\n",
            "Iteration: 341600 \t Train: Acc=46%, Loss=0.6999680995941162 \t\t Validation: Acc=44%, Loss=0.6951431632041931\n",
            "Iteration: 341610 \t Train: Acc=53%, Loss=0.6869843006134033 \t\t Validation: Acc=49%, Loss=0.6917389035224915\n",
            "Iteration: 341620 \t Train: Acc=48%, Loss=0.6933484673500061 \t\t Validation: Acc=53%, Loss=0.6942526698112488\n",
            "Iteration: 341630 \t Train: Acc=48%, Loss=0.6994543075561523 \t\t Validation: Acc=50%, Loss=0.6890496015548706\n",
            "Iteration: 341640 \t Train: Acc=49%, Loss=0.6943793296813965 \t\t Validation: Acc=60%, Loss=0.691403865814209\n",
            "Iteration: 341650 \t Train: Acc=51%, Loss=0.691993236541748 \t\t Validation: Acc=53%, Loss=0.685345470905304\n",
            "Iteration: 341660 \t Train: Acc=55%, Loss=0.6854758858680725 \t\t Validation: Acc=54%, Loss=0.6894738674163818\n",
            "Iteration: 341670 \t Train: Acc=47%, Loss=0.6899222731590271 \t\t Validation: Acc=50%, Loss=0.7039110660552979\n",
            "Iteration: 341680 \t Train: Acc=52%, Loss=0.6880435943603516 \t\t Validation: Acc=53%, Loss=0.691291093826294\n",
            "Iteration: 341690 \t Train: Acc=52%, Loss=0.686127781867981 \t\t Validation: Acc=53%, Loss=0.6927666068077087\n",
            "Iteration: 341700 \t Train: Acc=52%, Loss=0.6799310445785522 \t\t Validation: Acc=50%, Loss=0.6892094612121582\n",
            "Iteration: 341710 \t Train: Acc=46%, Loss=0.6958081722259521 \t\t Validation: Acc=50%, Loss=0.6911171674728394\n",
            "Iteration: 341720 \t Train: Acc=54%, Loss=0.6911218762397766 \t\t Validation: Acc=53%, Loss=0.6901667714118958\n",
            "Iteration: 341730 \t Train: Acc=50%, Loss=0.6971016526222229 \t\t Validation: Acc=51%, Loss=0.6927058100700378\n",
            "Iteration: 341740 \t Train: Acc=56%, Loss=0.6889676451683044 \t\t Validation: Acc=50%, Loss=0.6971650123596191\n",
            "Iteration: 341750 \t Train: Acc=50%, Loss=0.6926606893539429 \t\t Validation: Acc=51%, Loss=0.687824547290802\n",
            "Iteration: 341760 \t Train: Acc=48%, Loss=0.6899195909500122 \t\t Validation: Acc=50%, Loss=0.6926337480545044\n",
            "Iteration: 341770 \t Train: Acc=53%, Loss=0.6884759068489075 \t\t Validation: Acc=54%, Loss=0.6875249147415161\n",
            "Iteration: 341780 \t Train: Acc=50%, Loss=0.6966124773025513 \t\t Validation: Acc=46%, Loss=0.6962524056434631\n",
            "Iteration: 341790 \t Train: Acc=52%, Loss=0.6917513012886047 \t\t Validation: Acc=47%, Loss=0.6957187056541443\n",
            "Iteration: 341800 \t Train: Acc=50%, Loss=0.6931397914886475 \t\t Validation: Acc=53%, Loss=0.6942949295043945\n",
            "Iteration: 341810 \t Train: Acc=46%, Loss=0.6967704892158508 \t\t Validation: Acc=54%, Loss=0.693159282207489\n",
            "Iteration: 341820 \t Train: Acc=46%, Loss=0.7001363635063171 \t\t Validation: Acc=51%, Loss=0.6946215629577637\n",
            "Iteration: 341830 \t Train: Acc=54%, Loss=0.6813930869102478 \t\t Validation: Acc=53%, Loss=0.6952840089797974\n",
            "Iteration: 341840 \t Train: Acc=48%, Loss=0.6989197731018066 \t\t Validation: Acc=50%, Loss=0.6873409152030945\n",
            "Iteration: 341850 \t Train: Acc=56%, Loss=0.687833309173584 \t\t Validation: Acc=48%, Loss=0.695968747138977\n",
            "Iteration: 341860 \t Train: Acc=50%, Loss=0.685522735118866 \t\t Validation: Acc=51%, Loss=0.7006909251213074\n",
            "Iteration: 341870 \t Train: Acc=51%, Loss=0.6911669969558716 \t\t Validation: Acc=51%, Loss=0.6937815546989441\n",
            "Iteration: 341880 \t Train: Acc=50%, Loss=0.6902900338172913 \t\t Validation: Acc=50%, Loss=0.6959516406059265\n",
            "Iteration: 341890 \t Train: Acc=50%, Loss=0.6903654336929321 \t\t Validation: Acc=57%, Loss=0.6902080774307251\n",
            "Iteration: 341900 \t Train: Acc=56%, Loss=0.6946737766265869 \t\t Validation: Acc=50%, Loss=0.6917272210121155\n",
            "Iteration: 341910 \t Train: Acc=55%, Loss=0.6897104382514954 \t\t Validation: Acc=50%, Loss=0.6935045719146729\n",
            "Iteration: 341920 \t Train: Acc=46%, Loss=0.6933416128158569 \t\t Validation: Acc=53%, Loss=0.6914934515953064\n",
            "Iteration: 341930 \t Train: Acc=59%, Loss=0.6852438449859619 \t\t Validation: Acc=50%, Loss=0.6950935125350952\n",
            "Iteration: 341940 \t Train: Acc=52%, Loss=0.6914228796958923 \t\t Validation: Acc=50%, Loss=0.6866260766983032\n",
            "Iteration: 341950 \t Train: Acc=47%, Loss=0.6934136748313904 \t\t Validation: Acc=46%, Loss=0.6915946006774902\n",
            "Iteration: 341960 \t Train: Acc=45%, Loss=0.6992452144622803 \t\t Validation: Acc=53%, Loss=0.6911437511444092\n",
            "Iteration: 341970 \t Train: Acc=53%, Loss=0.6890301704406738 \t\t Validation: Acc=53%, Loss=0.6942752003669739\n",
            "Iteration: 341980 \t Train: Acc=54%, Loss=0.6883017420768738 \t\t Validation: Acc=48%, Loss=0.6975528001785278\n",
            "Iteration: 341990 \t Train: Acc=50%, Loss=0.6914315223693848 \t\t Validation: Acc=51%, Loss=0.6871464252471924\n",
            "Iteration: 342000 \t Train: Acc=48%, Loss=0.6972401142120361 \t\t Validation: Acc=48%, Loss=0.6981651782989502\n",
            "Iteration: 342010 \t Train: Acc=46%, Loss=0.6941934823989868 \t\t Validation: Acc=50%, Loss=0.6931955814361572\n",
            "Iteration: 342020 \t Train: Acc=56%, Loss=0.6898801922798157 \t\t Validation: Acc=50%, Loss=0.69617760181427\n",
            "Iteration: 342030 \t Train: Acc=60%, Loss=0.6882813572883606 \t\t Validation: Acc=53%, Loss=0.6891542673110962\n",
            "Iteration: 342040 \t Train: Acc=57%, Loss=0.6914769411087036 \t\t Validation: Acc=50%, Loss=0.7006994485855103\n",
            "Iteration: 342050 \t Train: Acc=46%, Loss=0.7007295489311218 \t\t Validation: Acc=50%, Loss=0.6923041343688965\n",
            "Iteration: 342060 \t Train: Acc=52%, Loss=0.6867335438728333 \t\t Validation: Acc=50%, Loss=0.6907232403755188\n",
            "Iteration: 342070 \t Train: Acc=49%, Loss=0.6910134553909302 \t\t Validation: Acc=55%, Loss=0.6912333369255066\n",
            "Iteration: 342080 \t Train: Acc=46%, Loss=0.6960468292236328 \t\t Validation: Acc=54%, Loss=0.6907175779342651\n",
            "Iteration: 342090 \t Train: Acc=47%, Loss=0.6982702016830444 \t\t Validation: Acc=53%, Loss=0.6900426149368286\n",
            "Iteration: 342100 \t Train: Acc=49%, Loss=0.6934870481491089 \t\t Validation: Acc=46%, Loss=0.6944645643234253\n",
            "Iteration: 342110 \t Train: Acc=52%, Loss=0.6931430697441101 \t\t Validation: Acc=51%, Loss=0.6931037902832031\n",
            "Iteration: 342120 \t Train: Acc=46%, Loss=0.6998569369316101 \t\t Validation: Acc=50%, Loss=0.6994451284408569\n",
            "Iteration: 342130 \t Train: Acc=53%, Loss=0.6910049319267273 \t\t Validation: Acc=52%, Loss=0.6922342777252197\n",
            "Iteration: 342140 \t Train: Acc=52%, Loss=0.6955579519271851 \t\t Validation: Acc=46%, Loss=0.6942794322967529\n",
            "Iteration: 342150 \t Train: Acc=51%, Loss=0.6898572444915771 \t\t Validation: Acc=49%, Loss=0.6996853947639465\n",
            "Iteration: 342160 \t Train: Acc=49%, Loss=0.6905866861343384 \t\t Validation: Acc=49%, Loss=0.6940028071403503\n",
            "Iteration: 342170 \t Train: Acc=50%, Loss=0.6909408569335938 \t\t Validation: Acc=53%, Loss=0.6844667196273804\n",
            "Iteration: 342180 \t Train: Acc=53%, Loss=0.6887906789779663 \t\t Validation: Acc=53%, Loss=0.691902220249176\n",
            "Iteration: 342190 \t Train: Acc=52%, Loss=0.6912903189659119 \t\t Validation: Acc=48%, Loss=0.6950104832649231\n",
            "Iteration: 342200 \t Train: Acc=48%, Loss=0.6965377926826477 \t\t Validation: Acc=50%, Loss=0.6907373666763306\n",
            "Iteration: 342210 \t Train: Acc=54%, Loss=0.6871736645698547 \t\t Validation: Acc=53%, Loss=0.6893497705459595\n",
            "Iteration: 342220 \t Train: Acc=50%, Loss=0.6938867568969727 \t\t Validation: Acc=51%, Loss=0.6903725266456604\n",
            "Iteration: 342230 \t Train: Acc=51%, Loss=0.6920895576477051 \t\t Validation: Acc=51%, Loss=0.6896123886108398\n",
            "Iteration: 342240 \t Train: Acc=56%, Loss=0.6843271255493164 \t\t Validation: Acc=52%, Loss=0.6921501159667969\n",
            "Iteration: 342250 \t Train: Acc=52%, Loss=0.6902749538421631 \t\t Validation: Acc=50%, Loss=0.6904853582382202\n",
            "Iteration: 342260 \t Train: Acc=52%, Loss=0.6921495795249939 \t\t Validation: Acc=48%, Loss=0.6971247792243958\n",
            "Iteration: 342270 \t Train: Acc=46%, Loss=0.6957870721817017 \t\t Validation: Acc=50%, Loss=0.6930123567581177\n",
            "Iteration: 342280 \t Train: Acc=42%, Loss=0.7006644010543823 \t\t Validation: Acc=49%, Loss=0.6942071914672852\n",
            "Iteration: 342290 \t Train: Acc=55%, Loss=0.6925703287124634 \t\t Validation: Acc=45%, Loss=0.6994343996047974\n",
            "Iteration: 342300 \t Train: Acc=53%, Loss=0.6903886795043945 \t\t Validation: Acc=54%, Loss=0.6920256018638611\n",
            "Iteration: 342310 \t Train: Acc=55%, Loss=0.6904440522193909 \t\t Validation: Acc=49%, Loss=0.6939001083374023\n",
            "Iteration: 342320 \t Train: Acc=53%, Loss=0.6882887482643127 \t\t Validation: Acc=46%, Loss=0.6940219402313232\n",
            "Iteration: 342330 \t Train: Acc=50%, Loss=0.6916993856430054 \t\t Validation: Acc=50%, Loss=0.6917110085487366\n",
            "Iteration: 342340 \t Train: Acc=44%, Loss=0.6923908591270447 \t\t Validation: Acc=53%, Loss=0.6909275054931641\n",
            "Iteration: 342350 \t Train: Acc=51%, Loss=0.6956358551979065 \t\t Validation: Acc=53%, Loss=0.6926488280296326\n",
            "Iteration: 342360 \t Train: Acc=50%, Loss=0.6908863186836243 \t\t Validation: Acc=51%, Loss=0.6911725997924805\n",
            "Iteration: 342370 \t Train: Acc=57%, Loss=0.6905753016471863 \t\t Validation: Acc=55%, Loss=0.6861732006072998\n",
            "Iteration: 342380 \t Train: Acc=49%, Loss=0.6929841637611389 \t\t Validation: Acc=46%, Loss=0.6980019211769104\n",
            "Iteration: 342390 \t Train: Acc=50%, Loss=0.6892121434211731 \t\t Validation: Acc=50%, Loss=0.6930376291275024\n",
            "Iteration: 342400 \t Train: Acc=53%, Loss=0.6904537081718445 \t\t Validation: Acc=46%, Loss=0.6997995376586914\n",
            "Iteration: 342410 \t Train: Acc=54%, Loss=0.6923049092292786 \t\t Validation: Acc=50%, Loss=0.6900855898857117\n",
            "Iteration: 342420 \t Train: Acc=48%, Loss=0.6975374221801758 \t\t Validation: Acc=48%, Loss=0.6925366520881653\n",
            "Iteration: 342430 \t Train: Acc=48%, Loss=0.7055330276489258 \t\t Validation: Acc=48%, Loss=0.697763979434967\n",
            "Iteration: 342440 \t Train: Acc=50%, Loss=0.689228892326355 \t\t Validation: Acc=53%, Loss=0.6892024278640747\n",
            "Iteration: 342450 \t Train: Acc=48%, Loss=0.6931124925613403 \t\t Validation: Acc=55%, Loss=0.6846604347229004\n",
            "Iteration: 342460 \t Train: Acc=39%, Loss=0.7022883892059326 \t\t Validation: Acc=52%, Loss=0.6916452050209045\n",
            "Iteration: 342470 \t Train: Acc=55%, Loss=0.6873975992202759 \t\t Validation: Acc=48%, Loss=0.6951977014541626\n",
            "Iteration: 342480 \t Train: Acc=54%, Loss=0.6894000768661499 \t\t Validation: Acc=51%, Loss=0.6860954761505127\n",
            "Iteration: 342490 \t Train: Acc=52%, Loss=0.689360499382019 \t\t Validation: Acc=50%, Loss=0.6953277587890625\n",
            "Iteration: 342500 \t Train: Acc=58%, Loss=0.6853250861167908 \t\t Validation: Acc=47%, Loss=0.6957129836082458\n",
            "Iteration: 342510 \t Train: Acc=50%, Loss=0.6955311894416809 \t\t Validation: Acc=52%, Loss=0.6879236698150635\n",
            "Iteration: 342520 \t Train: Acc=47%, Loss=0.6972092390060425 \t\t Validation: Acc=47%, Loss=0.6980723142623901\n",
            "Iteration: 342530 \t Train: Acc=50%, Loss=0.6921607255935669 \t\t Validation: Acc=51%, Loss=0.692653477191925\n",
            "Iteration: 342540 \t Train: Acc=51%, Loss=0.6963384747505188 \t\t Validation: Acc=52%, Loss=0.6883999705314636\n",
            "Iteration: 342550 \t Train: Acc=50%, Loss=0.6925157904624939 \t\t Validation: Acc=53%, Loss=0.6942678689956665\n",
            "Iteration: 342560 \t Train: Acc=50%, Loss=0.6890215277671814 \t\t Validation: Acc=55%, Loss=0.6827104687690735\n",
            "Iteration: 342570 \t Train: Acc=53%, Loss=0.6901328563690186 \t\t Validation: Acc=52%, Loss=0.6930887699127197\n",
            "Iteration: 342580 \t Train: Acc=44%, Loss=0.6981413960456848 \t\t Validation: Acc=48%, Loss=0.6947588920593262\n",
            "Iteration: 342590 \t Train: Acc=53%, Loss=0.6922838687896729 \t\t Validation: Acc=45%, Loss=0.7025805115699768\n",
            "Iteration: 342600 \t Train: Acc=46%, Loss=0.6948399543762207 \t\t Validation: Acc=50%, Loss=0.69405597448349\n",
            "Iteration: 342610 \t Train: Acc=53%, Loss=0.6902417540550232 \t\t Validation: Acc=53%, Loss=0.6907823085784912\n",
            "Iteration: 342620 \t Train: Acc=54%, Loss=0.690417468547821 \t\t Validation: Acc=51%, Loss=0.6875518560409546\n",
            "Iteration: 342630 \t Train: Acc=59%, Loss=0.6809756755828857 \t\t Validation: Acc=47%, Loss=0.6958467364311218\n",
            "Iteration: 342640 \t Train: Acc=47%, Loss=0.6932696104049683 \t\t Validation: Acc=49%, Loss=0.69398033618927\n",
            "Iteration: 342650 \t Train: Acc=48%, Loss=0.691352367401123 \t\t Validation: Acc=55%, Loss=0.6924936175346375\n",
            "Iteration: 342660 \t Train: Acc=55%, Loss=0.6873388886451721 \t\t Validation: Acc=52%, Loss=0.6906630396842957\n",
            "Iteration: 342670 \t Train: Acc=39%, Loss=0.7070937752723694 \t\t Validation: Acc=47%, Loss=0.6903125047683716\n",
            "Iteration: 342680 \t Train: Acc=56%, Loss=0.6836754679679871 \t\t Validation: Acc=52%, Loss=0.6940369009971619\n",
            "Iteration: 342690 \t Train: Acc=44%, Loss=0.7012518644332886 \t\t Validation: Acc=49%, Loss=0.6934574842453003\n",
            "Iteration: 342700 \t Train: Acc=55%, Loss=0.6879897713661194 \t\t Validation: Acc=47%, Loss=0.6922119855880737\n",
            "Iteration: 342710 \t Train: Acc=53%, Loss=0.6853313446044922 \t\t Validation: Acc=50%, Loss=0.6955865025520325\n",
            "Iteration: 342720 \t Train: Acc=53%, Loss=0.6843069791793823 \t\t Validation: Acc=49%, Loss=0.6903383731842041\n",
            "Iteration: 342730 \t Train: Acc=53%, Loss=0.6950877904891968 \t\t Validation: Acc=50%, Loss=0.6965374946594238\n",
            "Iteration: 342740 \t Train: Acc=53%, Loss=0.6857688426971436 \t\t Validation: Acc=53%, Loss=0.692827582359314\n",
            "Iteration: 342750 \t Train: Acc=53%, Loss=0.6902709007263184 \t\t Validation: Acc=51%, Loss=0.6897060871124268\n",
            "Iteration: 342760 \t Train: Acc=46%, Loss=0.6994569897651672 \t\t Validation: Acc=50%, Loss=0.7046535015106201\n",
            "Iteration: 342770 \t Train: Acc=51%, Loss=0.691774845123291 \t\t Validation: Acc=50%, Loss=0.689413845539093\n",
            "Iteration: 342780 \t Train: Acc=50%, Loss=0.6876459121704102 \t\t Validation: Acc=50%, Loss=0.704399049282074\n",
            "Iteration: 342790 \t Train: Acc=50%, Loss=0.6887423992156982 \t\t Validation: Acc=50%, Loss=0.6934431791305542\n",
            "Iteration: 342800 \t Train: Acc=50%, Loss=0.695805549621582 \t\t Validation: Acc=50%, Loss=0.6921321153640747\n",
            "Iteration: 342810 \t Train: Acc=52%, Loss=0.6938689351081848 \t\t Validation: Acc=52%, Loss=0.6873444318771362\n",
            "Iteration: 342820 \t Train: Acc=53%, Loss=0.6905064582824707 \t\t Validation: Acc=56%, Loss=0.6891170144081116\n",
            "Iteration: 342830 \t Train: Acc=50%, Loss=0.6996510028839111 \t\t Validation: Acc=49%, Loss=0.6972631812095642\n",
            "Iteration: 342840 \t Train: Acc=50%, Loss=0.690781831741333 \t\t Validation: Acc=50%, Loss=0.694031834602356\n",
            "Iteration: 342850 \t Train: Acc=48%, Loss=0.692689836025238 \t\t Validation: Acc=50%, Loss=0.6896695494651794\n",
            "Iteration: 342860 \t Train: Acc=50%, Loss=0.6978240013122559 \t\t Validation: Acc=49%, Loss=0.6946746110916138\n",
            "Iteration: 342870 \t Train: Acc=50%, Loss=0.6937817335128784 \t\t Validation: Acc=51%, Loss=0.6910303831100464\n",
            "Iteration: 342880 \t Train: Acc=47%, Loss=0.6913346648216248 \t\t Validation: Acc=53%, Loss=0.6945558786392212\n",
            "Iteration: 342890 \t Train: Acc=57%, Loss=0.6813760995864868 \t\t Validation: Acc=50%, Loss=0.6895225644111633\n",
            "Iteration: 342900 \t Train: Acc=51%, Loss=0.692888081073761 \t\t Validation: Acc=53%, Loss=0.6885198950767517\n",
            "Iteration: 342910 \t Train: Acc=53%, Loss=0.6912563443183899 \t\t Validation: Acc=46%, Loss=0.6995621919631958\n",
            "Iteration: 342920 \t Train: Acc=53%, Loss=0.6891873478889465 \t\t Validation: Acc=56%, Loss=0.6923425197601318\n",
            "Iteration: 342930 \t Train: Acc=47%, Loss=0.6945736408233643 \t\t Validation: Acc=46%, Loss=0.6988179683685303\n",
            "Iteration: 342940 \t Train: Acc=48%, Loss=0.6978060007095337 \t\t Validation: Acc=53%, Loss=0.6864287853240967\n",
            "Iteration: 342950 \t Train: Acc=56%, Loss=0.6918116211891174 \t\t Validation: Acc=50%, Loss=0.6952449679374695\n",
            "Iteration: 342960 \t Train: Acc=50%, Loss=0.6882901191711426 \t\t Validation: Acc=54%, Loss=0.6891738176345825\n",
            "Iteration: 342970 \t Train: Acc=53%, Loss=0.6860830187797546 \t\t Validation: Acc=51%, Loss=0.6927348375320435\n",
            "Iteration: 342980 \t Train: Acc=55%, Loss=0.6928415298461914 \t\t Validation: Acc=50%, Loss=0.6968305110931396\n",
            "Iteration: 342990 \t Train: Acc=49%, Loss=0.6964496374130249 \t\t Validation: Acc=48%, Loss=0.6966124176979065\n",
            "Iteration: 343000 \t Train: Acc=57%, Loss=0.6911563277244568 \t\t Validation: Acc=54%, Loss=0.6945932507514954\n",
            "Iteration: 343010 \t Train: Acc=54%, Loss=0.6898083686828613 \t\t Validation: Acc=53%, Loss=0.6910311579704285\n",
            "Iteration: 343020 \t Train: Acc=51%, Loss=0.6889316439628601 \t\t Validation: Acc=53%, Loss=0.6918943524360657\n",
            "Iteration: 343030 \t Train: Acc=52%, Loss=0.691853940486908 \t\t Validation: Acc=50%, Loss=0.6942549347877502\n",
            "Iteration: 343040 \t Train: Acc=50%, Loss=0.6948250532150269 \t\t Validation: Acc=52%, Loss=0.6952148675918579\n",
            "Iteration: 343050 \t Train: Acc=50%, Loss=0.6894647479057312 \t\t Validation: Acc=48%, Loss=0.6938040256500244\n",
            "Iteration: 343060 \t Train: Acc=50%, Loss=0.6911658644676208 \t\t Validation: Acc=50%, Loss=0.6924436092376709\n",
            "Iteration: 343070 \t Train: Acc=46%, Loss=0.6928859353065491 \t\t Validation: Acc=47%, Loss=0.697856068611145\n",
            "Iteration: 343080 \t Train: Acc=53%, Loss=0.6920244097709656 \t\t Validation: Acc=48%, Loss=0.6935814023017883\n",
            "Iteration: 343090 \t Train: Acc=48%, Loss=0.6977908611297607 \t\t Validation: Acc=50%, Loss=0.692037045955658\n",
            "Iteration: 343100 \t Train: Acc=49%, Loss=0.6921253800392151 \t\t Validation: Acc=56%, Loss=0.6884695291519165\n",
            "Iteration: 343110 \t Train: Acc=54%, Loss=0.6887193918228149 \t\t Validation: Acc=48%, Loss=0.6928073763847351\n",
            "Iteration: 343120 \t Train: Acc=50%, Loss=0.6876406669616699 \t\t Validation: Acc=48%, Loss=0.6993414163589478\n",
            "Iteration: 343130 \t Train: Acc=50%, Loss=0.6902785897254944 \t\t Validation: Acc=54%, Loss=0.685998797416687\n",
            "Iteration: 343140 \t Train: Acc=47%, Loss=0.6952500343322754 \t\t Validation: Acc=50%, Loss=0.6909427046775818\n",
            "Iteration: 343150 \t Train: Acc=49%, Loss=0.6875655055046082 \t\t Validation: Acc=54%, Loss=0.6905900835990906\n",
            "Iteration: 343160 \t Train: Acc=53%, Loss=0.6894857883453369 \t\t Validation: Acc=50%, Loss=0.6875388026237488\n",
            "Iteration: 343170 \t Train: Acc=51%, Loss=0.6865587830543518 \t\t Validation: Acc=53%, Loss=0.68681800365448\n",
            "Iteration: 343180 \t Train: Acc=53%, Loss=0.690098762512207 \t\t Validation: Acc=50%, Loss=0.6930426359176636\n",
            "Iteration: 343190 \t Train: Acc=52%, Loss=0.6928989291191101 \t\t Validation: Acc=50%, Loss=0.6926517486572266\n",
            "Iteration: 343200 \t Train: Acc=55%, Loss=0.6876838207244873 \t\t Validation: Acc=53%, Loss=0.6934100389480591\n",
            "Iteration: 343210 \t Train: Acc=52%, Loss=0.6900219321250916 \t\t Validation: Acc=51%, Loss=0.6864272952079773\n",
            "Iteration: 343220 \t Train: Acc=54%, Loss=0.6858000755310059 \t\t Validation: Acc=49%, Loss=0.6888805627822876\n",
            "Iteration: 343230 \t Train: Acc=54%, Loss=0.6883288025856018 \t\t Validation: Acc=51%, Loss=0.6909165978431702\n",
            "Iteration: 343240 \t Train: Acc=48%, Loss=0.6940780878067017 \t\t Validation: Acc=51%, Loss=0.6896953582763672\n",
            "Iteration: 343250 \t Train: Acc=51%, Loss=0.6860584616661072 \t\t Validation: Acc=49%, Loss=0.6928198337554932\n",
            "Iteration: 343260 \t Train: Acc=53%, Loss=0.6875333189964294 \t\t Validation: Acc=42%, Loss=0.7013581395149231\n",
            "Iteration: 343270 \t Train: Acc=50%, Loss=0.6978982090950012 \t\t Validation: Acc=47%, Loss=0.6957792043685913\n",
            "Iteration: 343280 \t Train: Acc=53%, Loss=0.6943773627281189 \t\t Validation: Acc=51%, Loss=0.6842303276062012\n",
            "Iteration: 343290 \t Train: Acc=55%, Loss=0.6842260956764221 \t\t Validation: Acc=53%, Loss=0.6841424703598022\n",
            "Iteration: 343300 \t Train: Acc=47%, Loss=0.7006967663764954 \t\t Validation: Acc=53%, Loss=0.6904219388961792\n",
            "Iteration: 343310 \t Train: Acc=45%, Loss=0.6923398375511169 \t\t Validation: Acc=50%, Loss=0.6983287334442139\n",
            "Iteration: 343320 \t Train: Acc=50%, Loss=0.691569983959198 \t\t Validation: Acc=50%, Loss=0.6922777891159058\n",
            "Iteration: 343330 \t Train: Acc=54%, Loss=0.6842520833015442 \t\t Validation: Acc=50%, Loss=0.6925308704376221\n",
            "Iteration: 343340 \t Train: Acc=50%, Loss=0.6913016438484192 \t\t Validation: Acc=47%, Loss=0.7020087838172913\n",
            "Iteration: 343350 \t Train: Acc=57%, Loss=0.6847498416900635 \t\t Validation: Acc=57%, Loss=0.6917261481285095\n",
            "Iteration: 343360 \t Train: Acc=53%, Loss=0.6896499991416931 \t\t Validation: Acc=49%, Loss=0.6920928955078125\n",
            "Iteration: 343370 \t Train: Acc=52%, Loss=0.689743161201477 \t\t Validation: Acc=47%, Loss=0.6971065998077393\n",
            "Iteration: 343380 \t Train: Acc=51%, Loss=0.6917893886566162 \t\t Validation: Acc=51%, Loss=0.6916570067405701\n",
            "Iteration: 343390 \t Train: Acc=46%, Loss=0.6954014897346497 \t\t Validation: Acc=53%, Loss=0.6832045316696167\n",
            "Iteration: 343400 \t Train: Acc=51%, Loss=0.6919795870780945 \t\t Validation: Acc=53%, Loss=0.6964033842086792\n",
            "Iteration: 343410 \t Train: Acc=48%, Loss=0.6866791248321533 \t\t Validation: Acc=50%, Loss=0.6938197612762451\n",
            "Iteration: 343420 \t Train: Acc=44%, Loss=0.6917840242385864 \t\t Validation: Acc=53%, Loss=0.6844090819358826\n",
            "Iteration: 343430 \t Train: Acc=60%, Loss=0.6821727752685547 \t\t Validation: Acc=48%, Loss=0.6923141479492188\n",
            "Iteration: 343440 \t Train: Acc=44%, Loss=0.6977529525756836 \t\t Validation: Acc=51%, Loss=0.6929803490638733\n",
            "Iteration: 343450 \t Train: Acc=50%, Loss=0.7081313729286194 \t\t Validation: Acc=50%, Loss=0.6882169842720032\n",
            "Iteration: 343460 \t Train: Acc=51%, Loss=0.6980307102203369 \t\t Validation: Acc=46%, Loss=0.6934795379638672\n",
            "Iteration: 343470 \t Train: Acc=50%, Loss=0.6895613670349121 \t\t Validation: Acc=50%, Loss=0.6903402805328369\n",
            "Iteration: 343480 \t Train: Acc=46%, Loss=0.6877381205558777 \t\t Validation: Acc=54%, Loss=0.6908143758773804\n",
            "Iteration: 343490 \t Train: Acc=51%, Loss=0.6895714402198792 \t\t Validation: Acc=45%, Loss=0.6930831074714661\n",
            "Iteration: 343500 \t Train: Acc=51%, Loss=0.6918246746063232 \t\t Validation: Acc=49%, Loss=0.6891363859176636\n",
            "Iteration: 343510 \t Train: Acc=51%, Loss=0.6942183375358582 \t\t Validation: Acc=50%, Loss=0.6970735192298889\n",
            "Iteration: 343520 \t Train: Acc=52%, Loss=0.6887403726577759 \t\t Validation: Acc=51%, Loss=0.6900575160980225\n",
            "Iteration: 343530 \t Train: Acc=53%, Loss=0.692658543586731 \t\t Validation: Acc=48%, Loss=0.6909205913543701\n",
            "Iteration: 343540 \t Train: Acc=57%, Loss=0.6930657625198364 \t\t Validation: Acc=49%, Loss=0.6910244226455688\n",
            "Iteration: 343550 \t Train: Acc=50%, Loss=0.7013460397720337 \t\t Validation: Acc=52%, Loss=0.6904363036155701\n",
            "Iteration: 343560 \t Train: Acc=57%, Loss=0.6833636164665222 \t\t Validation: Acc=48%, Loss=0.6955389976501465\n",
            "Iteration: 343570 \t Train: Acc=50%, Loss=0.6936078071594238 \t\t Validation: Acc=49%, Loss=0.6902884840965271\n",
            "It's been too long since we last saved the model. Saving...\n",
            "Iteration: 343580 \t Train: Acc=49%, Loss=0.6934594511985779 \t\t Validation: Acc=48%, Loss=0.6959197521209717\n",
            "Iteration: 343590 \t Train: Acc=52%, Loss=0.7009977102279663 \t\t Validation: Acc=50%, Loss=0.6936113834381104\n",
            "Iteration: 343600 \t Train: Acc=44%, Loss=0.7001262307167053 \t\t Validation: Acc=54%, Loss=0.6867765188217163\n",
            "Iteration: 343610 \t Train: Acc=57%, Loss=0.6875889301300049 \t\t Validation: Acc=50%, Loss=0.6896395683288574\n",
            "Iteration: 343620 \t Train: Acc=59%, Loss=0.6862951517105103 \t\t Validation: Acc=50%, Loss=0.6917459964752197\n",
            "Iteration: 343630 \t Train: Acc=50%, Loss=0.6942160725593567 \t\t Validation: Acc=51%, Loss=0.6918189525604248\n",
            "Iteration: 343640 \t Train: Acc=50%, Loss=0.6922913789749146 \t\t Validation: Acc=55%, Loss=0.6897203326225281\n",
            "Iteration: 343650 \t Train: Acc=50%, Loss=0.6906332969665527 \t\t Validation: Acc=56%, Loss=0.6887105703353882\n",
            "Iteration: 343660 \t Train: Acc=50%, Loss=0.6876730918884277 \t\t Validation: Acc=49%, Loss=0.687195360660553\n",
            "Iteration: 343670 \t Train: Acc=50%, Loss=0.689570963382721 \t\t Validation: Acc=47%, Loss=0.6943992376327515\n",
            "Iteration: 343680 \t Train: Acc=47%, Loss=0.6938123106956482 \t\t Validation: Acc=51%, Loss=0.6888651847839355\n",
            "Iteration: 343690 \t Train: Acc=49%, Loss=0.7020061016082764 \t\t Validation: Acc=47%, Loss=0.6911587119102478\n",
            "Iteration: 343700 \t Train: Acc=53%, Loss=0.6925743818283081 \t\t Validation: Acc=47%, Loss=0.6905805468559265\n",
            "Iteration: 343710 \t Train: Acc=50%, Loss=0.6890697479248047 \t\t Validation: Acc=48%, Loss=0.6965028047561646\n",
            "Iteration: 343720 \t Train: Acc=50%, Loss=0.6960629224777222 \t\t Validation: Acc=47%, Loss=0.7010316252708435\n",
            "Iteration: 343730 \t Train: Acc=51%, Loss=0.6913018226623535 \t\t Validation: Acc=50%, Loss=0.692213237285614\n",
            "Iteration: 343740 \t Train: Acc=50%, Loss=0.6915905475616455 \t\t Validation: Acc=53%, Loss=0.689959704875946\n",
            "Iteration: 343750 \t Train: Acc=56%, Loss=0.6859296560287476 \t\t Validation: Acc=48%, Loss=0.6924503445625305\n",
            "Iteration: 343760 \t Train: Acc=50%, Loss=0.6925033330917358 \t\t Validation: Acc=53%, Loss=0.6892887949943542\n",
            "Iteration: 343770 \t Train: Acc=49%, Loss=0.6932584047317505 \t\t Validation: Acc=47%, Loss=0.6929916739463806\n",
            "Iteration: 343780 \t Train: Acc=46%, Loss=0.6950649619102478 \t\t Validation: Acc=48%, Loss=0.7027497291564941\n",
            "Iteration: 343790 \t Train: Acc=53%, Loss=0.6892424821853638 \t\t Validation: Acc=50%, Loss=0.6949165463447571\n",
            "Iteration: 343800 \t Train: Acc=49%, Loss=0.695865273475647 \t\t Validation: Acc=52%, Loss=0.6893414258956909\n",
            "Iteration: 343810 \t Train: Acc=46%, Loss=0.6927101612091064 \t\t Validation: Acc=46%, Loss=0.6992771625518799\n",
            "Iteration: 343820 \t Train: Acc=56%, Loss=0.6875009536743164 \t\t Validation: Acc=50%, Loss=0.6962581872940063\n",
            "Iteration: 343830 \t Train: Acc=46%, Loss=0.6953834891319275 \t\t Validation: Acc=51%, Loss=0.6923904418945312\n",
            "Iteration: 343840 \t Train: Acc=52%, Loss=0.6867824792861938 \t\t Validation: Acc=53%, Loss=0.6907046437263489\n",
            "Iteration: 343850 \t Train: Acc=55%, Loss=0.6897237300872803 \t\t Validation: Acc=49%, Loss=0.6989689469337463\n",
            "Iteration: 343860 \t Train: Acc=57%, Loss=0.6863261461257935 \t\t Validation: Acc=46%, Loss=0.6970092058181763\n",
            "Iteration: 343870 \t Train: Acc=53%, Loss=0.6913458704948425 \t\t Validation: Acc=49%, Loss=0.691321074962616\n",
            "Iteration: 343880 \t Train: Acc=53%, Loss=0.6901075839996338 \t\t Validation: Acc=48%, Loss=0.6960006952285767\n",
            "Iteration: 343890 \t Train: Acc=49%, Loss=0.6909170150756836 \t\t Validation: Acc=50%, Loss=0.6922716498374939\n",
            "Iteration: 343900 \t Train: Acc=55%, Loss=0.6876224279403687 \t\t Validation: Acc=50%, Loss=0.6911113858222961\n",
            "Iteration: 343910 \t Train: Acc=49%, Loss=0.691801905632019 \t\t Validation: Acc=54%, Loss=0.6939322352409363\n",
            "Iteration: 343920 \t Train: Acc=54%, Loss=0.6918971538543701 \t\t Validation: Acc=54%, Loss=0.6938841342926025\n",
            "Iteration: 343930 \t Train: Acc=52%, Loss=0.682132363319397 \t\t Validation: Acc=49%, Loss=0.6917897462844849\n",
            "Iteration: 343940 \t Train: Acc=47%, Loss=0.7001523971557617 \t\t Validation: Acc=51%, Loss=0.6927934288978577\n",
            "Iteration: 343950 \t Train: Acc=52%, Loss=0.6922705769538879 \t\t Validation: Acc=57%, Loss=0.6857922673225403\n",
            "Iteration: 343960 \t Train: Acc=53%, Loss=0.6822871565818787 \t\t Validation: Acc=53%, Loss=0.6864888072013855\n",
            "Iteration: 343970 \t Train: Acc=55%, Loss=0.6935262680053711 \t\t Validation: Acc=49%, Loss=0.6968724727630615\n",
            "Iteration: 343980 \t Train: Acc=49%, Loss=0.6985117793083191 \t\t Validation: Acc=47%, Loss=0.6972721815109253\n",
            "Iteration: 343990 \t Train: Acc=47%, Loss=0.6980689764022827 \t\t Validation: Acc=48%, Loss=0.694555938243866\n",
            "Iteration: 344000 \t Train: Acc=46%, Loss=0.6942884922027588 \t\t Validation: Acc=46%, Loss=0.6943386793136597\n",
            "Iteration: 344010 \t Train: Acc=54%, Loss=0.6864225268363953 \t\t Validation: Acc=51%, Loss=0.6884121894836426\n",
            "Iteration: 344020 \t Train: Acc=46%, Loss=0.6918039321899414 \t\t Validation: Acc=51%, Loss=0.6947757005691528\n",
            "Iteration: 344030 \t Train: Acc=49%, Loss=0.6916385293006897 \t\t Validation: Acc=51%, Loss=0.6898937821388245\n",
            "Iteration: 344040 \t Train: Acc=46%, Loss=0.6918100714683533 \t\t Validation: Acc=53%, Loss=0.6923503875732422\n",
            "Iteration: 344050 \t Train: Acc=53%, Loss=0.6887704730033875 \t\t Validation: Acc=50%, Loss=0.689753532409668\n",
            "Iteration: 344060 \t Train: Acc=50%, Loss=0.6929905414581299 \t\t Validation: Acc=50%, Loss=0.689536452293396\n",
            "Iteration: 344070 \t Train: Acc=54%, Loss=0.6802618503570557 \t\t Validation: Acc=52%, Loss=0.6909142136573792\n",
            "Iteration: 344080 \t Train: Acc=53%, Loss=0.6975570917129517 \t\t Validation: Acc=47%, Loss=0.6942421197891235\n",
            "Iteration: 344090 \t Train: Acc=55%, Loss=0.6912288069725037 \t\t Validation: Acc=49%, Loss=0.6919386982917786\n",
            "Iteration: 344100 \t Train: Acc=50%, Loss=0.693747878074646 \t\t Validation: Acc=47%, Loss=0.6953204274177551\n",
            "Iteration: 344110 \t Train: Acc=49%, Loss=0.6888221502304077 \t\t Validation: Acc=45%, Loss=0.6933556795120239\n",
            "Iteration: 344120 \t Train: Acc=49%, Loss=0.6890250444412231 \t\t Validation: Acc=50%, Loss=0.6982319355010986\n",
            "Iteration: 344130 \t Train: Acc=50%, Loss=0.6901284456253052 \t\t Validation: Acc=54%, Loss=0.6861382126808167\n",
            "Iteration: 344140 \t Train: Acc=47%, Loss=0.6937975287437439 \t\t Validation: Acc=52%, Loss=0.6943103075027466\n",
            "Iteration: 344150 \t Train: Acc=48%, Loss=0.6960668563842773 \t\t Validation: Acc=53%, Loss=0.6889462471008301\n",
            "Iteration: 344160 \t Train: Acc=53%, Loss=0.7013704180717468 \t\t Validation: Acc=54%, Loss=0.6910369992256165\n",
            "Iteration: 344170 \t Train: Acc=43%, Loss=0.6946007013320923 \t\t Validation: Acc=51%, Loss=0.6850894093513489\n",
            "Iteration: 344180 \t Train: Acc=57%, Loss=0.6816588640213013 \t\t Validation: Acc=51%, Loss=0.6965371370315552\n",
            "Iteration: 344190 \t Train: Acc=54%, Loss=0.6886792778968811 \t\t Validation: Acc=53%, Loss=0.690194845199585\n",
            "Iteration: 344200 \t Train: Acc=47%, Loss=0.7018927931785583 \t\t Validation: Acc=51%, Loss=0.6937490105628967\n",
            "Iteration: 344210 \t Train: Acc=50%, Loss=0.6898928284645081 \t\t Validation: Acc=52%, Loss=0.6896342039108276\n",
            "Iteration: 344220 \t Train: Acc=56%, Loss=0.6883032917976379 \t\t Validation: Acc=45%, Loss=0.6954682469367981\n",
            "Iteration: 344230 \t Train: Acc=51%, Loss=0.689821183681488 \t\t Validation: Acc=52%, Loss=0.6927380561828613\n",
            "Iteration: 344240 \t Train: Acc=55%, Loss=0.6873680949211121 \t\t Validation: Acc=51%, Loss=0.6925797462463379\n",
            "Iteration: 344250 \t Train: Acc=56%, Loss=0.6955431699752808 \t\t Validation: Acc=49%, Loss=0.6862342357635498\n",
            "Iteration: 344260 \t Train: Acc=53%, Loss=0.6908904314041138 \t\t Validation: Acc=54%, Loss=0.6914059519767761\n",
            "Iteration: 344270 \t Train: Acc=54%, Loss=0.6856966018676758 \t\t Validation: Acc=52%, Loss=0.6924909353256226\n",
            "Iteration: 344280 \t Train: Acc=59%, Loss=0.6865793466567993 \t\t Validation: Acc=47%, Loss=0.7056559920310974\n",
            "Iteration: 344290 \t Train: Acc=52%, Loss=0.6835712790489197 \t\t Validation: Acc=50%, Loss=0.7005268335342407\n",
            "Iteration: 344300 \t Train: Acc=54%, Loss=0.6853333711624146 \t\t Validation: Acc=51%, Loss=0.6879217028617859\n",
            "Iteration: 344310 \t Train: Acc=49%, Loss=0.6873005628585815 \t\t Validation: Acc=49%, Loss=0.6970054507255554\n",
            "Iteration: 344320 \t Train: Acc=51%, Loss=0.6916452050209045 \t\t Validation: Acc=55%, Loss=0.6820805072784424\n",
            "Iteration: 344330 \t Train: Acc=53%, Loss=0.689049243927002 \t\t Validation: Acc=49%, Loss=0.7026389837265015\n",
            "Iteration: 344340 \t Train: Acc=53%, Loss=0.6909922957420349 \t\t Validation: Acc=51%, Loss=0.6822018027305603\n",
            "Iteration: 344350 \t Train: Acc=54%, Loss=0.6906269788742065 \t\t Validation: Acc=49%, Loss=0.6922575831413269\n",
            "Iteration: 344360 \t Train: Acc=57%, Loss=0.6828868985176086 \t\t Validation: Acc=50%, Loss=0.6970459222793579\n",
            "Iteration: 344370 \t Train: Acc=49%, Loss=0.6956927180290222 \t\t Validation: Acc=50%, Loss=0.691565215587616\n",
            "Iteration: 344380 \t Train: Acc=54%, Loss=0.6884825825691223 \t\t Validation: Acc=52%, Loss=0.6957464814186096\n",
            "Iteration: 344390 \t Train: Acc=50%, Loss=0.6873103380203247 \t\t Validation: Acc=50%, Loss=0.698553204536438\n",
            "Iteration: 344400 \t Train: Acc=43%, Loss=0.6979283690452576 \t\t Validation: Acc=48%, Loss=0.7029713988304138\n",
            "Iteration: 344410 \t Train: Acc=59%, Loss=0.6849756240844727 \t\t Validation: Acc=54%, Loss=0.684240996837616\n",
            "Iteration: 344420 \t Train: Acc=54%, Loss=0.6821863651275635 \t\t Validation: Acc=49%, Loss=0.6913067102432251\n",
            "Iteration: 344430 \t Train: Acc=53%, Loss=0.6905782222747803 \t\t Validation: Acc=52%, Loss=0.6870737075805664\n",
            "Iteration: 344440 \t Train: Acc=50%, Loss=0.6892011761665344 \t\t Validation: Acc=52%, Loss=0.6948943138122559\n",
            "Iteration: 344450 \t Train: Acc=45%, Loss=0.6969516277313232 \t\t Validation: Acc=47%, Loss=0.693912148475647\n",
            "Iteration: 344460 \t Train: Acc=48%, Loss=0.6924079656600952 \t\t Validation: Acc=53%, Loss=0.6973066329956055\n",
            "Iteration: 344470 \t Train: Acc=54%, Loss=0.6857633590698242 \t\t Validation: Acc=50%, Loss=0.6862497925758362\n",
            "Iteration: 344480 \t Train: Acc=46%, Loss=0.7115544080734253 \t\t Validation: Acc=49%, Loss=0.690589427947998\n",
            "Iteration: 344490 \t Train: Acc=54%, Loss=0.6878622770309448 \t\t Validation: Acc=50%, Loss=0.6933058500289917\n",
            "Iteration: 344500 \t Train: Acc=57%, Loss=0.6846529245376587 \t\t Validation: Acc=48%, Loss=0.695785641670227\n",
            "Iteration: 344510 \t Train: Acc=50%, Loss=0.6941290497779846 \t\t Validation: Acc=50%, Loss=0.6920084357261658\n",
            "Iteration: 344520 \t Train: Acc=51%, Loss=0.6933260560035706 \t\t Validation: Acc=50%, Loss=0.6921595931053162\n",
            "Iteration: 344530 \t Train: Acc=50%, Loss=0.6932042241096497 \t\t Validation: Acc=50%, Loss=0.6922608613967896\n",
            "Iteration: 344540 \t Train: Acc=48%, Loss=0.6907342076301575 \t\t Validation: Acc=50%, Loss=0.6922988891601562\n",
            "Iteration: 344550 \t Train: Acc=49%, Loss=0.6983058452606201 \t\t Validation: Acc=55%, Loss=0.6884868144989014\n",
            "Iteration: 344560 \t Train: Acc=47%, Loss=0.6949764490127563 \t\t Validation: Acc=49%, Loss=0.6961448788642883\n",
            "Iteration: 344570 \t Train: Acc=52%, Loss=0.6913952827453613 \t\t Validation: Acc=50%, Loss=0.6972824335098267\n",
            "Iteration: 344580 \t Train: Acc=55%, Loss=0.684156596660614 \t\t Validation: Acc=53%, Loss=0.6881473660469055\n",
            "Iteration: 344590 \t Train: Acc=49%, Loss=0.7008631825447083 \t\t Validation: Acc=50%, Loss=0.6932187676429749\n",
            "Iteration: 344600 \t Train: Acc=53%, Loss=0.6943374276161194 \t\t Validation: Acc=44%, Loss=0.6960500478744507\n",
            "Iteration: 344610 \t Train: Acc=50%, Loss=0.6923354268074036 \t\t Validation: Acc=56%, Loss=0.68656986951828\n",
            "Iteration: 344620 \t Train: Acc=53%, Loss=0.6890934705734253 \t\t Validation: Acc=53%, Loss=0.696632981300354\n",
            "Iteration: 344630 \t Train: Acc=55%, Loss=0.6804883480072021 \t\t Validation: Acc=52%, Loss=0.6873559951782227\n",
            "Iteration: 344640 \t Train: Acc=54%, Loss=0.6856194138526917 \t\t Validation: Acc=50%, Loss=0.6895917057991028\n",
            "Iteration: 344650 \t Train: Acc=49%, Loss=0.6911439895629883 \t\t Validation: Acc=49%, Loss=0.6933884620666504\n",
            "Iteration: 344660 \t Train: Acc=54%, Loss=0.6878615617752075 \t\t Validation: Acc=46%, Loss=0.7022835612297058\n",
            "Iteration: 344670 \t Train: Acc=56%, Loss=0.6877981424331665 \t\t Validation: Acc=51%, Loss=0.6898001432418823\n",
            "Iteration: 344680 \t Train: Acc=51%, Loss=0.7014073729515076 \t\t Validation: Acc=49%, Loss=0.6941019296646118\n",
            "Iteration: 344690 \t Train: Acc=53%, Loss=0.693164587020874 \t\t Validation: Acc=49%, Loss=0.6934012174606323\n",
            "Iteration: 344700 \t Train: Acc=45%, Loss=0.6932758688926697 \t\t Validation: Acc=48%, Loss=0.696262001991272\n",
            "Iteration: 344710 \t Train: Acc=50%, Loss=0.6929477453231812 \t\t Validation: Acc=50%, Loss=0.6923397183418274\n",
            "Iteration: 344720 \t Train: Acc=46%, Loss=0.6891465187072754 \t\t Validation: Acc=53%, Loss=0.6891179084777832\n",
            "Iteration: 344730 \t Train: Acc=49%, Loss=0.691632866859436 \t\t Validation: Acc=55%, Loss=0.6913213133811951\n",
            "Iteration: 344740 \t Train: Acc=53%, Loss=0.6915543079376221 \t\t Validation: Acc=48%, Loss=0.6946304440498352\n",
            "Iteration: 344750 \t Train: Acc=49%, Loss=0.7039026021957397 \t\t Validation: Acc=47%, Loss=0.691422700881958\n",
            "Iteration: 344760 \t Train: Acc=44%, Loss=0.699344277381897 \t\t Validation: Acc=48%, Loss=0.6974243521690369\n",
            "Iteration: 344770 \t Train: Acc=47%, Loss=0.6904104948043823 \t\t Validation: Acc=47%, Loss=0.6964417099952698\n",
            "Iteration: 344780 \t Train: Acc=46%, Loss=0.6948925256729126 \t\t Validation: Acc=46%, Loss=0.7002338171005249\n",
            "Iteration: 344790 \t Train: Acc=50%, Loss=0.6923165321350098 \t\t Validation: Acc=53%, Loss=0.6900229454040527\n",
            "Iteration: 344800 \t Train: Acc=47%, Loss=0.6951574087142944 \t\t Validation: Acc=53%, Loss=0.6906402111053467\n",
            "Iteration: 344810 \t Train: Acc=51%, Loss=0.6955324411392212 \t\t Validation: Acc=46%, Loss=0.696633517742157\n",
            "Iteration: 344820 \t Train: Acc=56%, Loss=0.69097900390625 \t\t Validation: Acc=53%, Loss=0.6898351311683655\n",
            "Iteration: 344830 \t Train: Acc=53%, Loss=0.6941754221916199 \t\t Validation: Acc=46%, Loss=0.6914472579956055\n",
            "Iteration: 344840 \t Train: Acc=51%, Loss=0.6862849593162537 \t\t Validation: Acc=52%, Loss=0.6924958229064941\n",
            "Iteration: 344850 \t Train: Acc=50%, Loss=0.6883804798126221 \t\t Validation: Acc=50%, Loss=0.6924692988395691\n",
            "Iteration: 344860 \t Train: Acc=54%, Loss=0.6893064379692078 \t\t Validation: Acc=46%, Loss=0.697858452796936\n",
            "Iteration: 344870 \t Train: Acc=54%, Loss=0.6885213851928711 \t\t Validation: Acc=53%, Loss=0.6883171796798706\n",
            "Iteration: 344880 \t Train: Acc=53%, Loss=0.6831921935081482 \t\t Validation: Acc=48%, Loss=0.69461989402771\n",
            "Iteration: 344890 \t Train: Acc=53%, Loss=0.6904945373535156 \t\t Validation: Acc=50%, Loss=0.693411648273468\n",
            "Iteration: 344900 \t Train: Acc=56%, Loss=0.6915680170059204 \t\t Validation: Acc=51%, Loss=0.6967359185218811\n",
            "Iteration: 344910 \t Train: Acc=51%, Loss=0.7002472877502441 \t\t Validation: Acc=56%, Loss=0.6865816116333008\n",
            "Iteration: 344920 \t Train: Acc=51%, Loss=0.6883752346038818 \t\t Validation: Acc=52%, Loss=0.6899598836898804\n",
            "Iteration: 344930 \t Train: Acc=51%, Loss=0.6916103363037109 \t\t Validation: Acc=52%, Loss=0.6926329731941223\n",
            "Iteration: 344940 \t Train: Acc=46%, Loss=0.6941946744918823 \t\t Validation: Acc=51%, Loss=0.6904988288879395\n",
            "Iteration: 344950 \t Train: Acc=50%, Loss=0.6911375522613525 \t\t Validation: Acc=50%, Loss=0.690619707107544\n",
            "Iteration: 344960 \t Train: Acc=48%, Loss=0.6924434304237366 \t\t Validation: Acc=52%, Loss=0.6935595273971558\n",
            "Iteration: 344970 \t Train: Acc=48%, Loss=0.6917949318885803 \t\t Validation: Acc=52%, Loss=0.6921622157096863\n",
            "Iteration: 344980 \t Train: Acc=49%, Loss=0.6883833408355713 \t\t Validation: Acc=50%, Loss=0.6960732936859131\n",
            "Iteration: 344990 \t Train: Acc=53%, Loss=0.6889600157737732 \t\t Validation: Acc=50%, Loss=0.691366970539093\n",
            "Iteration: 345000 \t Train: Acc=52%, Loss=0.6906701326370239 \t\t Validation: Acc=50%, Loss=0.6927657723426819\n",
            "Iteration: 345010 \t Train: Acc=47%, Loss=0.6940463781356812 \t\t Validation: Acc=51%, Loss=0.6926345825195312\n",
            "Iteration: 345020 \t Train: Acc=53%, Loss=0.6958187818527222 \t\t Validation: Acc=50%, Loss=0.6969491243362427\n",
            "Iteration: 345030 \t Train: Acc=53%, Loss=0.6943607926368713 \t\t Validation: Acc=50%, Loss=0.69403475522995\n",
            "Iteration: 345040 \t Train: Acc=54%, Loss=0.6925053596496582 \t\t Validation: Acc=46%, Loss=0.6931627988815308\n",
            "Iteration: 345050 \t Train: Acc=50%, Loss=0.6912792921066284 \t\t Validation: Acc=52%, Loss=0.6879227161407471\n",
            "Iteration: 345060 \t Train: Acc=52%, Loss=0.6885740160942078 \t\t Validation: Acc=54%, Loss=0.6917052268981934\n",
            "Iteration: 345070 \t Train: Acc=47%, Loss=0.6916497945785522 \t\t Validation: Acc=55%, Loss=0.6907260417938232\n",
            "Iteration: 345080 \t Train: Acc=47%, Loss=0.6929852962493896 \t\t Validation: Acc=47%, Loss=0.6960504055023193\n",
            "Iteration: 345090 \t Train: Acc=60%, Loss=0.6845776438713074 \t\t Validation: Acc=53%, Loss=0.6916526556015015\n",
            "Iteration: 345100 \t Train: Acc=48%, Loss=0.6923152208328247 \t\t Validation: Acc=53%, Loss=0.69124436378479\n",
            "Iteration: 345110 \t Train: Acc=55%, Loss=0.686836838722229 \t\t Validation: Acc=52%, Loss=0.6856231689453125\n",
            "Iteration: 345120 \t Train: Acc=50%, Loss=0.6914465427398682 \t\t Validation: Acc=46%, Loss=0.6939868330955505\n",
            "Iteration: 345130 \t Train: Acc=50%, Loss=0.6959545612335205 \t\t Validation: Acc=49%, Loss=0.6915384531021118\n",
            "Iteration: 345140 \t Train: Acc=49%, Loss=0.6925307512283325 \t\t Validation: Acc=53%, Loss=0.6934152841567993\n",
            "Iteration: 345150 \t Train: Acc=51%, Loss=0.6858710050582886 \t\t Validation: Acc=52%, Loss=0.6886662244796753\n",
            "Iteration: 345160 \t Train: Acc=53%, Loss=0.6932325959205627 \t\t Validation: Acc=50%, Loss=0.6906351447105408\n",
            "Iteration: 345170 \t Train: Acc=50%, Loss=0.6991326808929443 \t\t Validation: Acc=54%, Loss=0.6905717253684998\n",
            "Iteration: 345180 \t Train: Acc=42%, Loss=0.6976581811904907 \t\t Validation: Acc=47%, Loss=0.697158932685852\n",
            "Iteration: 345190 \t Train: Acc=49%, Loss=0.6934970617294312 \t\t Validation: Acc=49%, Loss=0.6946601867675781\n",
            "Iteration: 345200 \t Train: Acc=56%, Loss=0.691018283367157 \t\t Validation: Acc=52%, Loss=0.6938057541847229\n",
            "Iteration: 345210 \t Train: Acc=52%, Loss=0.6923297047615051 \t\t Validation: Acc=51%, Loss=0.6896982192993164\n",
            "Iteration: 345220 \t Train: Acc=53%, Loss=0.6861469745635986 \t\t Validation: Acc=53%, Loss=0.6919078826904297\n",
            "Iteration: 345230 \t Train: Acc=44%, Loss=0.701526939868927 \t\t Validation: Acc=51%, Loss=0.6890249252319336\n",
            "Iteration: 345240 \t Train: Acc=48%, Loss=0.6949512362480164 \t\t Validation: Acc=53%, Loss=0.6875415444374084\n",
            "Iteration: 345250 \t Train: Acc=40%, Loss=0.693382203578949 \t\t Validation: Acc=54%, Loss=0.688281774520874\n",
            "Iteration: 345260 \t Train: Acc=49%, Loss=0.686366856098175 \t\t Validation: Acc=51%, Loss=0.6910139322280884\n",
            "Iteration: 345270 \t Train: Acc=48%, Loss=0.7004179358482361 \t\t Validation: Acc=53%, Loss=0.6896526217460632\n",
            "Iteration: 345280 \t Train: Acc=53%, Loss=0.6923485994338989 \t\t Validation: Acc=50%, Loss=0.6907853484153748\n",
            "Iteration: 345290 \t Train: Acc=54%, Loss=0.690182089805603 \t\t Validation: Acc=57%, Loss=0.6857782602310181\n",
            "Iteration: 345300 \t Train: Acc=55%, Loss=0.688055694103241 \t\t Validation: Acc=52%, Loss=0.691978394985199\n",
            "Iteration: 345310 \t Train: Acc=50%, Loss=0.6946374177932739 \t\t Validation: Acc=50%, Loss=0.6904690265655518\n",
            "Iteration: 345320 \t Train: Acc=57%, Loss=0.6907265782356262 \t\t Validation: Acc=50%, Loss=0.6972503066062927\n",
            "Iteration: 345330 \t Train: Acc=53%, Loss=0.6900737285614014 \t\t Validation: Acc=51%, Loss=0.6903520822525024\n",
            "Iteration: 345340 \t Train: Acc=49%, Loss=0.6982380151748657 \t\t Validation: Acc=49%, Loss=0.6957131028175354\n",
            "Iteration: 345350 \t Train: Acc=44%, Loss=0.7043155431747437 \t\t Validation: Acc=50%, Loss=0.692198634147644\n",
            "Iteration: 345360 \t Train: Acc=49%, Loss=0.6958553194999695 \t\t Validation: Acc=50%, Loss=0.6904771327972412\n",
            "Iteration: 345370 \t Train: Acc=49%, Loss=0.6969996094703674 \t\t Validation: Acc=51%, Loss=0.6968333125114441\n",
            "Iteration: 345380 \t Train: Acc=49%, Loss=0.6874514222145081 \t\t Validation: Acc=52%, Loss=0.6849614977836609\n",
            "Iteration: 345390 \t Train: Acc=51%, Loss=0.6939786672592163 \t\t Validation: Acc=48%, Loss=0.6959846615791321\n",
            "Iteration: 345400 \t Train: Acc=54%, Loss=0.6978046298027039 \t\t Validation: Acc=51%, Loss=0.6948562860488892\n",
            "Iteration: 345410 \t Train: Acc=52%, Loss=0.6933421492576599 \t\t Validation: Acc=49%, Loss=0.6967794299125671\n",
            "Iteration: 345420 \t Train: Acc=48%, Loss=0.6881815791130066 \t\t Validation: Acc=46%, Loss=0.6921719312667847\n",
            "Iteration: 345430 \t Train: Acc=47%, Loss=0.6918412446975708 \t\t Validation: Acc=53%, Loss=0.6892275214195251\n",
            "Iteration: 345440 \t Train: Acc=42%, Loss=0.6967631578445435 \t\t Validation: Acc=49%, Loss=0.6955039501190186\n",
            "Iteration: 345450 \t Train: Acc=52%, Loss=0.6876186728477478 \t\t Validation: Acc=46%, Loss=0.6962020993232727\n",
            "Iteration: 345460 \t Train: Acc=44%, Loss=0.690860390663147 \t\t Validation: Acc=53%, Loss=0.6940072774887085\n",
            "Iteration: 345470 \t Train: Acc=56%, Loss=0.6852493286132812 \t\t Validation: Acc=53%, Loss=0.6871203780174255\n",
            "Iteration: 345480 \t Train: Acc=53%, Loss=0.6976392865180969 \t\t Validation: Acc=53%, Loss=0.6907327175140381\n",
            "Iteration: 345490 \t Train: Acc=49%, Loss=0.6901770830154419 \t\t Validation: Acc=57%, Loss=0.6911260485649109\n",
            "Iteration: 345500 \t Train: Acc=48%, Loss=0.7008660435676575 \t\t Validation: Acc=50%, Loss=0.6960123777389526\n",
            "Iteration: 345510 \t Train: Acc=53%, Loss=0.6859071254730225 \t\t Validation: Acc=53%, Loss=0.6907747387886047\n",
            "Iteration: 345520 \t Train: Acc=46%, Loss=0.6903648376464844 \t\t Validation: Acc=50%, Loss=0.6924469470977783\n",
            "Iteration: 345530 \t Train: Acc=52%, Loss=0.6853295564651489 \t\t Validation: Acc=53%, Loss=0.68943190574646\n",
            "Iteration: 345540 \t Train: Acc=53%, Loss=0.6912057399749756 \t\t Validation: Acc=48%, Loss=0.6922429203987122\n",
            "Iteration: 345550 \t Train: Acc=50%, Loss=0.6959487795829773 \t\t Validation: Acc=50%, Loss=0.6971541047096252\n",
            "Iteration: 345560 \t Train: Acc=52%, Loss=0.6907155513763428 \t\t Validation: Acc=47%, Loss=0.6942565441131592\n",
            "Iteration: 345570 \t Train: Acc=53%, Loss=0.6862805485725403 \t\t Validation: Acc=49%, Loss=0.7010549306869507\n",
            "Iteration: 345580 \t Train: Acc=52%, Loss=0.688984751701355 \t\t Validation: Acc=46%, Loss=0.6924972534179688\n",
            "Iteration: 345590 \t Train: Acc=49%, Loss=0.6940768957138062 \t\t Validation: Acc=48%, Loss=0.69629967212677\n",
            "Iteration: 345600 \t Train: Acc=56%, Loss=0.6907646656036377 \t\t Validation: Acc=49%, Loss=0.6978782415390015\n",
            "Iteration: 345610 \t Train: Acc=52%, Loss=0.6891315579414368 \t\t Validation: Acc=47%, Loss=0.6879130601882935\n",
            "Iteration: 345620 \t Train: Acc=57%, Loss=0.684017539024353 \t\t Validation: Acc=51%, Loss=0.6870710253715515\n",
            "Iteration: 345630 \t Train: Acc=51%, Loss=0.6915739178657532 \t\t Validation: Acc=51%, Loss=0.699872612953186\n",
            "Iteration: 345640 \t Train: Acc=52%, Loss=0.6886112689971924 \t\t Validation: Acc=49%, Loss=0.6932817697525024\n",
            "Iteration: 345650 \t Train: Acc=47%, Loss=0.6937603950500488 \t\t Validation: Acc=53%, Loss=0.6878266334533691\n",
            "Iteration: 345660 \t Train: Acc=47%, Loss=0.698462963104248 \t\t Validation: Acc=47%, Loss=0.690475344657898\n",
            "Iteration: 345670 \t Train: Acc=46%, Loss=0.6963101029396057 \t\t Validation: Acc=51%, Loss=0.6927947998046875\n",
            "Iteration: 345680 \t Train: Acc=55%, Loss=0.6938357949256897 \t\t Validation: Acc=53%, Loss=0.6888186931610107\n",
            "Iteration: 345690 \t Train: Acc=49%, Loss=0.6994249820709229 \t\t Validation: Acc=48%, Loss=0.6914058923721313\n",
            "Iteration: 345700 \t Train: Acc=49%, Loss=0.6984407305717468 \t\t Validation: Acc=51%, Loss=0.6978166103363037\n",
            "Iteration: 345710 \t Train: Acc=48%, Loss=0.6928446888923645 \t\t Validation: Acc=50%, Loss=0.6992791891098022\n",
            "Iteration: 345720 \t Train: Acc=47%, Loss=0.691762387752533 \t\t Validation: Acc=52%, Loss=0.6903718709945679\n",
            "Iteration: 345730 \t Train: Acc=50%, Loss=0.6884505748748779 \t\t Validation: Acc=51%, Loss=0.6862662434577942\n",
            "Iteration: 345740 \t Train: Acc=53%, Loss=0.6940770149230957 \t\t Validation: Acc=49%, Loss=0.6989952325820923\n",
            "Iteration: 345750 \t Train: Acc=49%, Loss=0.6906051635742188 \t\t Validation: Acc=53%, Loss=0.6912291049957275\n",
            "Iteration: 345760 \t Train: Acc=54%, Loss=0.686475932598114 \t\t Validation: Acc=56%, Loss=0.6867585778236389\n",
            "Iteration: 345770 \t Train: Acc=51%, Loss=0.6927709579467773 \t\t Validation: Acc=47%, Loss=0.7060467600822449\n",
            "Iteration: 345780 \t Train: Acc=52%, Loss=0.6978390216827393 \t\t Validation: Acc=51%, Loss=0.6991276741027832\n",
            "Iteration: 345790 \t Train: Acc=43%, Loss=0.7040740847587585 \t\t Validation: Acc=50%, Loss=0.696845531463623\n",
            "Iteration: 345800 \t Train: Acc=47%, Loss=0.6930636763572693 \t\t Validation: Acc=48%, Loss=0.6894044876098633\n",
            "Iteration: 345810 \t Train: Acc=50%, Loss=0.693870484828949 \t\t Validation: Acc=52%, Loss=0.6889451146125793\n",
            "Iteration: 345820 \t Train: Acc=47%, Loss=0.6990950703620911 \t\t Validation: Acc=50%, Loss=0.6947541236877441\n",
            "Iteration: 345830 \t Train: Acc=50%, Loss=0.6909650564193726 \t\t Validation: Acc=53%, Loss=0.6966491937637329\n",
            "Iteration: 345840 \t Train: Acc=51%, Loss=0.6858786940574646 \t\t Validation: Acc=46%, Loss=0.6918768882751465\n",
            "Iteration: 345850 \t Train: Acc=51%, Loss=0.689089298248291 \t\t Validation: Acc=55%, Loss=0.6864106059074402\n",
            "Iteration: 345860 \t Train: Acc=45%, Loss=0.6965420842170715 \t\t Validation: Acc=45%, Loss=0.6938697099685669\n",
            "Iteration: 345870 \t Train: Acc=53%, Loss=0.6877054572105408 \t\t Validation: Acc=50%, Loss=0.6905149221420288\n",
            "Iteration: 345880 \t Train: Acc=53%, Loss=0.6906759738922119 \t\t Validation: Acc=56%, Loss=0.687899112701416\n",
            "Iteration: 345890 \t Train: Acc=52%, Loss=0.6946088075637817 \t\t Validation: Acc=57%, Loss=0.6939427852630615\n",
            "Iteration: 345900 \t Train: Acc=53%, Loss=0.6867148876190186 \t\t Validation: Acc=49%, Loss=0.6931232810020447\n",
            "Iteration: 345910 \t Train: Acc=54%, Loss=0.6891001462936401 \t\t Validation: Acc=50%, Loss=0.6912024617195129\n",
            "Iteration: 345920 \t Train: Acc=50%, Loss=0.6970164775848389 \t\t Validation: Acc=50%, Loss=0.690744936466217\n",
            "Iteration: 345930 \t Train: Acc=52%, Loss=0.6964337229728699 \t\t Validation: Acc=53%, Loss=0.6938039064407349\n",
            "Iteration: 345940 \t Train: Acc=52%, Loss=0.6858961582183838 \t\t Validation: Acc=48%, Loss=0.6919807195663452\n",
            "Iteration: 345950 \t Train: Acc=50%, Loss=0.6995835900306702 \t\t Validation: Acc=49%, Loss=0.692472517490387\n",
            "Iteration: 345960 \t Train: Acc=52%, Loss=0.6885089874267578 \t\t Validation: Acc=53%, Loss=0.6887978315353394\n",
            "Iteration: 345970 \t Train: Acc=51%, Loss=0.6935835480690002 \t\t Validation: Acc=46%, Loss=0.699067234992981\n",
            "Iteration: 345980 \t Train: Acc=52%, Loss=0.6884509921073914 \t\t Validation: Acc=51%, Loss=0.6887505650520325\n",
            "Iteration: 345990 \t Train: Acc=53%, Loss=0.6859158277511597 \t\t Validation: Acc=50%, Loss=0.6847573518753052\n",
            "Iteration: 346000 \t Train: Acc=51%, Loss=0.6956170201301575 \t\t Validation: Acc=53%, Loss=0.6869654655456543\n",
            "Iteration: 346010 \t Train: Acc=57%, Loss=0.6821576952934265 \t\t Validation: Acc=50%, Loss=0.6859867572784424\n",
            "Iteration: 346020 \t Train: Acc=59%, Loss=0.6894183158874512 \t\t Validation: Acc=46%, Loss=0.693473756313324\n",
            "Iteration: 346030 \t Train: Acc=48%, Loss=0.6967830657958984 \t\t Validation: Acc=50%, Loss=0.6949160695075989\n",
            "Iteration: 346040 \t Train: Acc=51%, Loss=0.6856110095977783 \t\t Validation: Acc=49%, Loss=0.6902071237564087\n",
            "Iteration: 346050 \t Train: Acc=50%, Loss=0.6930211186408997 \t\t Validation: Acc=51%, Loss=0.6918198466300964\n",
            "Iteration: 346060 \t Train: Acc=52%, Loss=0.6888902187347412 \t\t Validation: Acc=49%, Loss=0.6942440271377563\n",
            "Iteration: 346070 \t Train: Acc=50%, Loss=0.6907436847686768 \t\t Validation: Acc=53%, Loss=0.6897543668746948\n",
            "Iteration: 346080 \t Train: Acc=51%, Loss=0.6871151924133301 \t\t Validation: Acc=50%, Loss=0.6930680274963379\n",
            "Iteration: 346090 \t Train: Acc=53%, Loss=0.69293212890625 \t\t Validation: Acc=56%, Loss=0.6892213821411133\n",
            "Iteration: 346100 \t Train: Acc=52%, Loss=0.6985944509506226 \t\t Validation: Acc=53%, Loss=0.6909711360931396\n",
            "Iteration: 346110 \t Train: Acc=47%, Loss=0.692535400390625 \t\t Validation: Acc=51%, Loss=0.6941568851470947\n",
            "Iteration: 346120 \t Train: Acc=48%, Loss=0.6954094171524048 \t\t Validation: Acc=53%, Loss=0.6907558441162109\n",
            "Iteration: 346130 \t Train: Acc=50%, Loss=0.6919741034507751 \t\t Validation: Acc=50%, Loss=0.6950199604034424\n",
            "Iteration: 346140 \t Train: Acc=57%, Loss=0.6878746747970581 \t\t Validation: Acc=51%, Loss=0.6892099380493164\n",
            "Iteration: 346150 \t Train: Acc=48%, Loss=0.6810427308082581 \t\t Validation: Acc=52%, Loss=0.6901418566703796\n",
            "Iteration: 346160 \t Train: Acc=42%, Loss=0.6982212066650391 \t\t Validation: Acc=53%, Loss=0.687319815158844\n",
            "Iteration: 346170 \t Train: Acc=48%, Loss=0.6880093812942505 \t\t Validation: Acc=53%, Loss=0.6939576864242554\n",
            "Iteration: 346180 \t Train: Acc=46%, Loss=0.6957117915153503 \t\t Validation: Acc=51%, Loss=0.6911527514457703\n",
            "Iteration: 346190 \t Train: Acc=50%, Loss=0.6944775581359863 \t\t Validation: Acc=53%, Loss=0.6858811974525452\n",
            "Iteration: 346200 \t Train: Acc=53%, Loss=0.6872598528862 \t\t Validation: Acc=46%, Loss=0.6966671347618103\n",
            "Iteration: 346210 \t Train: Acc=56%, Loss=0.6868870854377747 \t\t Validation: Acc=48%, Loss=0.6970111727714539\n",
            "Iteration: 346220 \t Train: Acc=52%, Loss=0.6863014698028564 \t\t Validation: Acc=50%, Loss=0.6929887533187866\n",
            "Iteration: 346230 \t Train: Acc=50%, Loss=0.6894102692604065 \t\t Validation: Acc=51%, Loss=0.6921523213386536\n",
            "Iteration: 346240 \t Train: Acc=51%, Loss=0.695527970790863 \t\t Validation: Acc=51%, Loss=0.6938536167144775\n",
            "Iteration: 346250 \t Train: Acc=45%, Loss=0.6944839358329773 \t\t Validation: Acc=50%, Loss=0.6848951578140259\n",
            "Iteration: 346260 \t Train: Acc=51%, Loss=0.6821428537368774 \t\t Validation: Acc=49%, Loss=0.7005640864372253\n",
            "Iteration: 346270 \t Train: Acc=52%, Loss=0.6931294202804565 \t\t Validation: Acc=53%, Loss=0.6883720755577087\n",
            "Iteration: 346280 \t Train: Acc=59%, Loss=0.6933144927024841 \t\t Validation: Acc=50%, Loss=0.6932964324951172\n",
            "Iteration: 346290 \t Train: Acc=55%, Loss=0.689551830291748 \t\t Validation: Acc=52%, Loss=0.6924001574516296\n",
            "Iteration: 346300 \t Train: Acc=50%, Loss=0.6949542164802551 \t\t Validation: Acc=56%, Loss=0.6895889043807983\n",
            "Iteration: 346310 \t Train: Acc=53%, Loss=0.6988896727561951 \t\t Validation: Acc=45%, Loss=0.6947616934776306\n",
            "Iteration: 346320 \t Train: Acc=51%, Loss=0.6924367547035217 \t\t Validation: Acc=48%, Loss=0.692366898059845\n",
            "Iteration: 346330 \t Train: Acc=44%, Loss=0.6993510127067566 \t\t Validation: Acc=51%, Loss=0.691154956817627\n",
            "Iteration: 346340 \t Train: Acc=51%, Loss=0.6915881037712097 \t\t Validation: Acc=51%, Loss=0.688423752784729\n",
            "Iteration: 346350 \t Train: Acc=52%, Loss=0.6955687999725342 \t\t Validation: Acc=49%, Loss=0.6961814165115356\n",
            "Iteration: 346360 \t Train: Acc=56%, Loss=0.6903415322303772 \t\t Validation: Acc=52%, Loss=0.688398540019989\n",
            "Iteration: 346370 \t Train: Acc=58%, Loss=0.6904033422470093 \t\t Validation: Acc=50%, Loss=0.6909898519515991\n",
            "Iteration: 346380 \t Train: Acc=47%, Loss=0.6962447166442871 \t\t Validation: Acc=53%, Loss=0.6970025897026062\n",
            "Iteration: 346390 \t Train: Acc=54%, Loss=0.6928390860557556 \t\t Validation: Acc=48%, Loss=0.6954163908958435\n",
            "Iteration: 346400 \t Train: Acc=58%, Loss=0.6858825087547302 \t\t Validation: Acc=49%, Loss=0.6963824033737183\n",
            "Iteration: 346410 \t Train: Acc=50%, Loss=0.6872293949127197 \t\t Validation: Acc=50%, Loss=0.6941561698913574\n",
            "Iteration: 346420 \t Train: Acc=52%, Loss=0.6949495673179626 \t\t Validation: Acc=52%, Loss=0.6894500851631165\n",
            "Iteration: 346430 \t Train: Acc=50%, Loss=0.693675696849823 \t\t Validation: Acc=49%, Loss=0.6891821026802063\n",
            "Iteration: 346440 \t Train: Acc=52%, Loss=0.6887375712394714 \t\t Validation: Acc=50%, Loss=0.6881439685821533\n",
            "Iteration: 346450 \t Train: Acc=52%, Loss=0.6956530809402466 \t\t Validation: Acc=49%, Loss=0.691785454750061\n",
            "Iteration: 346460 \t Train: Acc=50%, Loss=0.6909776926040649 \t\t Validation: Acc=50%, Loss=0.6941378116607666\n",
            "Iteration: 346470 \t Train: Acc=50%, Loss=0.6931231021881104 \t\t Validation: Acc=47%, Loss=0.6936358213424683\n",
            "Iteration: 346480 \t Train: Acc=48%, Loss=0.6928313374519348 \t\t Validation: Acc=47%, Loss=0.6944423913955688\n",
            "Iteration: 346490 \t Train: Acc=59%, Loss=0.6906140446662903 \t\t Validation: Acc=50%, Loss=0.6947898268699646\n",
            "Iteration: 346500 \t Train: Acc=50%, Loss=0.6890406608581543 \t\t Validation: Acc=53%, Loss=0.6900428533554077\n",
            "Iteration: 346510 \t Train: Acc=51%, Loss=0.6933251619338989 \t\t Validation: Acc=56%, Loss=0.6979194283485413\n",
            "Iteration: 346520 \t Train: Acc=48%, Loss=0.689991295337677 \t\t Validation: Acc=50%, Loss=0.687637209892273\n",
            "Iteration: 346530 \t Train: Acc=54%, Loss=0.6825725436210632 \t\t Validation: Acc=46%, Loss=0.694617748260498\n",
            "Iteration: 346540 \t Train: Acc=53%, Loss=0.6871890425682068 \t\t Validation: Acc=54%, Loss=0.6897985935211182\n",
            "Iteration: 346550 \t Train: Acc=57%, Loss=0.6859923005104065 \t\t Validation: Acc=56%, Loss=0.6833409070968628\n",
            "Iteration: 346560 \t Train: Acc=57%, Loss=0.6889917850494385 \t\t Validation: Acc=53%, Loss=0.6818916201591492\n",
            "Iteration: 346570 \t Train: Acc=45%, Loss=0.6981170773506165 \t\t Validation: Acc=48%, Loss=0.6913541555404663\n",
            "Iteration: 346580 \t Train: Acc=53%, Loss=0.6861366033554077 \t\t Validation: Acc=47%, Loss=0.6913671493530273\n",
            "Iteration: 346590 \t Train: Acc=49%, Loss=0.6918094754219055 \t\t Validation: Acc=47%, Loss=0.6953250169754028\n",
            "Iteration: 346600 \t Train: Acc=50%, Loss=0.6948318481445312 \t\t Validation: Acc=53%, Loss=0.6946460008621216\n",
            "Iteration: 346610 \t Train: Acc=46%, Loss=0.7000212669372559 \t\t Validation: Acc=53%, Loss=0.6984211206436157\n",
            "Iteration: 346620 \t Train: Acc=53%, Loss=0.6941017508506775 \t\t Validation: Acc=49%, Loss=0.6950758099555969\n",
            "Iteration: 346630 \t Train: Acc=53%, Loss=0.6929599642753601 \t\t Validation: Acc=49%, Loss=0.6907479763031006\n",
            "Iteration: 346640 \t Train: Acc=53%, Loss=0.6891506314277649 \t\t Validation: Acc=54%, Loss=0.6978814005851746\n",
            "Iteration: 346650 \t Train: Acc=50%, Loss=0.6972113847732544 \t\t Validation: Acc=52%, Loss=0.6935727000236511\n",
            "Iteration: 346660 \t Train: Acc=50%, Loss=0.690643310546875 \t\t Validation: Acc=50%, Loss=0.6923980712890625\n",
            "Iteration: 346670 \t Train: Acc=46%, Loss=0.6956406235694885 \t\t Validation: Acc=46%, Loss=0.684118926525116\n",
            "Iteration: 346680 \t Train: Acc=48%, Loss=0.6932142972946167 \t\t Validation: Acc=51%, Loss=0.6948724985122681\n",
            "Iteration: 346690 \t Train: Acc=54%, Loss=0.6886098384857178 \t\t Validation: Acc=53%, Loss=0.6877729892730713\n",
            "Iteration: 346700 \t Train: Acc=52%, Loss=0.6894046068191528 \t\t Validation: Acc=51%, Loss=0.6881835460662842\n",
            "Iteration: 346710 \t Train: Acc=57%, Loss=0.6852754354476929 \t\t Validation: Acc=53%, Loss=0.6910829544067383\n",
            "Iteration: 346720 \t Train: Acc=52%, Loss=0.6927305459976196 \t\t Validation: Acc=52%, Loss=0.6873989701271057\n",
            "Iteration: 346730 \t Train: Acc=49%, Loss=0.7018303275108337 \t\t Validation: Acc=51%, Loss=0.6867772340774536\n",
            "Iteration: 346740 \t Train: Acc=50%, Loss=0.6931747794151306 \t\t Validation: Acc=52%, Loss=0.6871175169944763\n",
            "Iteration: 346750 \t Train: Acc=55%, Loss=0.6834335327148438 \t\t Validation: Acc=50%, Loss=0.6934269666671753\n",
            "Iteration: 346760 \t Train: Acc=48%, Loss=0.691925048828125 \t\t Validation: Acc=53%, Loss=0.6991502046585083\n",
            "Iteration: 346770 \t Train: Acc=46%, Loss=0.6910266876220703 \t\t Validation: Acc=49%, Loss=0.6923172473907471\n",
            "Iteration: 346780 \t Train: Acc=54%, Loss=0.6843820810317993 \t\t Validation: Acc=51%, Loss=0.6925229430198669\n",
            "Iteration: 346790 \t Train: Acc=46%, Loss=0.6913931965827942 \t\t Validation: Acc=47%, Loss=0.6957882642745972\n",
            "Iteration: 346800 \t Train: Acc=54%, Loss=0.6990355849266052 \t\t Validation: Acc=50%, Loss=0.688571572303772\n",
            "Iteration: 346810 \t Train: Acc=49%, Loss=0.6912738084793091 \t\t Validation: Acc=47%, Loss=0.6927079558372498\n",
            "Iteration: 346820 \t Train: Acc=46%, Loss=0.6964538097381592 \t\t Validation: Acc=42%, Loss=0.6949474215507507\n",
            "Iteration: 346830 \t Train: Acc=55%, Loss=0.69622802734375 \t\t Validation: Acc=48%, Loss=0.6969558000564575\n",
            "Iteration: 346840 \t Train: Acc=53%, Loss=0.6929004192352295 \t\t Validation: Acc=50%, Loss=0.6940261125564575\n",
            "Iteration: 346850 \t Train: Acc=52%, Loss=0.6943840384483337 \t\t Validation: Acc=50%, Loss=0.691007673740387\n",
            "Iteration: 346860 \t Train: Acc=44%, Loss=0.6971219182014465 \t\t Validation: Acc=49%, Loss=0.689171552658081\n",
            "Iteration: 346870 \t Train: Acc=55%, Loss=0.68827885389328 \t\t Validation: Acc=53%, Loss=0.6947337985038757\n",
            "Iteration: 346880 \t Train: Acc=51%, Loss=0.6894884705543518 \t\t Validation: Acc=55%, Loss=0.6840551495552063\n",
            "Iteration: 346890 \t Train: Acc=54%, Loss=0.6829527616500854 \t\t Validation: Acc=50%, Loss=0.6905806064605713\n",
            "Iteration: 346900 \t Train: Acc=52%, Loss=0.6879760026931763 \t\t Validation: Acc=53%, Loss=0.6906532645225525\n",
            "Iteration: 346910 \t Train: Acc=49%, Loss=0.6931765079498291 \t\t Validation: Acc=44%, Loss=0.7039014101028442\n",
            "Iteration: 346920 \t Train: Acc=48%, Loss=0.6919267773628235 \t\t Validation: Acc=52%, Loss=0.6893835663795471\n",
            "Iteration: 346930 \t Train: Acc=50%, Loss=0.6891748309135437 \t\t Validation: Acc=56%, Loss=0.6919997930526733\n",
            "Iteration: 346940 \t Train: Acc=53%, Loss=0.6912146806716919 \t\t Validation: Acc=52%, Loss=0.69171541929245\n",
            "Iteration: 346950 \t Train: Acc=55%, Loss=0.6830193400382996 \t\t Validation: Acc=51%, Loss=0.6938372254371643\n",
            "Iteration: 346960 \t Train: Acc=53%, Loss=0.694017767906189 \t\t Validation: Acc=46%, Loss=0.6936386227607727\n",
            "Iteration: 346970 \t Train: Acc=49%, Loss=0.6917530298233032 \t\t Validation: Acc=52%, Loss=0.6927293539047241\n",
            "Iteration: 346980 \t Train: Acc=51%, Loss=0.6930569410324097 \t\t Validation: Acc=49%, Loss=0.693878173828125\n",
            "Iteration: 346990 \t Train: Acc=54%, Loss=0.682296872138977 \t\t Validation: Acc=52%, Loss=0.6873688697814941\n",
            "Iteration: 347000 \t Train: Acc=46%, Loss=0.6922577023506165 \t\t Validation: Acc=49%, Loss=0.6911432147026062\n",
            "Iteration: 347010 \t Train: Acc=51%, Loss=0.6937821507453918 \t\t Validation: Acc=52%, Loss=0.6920739412307739\n",
            "Iteration: 347020 \t Train: Acc=49%, Loss=0.6925761699676514 \t\t Validation: Acc=46%, Loss=0.698530912399292\n",
            "Iteration: 347030 \t Train: Acc=46%, Loss=0.6880042552947998 \t\t Validation: Acc=50%, Loss=0.6863171458244324\n",
            "Iteration: 347040 \t Train: Acc=50%, Loss=0.6930540800094604 \t\t Validation: Acc=51%, Loss=0.690191924571991\n",
            "Iteration: 347050 \t Train: Acc=51%, Loss=0.6976866126060486 \t\t Validation: Acc=46%, Loss=0.6943967938423157\n",
            "Iteration: 347060 \t Train: Acc=49%, Loss=0.691332221031189 \t\t Validation: Acc=52%, Loss=0.6866931319236755\n",
            "Iteration: 347070 \t Train: Acc=50%, Loss=0.6830241680145264 \t\t Validation: Acc=49%, Loss=0.6905522346496582\n",
            "Iteration: 347080 \t Train: Acc=51%, Loss=0.6912837028503418 \t\t Validation: Acc=53%, Loss=0.6967693567276001\n",
            "Iteration: 347090 \t Train: Acc=49%, Loss=0.6942064762115479 \t\t Validation: Acc=55%, Loss=0.6908929347991943\n",
            "Iteration: 347100 \t Train: Acc=50%, Loss=0.6932085156440735 \t\t Validation: Acc=47%, Loss=0.6901472806930542\n",
            "Iteration: 347110 \t Train: Acc=44%, Loss=0.6906836032867432 \t\t Validation: Acc=55%, Loss=0.6983586549758911\n",
            "Iteration: 347120 \t Train: Acc=55%, Loss=0.6902068853378296 \t\t Validation: Acc=47%, Loss=0.6985312104225159\n",
            "Iteration: 347130 \t Train: Acc=52%, Loss=0.6849031448364258 \t\t Validation: Acc=53%, Loss=0.693801760673523\n",
            "Iteration: 347140 \t Train: Acc=52%, Loss=0.6954838633537292 \t\t Validation: Acc=51%, Loss=0.6989628076553345\n",
            "Iteration: 347150 \t Train: Acc=53%, Loss=0.6961244940757751 \t\t Validation: Acc=51%, Loss=0.6907825469970703\n",
            "Iteration: 347160 \t Train: Acc=51%, Loss=0.6877769827842712 \t\t Validation: Acc=52%, Loss=0.6925193071365356\n",
            "Iteration: 347170 \t Train: Acc=46%, Loss=0.692499041557312 \t\t Validation: Acc=50%, Loss=0.6835851669311523\n",
            "Iteration: 347180 \t Train: Acc=53%, Loss=0.6909686326980591 \t\t Validation: Acc=50%, Loss=0.6936378479003906\n",
            "Iteration: 347190 \t Train: Acc=50%, Loss=0.6974394917488098 \t\t Validation: Acc=48%, Loss=0.6952918767929077\n",
            "Iteration: 347200 \t Train: Acc=52%, Loss=0.6890525817871094 \t\t Validation: Acc=50%, Loss=0.6943332552909851\n",
            "Iteration: 347210 \t Train: Acc=50%, Loss=0.6902359127998352 \t\t Validation: Acc=48%, Loss=0.6932423710823059\n",
            "Iteration: 347220 \t Train: Acc=46%, Loss=0.6964247822761536 \t\t Validation: Acc=53%, Loss=0.6882243156433105\n",
            "Iteration: 347230 \t Train: Acc=51%, Loss=0.6965208649635315 \t\t Validation: Acc=50%, Loss=0.6932483911514282\n",
            "Iteration: 347240 \t Train: Acc=49%, Loss=0.6964555382728577 \t\t Validation: Acc=51%, Loss=0.689913272857666\n",
            "Iteration: 347250 \t Train: Acc=50%, Loss=0.6917268633842468 \t\t Validation: Acc=51%, Loss=0.6888955235481262\n",
            "Iteration: 347260 \t Train: Acc=58%, Loss=0.6915706992149353 \t\t Validation: Acc=53%, Loss=0.6956185698509216\n",
            "Iteration: 347270 \t Train: Acc=50%, Loss=0.6951816082000732 \t\t Validation: Acc=46%, Loss=0.7009375095367432\n",
            "Iteration: 347280 \t Train: Acc=50%, Loss=0.6921229958534241 \t\t Validation: Acc=50%, Loss=0.6947664022445679\n",
            "Iteration: 347290 \t Train: Acc=53%, Loss=0.6931518912315369 \t\t Validation: Acc=45%, Loss=0.6963614225387573\n",
            "Iteration: 347300 \t Train: Acc=46%, Loss=0.691277801990509 \t\t Validation: Acc=46%, Loss=0.6944344639778137\n",
            "Iteration: 347310 \t Train: Acc=50%, Loss=0.6895630955696106 \t\t Validation: Acc=56%, Loss=0.6896288394927979\n",
            "Iteration: 347320 \t Train: Acc=53%, Loss=0.6912366151809692 \t\t Validation: Acc=50%, Loss=0.6939422488212585\n",
            "Iteration: 347330 \t Train: Acc=51%, Loss=0.6900522708892822 \t\t Validation: Acc=54%, Loss=0.6907137036323547\n",
            "Iteration: 347340 \t Train: Acc=46%, Loss=0.6860541701316833 \t\t Validation: Acc=50%, Loss=0.6921025514602661\n",
            "Iteration: 347350 \t Train: Acc=52%, Loss=0.6886504888534546 \t\t Validation: Acc=50%, Loss=0.6892159581184387\n",
            "Iteration: 347360 \t Train: Acc=51%, Loss=0.690256655216217 \t\t Validation: Acc=52%, Loss=0.683556079864502\n",
            "Iteration: 347370 \t Train: Acc=48%, Loss=0.6937888264656067 \t\t Validation: Acc=48%, Loss=0.6935452222824097\n",
            "Iteration: 347380 \t Train: Acc=46%, Loss=0.7001873850822449 \t\t Validation: Acc=53%, Loss=0.6898228526115417\n",
            "Iteration: 347390 \t Train: Acc=53%, Loss=0.6907906532287598 \t\t Validation: Acc=53%, Loss=0.6919302940368652\n",
            "Iteration: 347400 \t Train: Acc=49%, Loss=0.6927403807640076 \t\t Validation: Acc=48%, Loss=0.6914726495742798\n",
            "Iteration: 347410 \t Train: Acc=53%, Loss=0.6974786520004272 \t\t Validation: Acc=46%, Loss=0.6920402646064758\n",
            "Iteration: 347420 \t Train: Acc=52%, Loss=0.6959443092346191 \t\t Validation: Acc=54%, Loss=0.6924598813056946\n",
            "Iteration: 347430 \t Train: Acc=55%, Loss=0.6761072874069214 \t\t Validation: Acc=51%, Loss=0.6907415986061096\n",
            "Iteration: 347440 \t Train: Acc=51%, Loss=0.6929328441619873 \t\t Validation: Acc=47%, Loss=0.6960335969924927\n",
            "Iteration: 347450 \t Train: Acc=52%, Loss=0.689339280128479 \t\t Validation: Acc=46%, Loss=0.6943356394767761\n",
            "Iteration: 347460 \t Train: Acc=52%, Loss=0.697455108165741 \t\t Validation: Acc=58%, Loss=0.6900851726531982\n",
            "Iteration: 347470 \t Train: Acc=54%, Loss=0.6870244741439819 \t\t Validation: Acc=52%, Loss=0.693856418132782\n",
            "Iteration: 347480 \t Train: Acc=53%, Loss=0.6955845952033997 \t\t Validation: Acc=52%, Loss=0.6956074833869934\n",
            "Iteration: 347490 \t Train: Acc=45%, Loss=0.6973727941513062 \t\t Validation: Acc=51%, Loss=0.6892240643501282\n",
            "Iteration: 347500 \t Train: Acc=53%, Loss=0.6847005486488342 \t\t Validation: Acc=48%, Loss=0.6931971311569214\n",
            "Iteration: 347510 \t Train: Acc=53%, Loss=0.6894466280937195 \t\t Validation: Acc=52%, Loss=0.6947103142738342\n",
            "Iteration: 347520 \t Train: Acc=49%, Loss=0.6895807981491089 \t\t Validation: Acc=55%, Loss=0.6916141510009766\n",
            "Iteration: 347530 \t Train: Acc=50%, Loss=0.6980960369110107 \t\t Validation: Acc=48%, Loss=0.6920051574707031\n",
            "Iteration: 347540 \t Train: Acc=48%, Loss=0.6947598457336426 \t\t Validation: Acc=43%, Loss=0.6962693929672241\n",
            "Iteration: 347550 \t Train: Acc=53%, Loss=0.6864635348320007 \t\t Validation: Acc=51%, Loss=0.6960615515708923\n",
            "Iteration: 347560 \t Train: Acc=47%, Loss=0.6949870586395264 \t\t Validation: Acc=52%, Loss=0.6931909918785095\n",
            "Iteration: 347570 \t Train: Acc=49%, Loss=0.6933267116546631 \t\t Validation: Acc=53%, Loss=0.6921839714050293\n",
            "Iteration: 347580 \t Train: Acc=50%, Loss=0.6896672248840332 \t\t Validation: Acc=42%, Loss=0.6977845430374146\n",
            "Iteration: 347590 \t Train: Acc=52%, Loss=0.6867279410362244 \t\t Validation: Acc=53%, Loss=0.6938291788101196\n",
            "Iteration: 347600 \t Train: Acc=50%, Loss=0.6892824172973633 \t\t Validation: Acc=52%, Loss=0.6846425533294678\n",
            "Iteration: 347610 \t Train: Acc=50%, Loss=0.6895338892936707 \t\t Validation: Acc=50%, Loss=0.6883150339126587\n",
            "Iteration: 347620 \t Train: Acc=50%, Loss=0.6956772208213806 \t\t Validation: Acc=53%, Loss=0.6935084462165833\n",
            "Iteration: 347630 \t Train: Acc=53%, Loss=0.6833710670471191 \t\t Validation: Acc=46%, Loss=0.691826343536377\n",
            "Iteration: 347640 \t Train: Acc=51%, Loss=0.687124490737915 \t\t Validation: Acc=51%, Loss=0.6890590190887451\n",
            "Iteration: 347650 \t Train: Acc=48%, Loss=0.6864749193191528 \t\t Validation: Acc=50%, Loss=0.692122220993042\n",
            "Iteration: 347660 \t Train: Acc=47%, Loss=0.6941152811050415 \t\t Validation: Acc=53%, Loss=0.6912065744400024\n",
            "Iteration: 347670 \t Train: Acc=46%, Loss=0.7012122869491577 \t\t Validation: Acc=47%, Loss=0.6907365322113037\n",
            "Iteration: 347680 \t Train: Acc=54%, Loss=0.6849303245544434 \t\t Validation: Acc=50%, Loss=0.6951431632041931\n",
            "Iteration: 347690 \t Train: Acc=51%, Loss=0.6891849637031555 \t\t Validation: Acc=51%, Loss=0.6936324834823608\n",
            "Iteration: 347700 \t Train: Acc=51%, Loss=0.6843529343605042 \t\t Validation: Acc=52%, Loss=0.6905232667922974\n",
            "Iteration: 347710 \t Train: Acc=50%, Loss=0.69106125831604 \t\t Validation: Acc=48%, Loss=0.6928814649581909\n",
            "Iteration: 347720 \t Train: Acc=52%, Loss=0.6940495371818542 \t\t Validation: Acc=51%, Loss=0.6896166205406189\n",
            "Iteration: 347730 \t Train: Acc=53%, Loss=0.6911422610282898 \t\t Validation: Acc=49%, Loss=0.6938012838363647\n",
            "Iteration: 347740 \t Train: Acc=53%, Loss=0.6833245158195496 \t\t Validation: Acc=52%, Loss=0.6883227825164795\n",
            "Iteration: 347750 \t Train: Acc=52%, Loss=0.6956578493118286 \t\t Validation: Acc=52%, Loss=0.6908782720565796\n",
            "Iteration: 347760 \t Train: Acc=50%, Loss=0.6926723122596741 \t\t Validation: Acc=52%, Loss=0.6909406781196594\n",
            "Iteration: 347770 \t Train: Acc=50%, Loss=0.6912686824798584 \t\t Validation: Acc=48%, Loss=0.6976555585861206\n",
            "Iteration: 347780 \t Train: Acc=46%, Loss=0.7033352851867676 \t\t Validation: Acc=49%, Loss=0.6921824216842651\n",
            "Iteration: 347790 \t Train: Acc=51%, Loss=0.6913504004478455 \t\t Validation: Acc=54%, Loss=0.6974948048591614\n",
            "Iteration: 347800 \t Train: Acc=53%, Loss=0.6953524947166443 \t\t Validation: Acc=54%, Loss=0.6909184455871582\n",
            "Iteration: 347810 \t Train: Acc=50%, Loss=0.690174400806427 \t\t Validation: Acc=53%, Loss=0.6915453672409058\n",
            "Iteration: 347820 \t Train: Acc=49%, Loss=0.6885622143745422 \t\t Validation: Acc=56%, Loss=0.6909661889076233\n",
            "Iteration: 347830 \t Train: Acc=51%, Loss=0.6902589797973633 \t\t Validation: Acc=51%, Loss=0.6929801106452942\n",
            "Iteration: 347840 \t Train: Acc=46%, Loss=0.6927462816238403 \t\t Validation: Acc=51%, Loss=0.693707287311554\n",
            "Iteration: 347850 \t Train: Acc=49%, Loss=0.6952840089797974 \t\t Validation: Acc=53%, Loss=0.6828486919403076\n",
            "Iteration: 347860 \t Train: Acc=50%, Loss=0.6896289587020874 \t\t Validation: Acc=48%, Loss=0.695180356502533\n",
            "Iteration: 347870 \t Train: Acc=56%, Loss=0.6951730847358704 \t\t Validation: Acc=57%, Loss=0.6884915828704834\n",
            "Iteration: 347880 \t Train: Acc=59%, Loss=0.6906488537788391 \t\t Validation: Acc=54%, Loss=0.6890329122543335\n",
            "Iteration: 347890 \t Train: Acc=51%, Loss=0.6910986304283142 \t\t Validation: Acc=46%, Loss=0.6927413940429688\n",
            "Iteration: 347900 \t Train: Acc=60%, Loss=0.681189239025116 \t\t Validation: Acc=52%, Loss=0.6947197318077087\n",
            "Iteration: 347910 \t Train: Acc=53%, Loss=0.6930447816848755 \t\t Validation: Acc=49%, Loss=0.6934241056442261\n",
            "Iteration: 347920 \t Train: Acc=52%, Loss=0.6884863376617432 \t\t Validation: Acc=47%, Loss=0.6961008310317993\n",
            "Iteration: 347930 \t Train: Acc=50%, Loss=0.6904779672622681 \t\t Validation: Acc=48%, Loss=0.6962661147117615\n",
            "Iteration: 347940 \t Train: Acc=49%, Loss=0.6920825242996216 \t\t Validation: Acc=47%, Loss=0.6926254630088806\n",
            "Iteration: 347950 \t Train: Acc=50%, Loss=0.6928964853286743 \t\t Validation: Acc=53%, Loss=0.6906844973564148\n",
            "Iteration: 347960 \t Train: Acc=46%, Loss=0.6945534944534302 \t\t Validation: Acc=46%, Loss=0.695382833480835\n",
            "Iteration: 347970 \t Train: Acc=53%, Loss=0.687746524810791 \t\t Validation: Acc=48%, Loss=0.6995506286621094\n",
            "Iteration: 347980 \t Train: Acc=44%, Loss=0.6975134015083313 \t\t Validation: Acc=53%, Loss=0.6867517828941345\n",
            "Iteration: 347990 \t Train: Acc=44%, Loss=0.7004097104072571 \t\t Validation: Acc=48%, Loss=0.6945022344589233\n",
            "Iteration: 348000 \t Train: Acc=49%, Loss=0.6949533224105835 \t\t Validation: Acc=49%, Loss=0.6897803544998169\n",
            "Iteration: 348010 \t Train: Acc=55%, Loss=0.6896491050720215 \t\t Validation: Acc=50%, Loss=0.6919060945510864\n",
            "Iteration: 348020 \t Train: Acc=46%, Loss=0.6988764405250549 \t\t Validation: Acc=51%, Loss=0.6906018257141113\n",
            "Iteration: 348030 \t Train: Acc=47%, Loss=0.6832552552223206 \t\t Validation: Acc=49%, Loss=0.6930037140846252\n",
            "Iteration: 348040 \t Train: Acc=50%, Loss=0.6886547207832336 \t\t Validation: Acc=49%, Loss=0.6947346925735474\n",
            "Iteration: 348050 \t Train: Acc=51%, Loss=0.6918401718139648 \t\t Validation: Acc=51%, Loss=0.6939497590065002\n",
            "Iteration: 348060 \t Train: Acc=46%, Loss=0.6949779987335205 \t\t Validation: Acc=54%, Loss=0.6920402646064758\n",
            "Iteration: 348070 \t Train: Acc=50%, Loss=0.6961723566055298 \t\t Validation: Acc=52%, Loss=0.6938350200653076\n",
            "Iteration: 348080 \t Train: Acc=53%, Loss=0.6921074390411377 \t\t Validation: Acc=52%, Loss=0.6926575303077698\n",
            "Iteration: 348090 \t Train: Acc=49%, Loss=0.6938016414642334 \t\t Validation: Acc=48%, Loss=0.692654013633728\n",
            "Iteration: 348100 \t Train: Acc=53%, Loss=0.6900457739830017 \t\t Validation: Acc=50%, Loss=0.6920510530471802\n",
            "Iteration: 348110 \t Train: Acc=48%, Loss=0.694083034992218 \t\t Validation: Acc=55%, Loss=0.6938823461532593\n",
            "Iteration: 348120 \t Train: Acc=57%, Loss=0.6905969381332397 \t\t Validation: Acc=56%, Loss=0.6913094520568848\n",
            "Iteration: 348130 \t Train: Acc=55%, Loss=0.6880010366439819 \t\t Validation: Acc=50%, Loss=0.6943261623382568\n",
            "Iteration: 348140 \t Train: Acc=49%, Loss=0.6900292038917542 \t\t Validation: Acc=53%, Loss=0.6907446384429932\n",
            "Iteration: 348150 \t Train: Acc=50%, Loss=0.6877456307411194 \t\t Validation: Acc=52%, Loss=0.6955335140228271\n",
            "Iteration: 348160 \t Train: Acc=49%, Loss=0.6848015785217285 \t\t Validation: Acc=45%, Loss=0.6929001212120056\n",
            "Iteration: 348170 \t Train: Acc=56%, Loss=0.6817229986190796 \t\t Validation: Acc=46%, Loss=0.6937875151634216\n",
            "Iteration: 348180 \t Train: Acc=53%, Loss=0.6937482953071594 \t\t Validation: Acc=50%, Loss=0.6926442384719849\n",
            "Iteration: 348190 \t Train: Acc=53%, Loss=0.6915361881256104 \t\t Validation: Acc=51%, Loss=0.6883785128593445\n",
            "Iteration: 348200 \t Train: Acc=50%, Loss=0.6875458359718323 \t\t Validation: Acc=49%, Loss=0.6946079730987549\n",
            "Iteration: 348210 \t Train: Acc=48%, Loss=0.6930097937583923 \t\t Validation: Acc=48%, Loss=0.6941982507705688\n",
            "Iteration: 348220 \t Train: Acc=58%, Loss=0.693157434463501 \t\t Validation: Acc=48%, Loss=0.6852254867553711\n",
            "Iteration: 348230 \t Train: Acc=50%, Loss=0.6955776214599609 \t\t Validation: Acc=51%, Loss=0.688208818435669\n",
            "Iteration: 348240 \t Train: Acc=46%, Loss=0.6975677609443665 \t\t Validation: Acc=47%, Loss=0.6962105631828308\n",
            "Iteration: 348250 \t Train: Acc=54%, Loss=0.6794124245643616 \t\t Validation: Acc=50%, Loss=0.6903700232505798\n",
            "Iteration: 348260 \t Train: Acc=51%, Loss=0.6885915994644165 \t\t Validation: Acc=50%, Loss=0.6955282688140869\n",
            "Iteration: 348270 \t Train: Acc=54%, Loss=0.689662516117096 \t\t Validation: Acc=47%, Loss=0.6831416487693787\n",
            "Iteration: 348280 \t Train: Acc=46%, Loss=0.7043492794036865 \t\t Validation: Acc=47%, Loss=0.6881517171859741\n",
            "Iteration: 348290 \t Train: Acc=50%, Loss=0.6968531608581543 \t\t Validation: Acc=56%, Loss=0.6891436576843262\n",
            "Iteration: 348300 \t Train: Acc=47%, Loss=0.6944212913513184 \t\t Validation: Acc=53%, Loss=0.6894868612289429\n",
            "Iteration: 348310 \t Train: Acc=46%, Loss=0.6893527507781982 \t\t Validation: Acc=53%, Loss=0.6872006058692932\n",
            "Iteration: 348320 \t Train: Acc=60%, Loss=0.6824976801872253 \t\t Validation: Acc=57%, Loss=0.6875249743461609\n",
            "Iteration: 348330 \t Train: Acc=47%, Loss=0.6903030276298523 \t\t Validation: Acc=49%, Loss=0.6903872489929199\n",
            "Iteration: 348340 \t Train: Acc=57%, Loss=0.6900444030761719 \t\t Validation: Acc=50%, Loss=0.6931021809577942\n",
            "Iteration: 348350 \t Train: Acc=48%, Loss=0.6951219439506531 \t\t Validation: Acc=55%, Loss=0.6913830041885376\n",
            "Iteration: 348360 \t Train: Acc=57%, Loss=0.6902689933776855 \t\t Validation: Acc=50%, Loss=0.6898885369300842\n",
            "Iteration: 348370 \t Train: Acc=51%, Loss=0.6962558627128601 \t\t Validation: Acc=48%, Loss=0.6913156509399414\n",
            "Iteration: 348380 \t Train: Acc=55%, Loss=0.6899337768554688 \t\t Validation: Acc=47%, Loss=0.693797767162323\n",
            "Iteration: 348390 \t Train: Acc=52%, Loss=0.6942746639251709 \t\t Validation: Acc=46%, Loss=0.6969592571258545\n",
            "Iteration: 348400 \t Train: Acc=51%, Loss=0.6938495635986328 \t\t Validation: Acc=46%, Loss=0.6940047144889832\n",
            "Iteration: 348410 \t Train: Acc=54%, Loss=0.6939170360565186 \t\t Validation: Acc=46%, Loss=0.6989182233810425\n",
            "Iteration: 348420 \t Train: Acc=53%, Loss=0.6900174617767334 \t\t Validation: Acc=48%, Loss=0.6935701370239258\n",
            "Iteration: 348430 \t Train: Acc=50%, Loss=0.7000588178634644 \t\t Validation: Acc=50%, Loss=0.6989753246307373\n",
            "Iteration: 348440 \t Train: Acc=55%, Loss=0.6900103092193604 \t\t Validation: Acc=50%, Loss=0.6951056718826294\n",
            "Iteration: 348450 \t Train: Acc=42%, Loss=0.7043004035949707 \t\t Validation: Acc=50%, Loss=0.6956090927124023\n",
            "Iteration: 348460 \t Train: Acc=55%, Loss=0.687247633934021 \t\t Validation: Acc=53%, Loss=0.6856554746627808\n",
            "Iteration: 348470 \t Train: Acc=50%, Loss=0.6949028372764587 \t\t Validation: Acc=53%, Loss=0.6922718286514282\n",
            "Iteration: 348480 \t Train: Acc=49%, Loss=0.6966362595558167 \t\t Validation: Acc=48%, Loss=0.694260835647583\n",
            "Iteration: 348490 \t Train: Acc=53%, Loss=0.6892657279968262 \t\t Validation: Acc=53%, Loss=0.6887924075126648\n",
            "Iteration: 348500 \t Train: Acc=52%, Loss=0.686318039894104 \t\t Validation: Acc=46%, Loss=0.6981335282325745\n",
            "Iteration: 348510 \t Train: Acc=52%, Loss=0.6846991777420044 \t\t Validation: Acc=53%, Loss=0.6946936845779419\n",
            "Iteration: 348520 \t Train: Acc=49%, Loss=0.6899210810661316 \t\t Validation: Acc=55%, Loss=0.6898900270462036\n",
            "Iteration: 348530 \t Train: Acc=50%, Loss=0.6933090686798096 \t\t Validation: Acc=46%, Loss=0.6943224668502808\n",
            "Iteration: 348540 \t Train: Acc=50%, Loss=0.6889731884002686 \t\t Validation: Acc=46%, Loss=0.6942936182022095\n",
            "Iteration: 348550 \t Train: Acc=46%, Loss=0.6957507133483887 \t\t Validation: Acc=47%, Loss=0.6885365843772888\n",
            "Iteration: 348560 \t Train: Acc=49%, Loss=0.6984740495681763 \t\t Validation: Acc=53%, Loss=0.6912407279014587\n",
            "Iteration: 348570 \t Train: Acc=47%, Loss=0.6874464750289917 \t\t Validation: Acc=55%, Loss=0.6890273094177246\n",
            "Iteration: 348580 \t Train: Acc=50%, Loss=0.6879387497901917 \t\t Validation: Acc=50%, Loss=0.693193256855011\n",
            "Iteration: 348590 \t Train: Acc=53%, Loss=0.6845349073410034 \t\t Validation: Acc=47%, Loss=0.69452303647995\n",
            "Iteration: 348600 \t Train: Acc=53%, Loss=0.6893582344055176 \t\t Validation: Acc=47%, Loss=0.6947281956672668\n",
            "Iteration: 348610 \t Train: Acc=49%, Loss=0.6905357837677002 \t\t Validation: Acc=53%, Loss=0.6879308819770813\n",
            "Iteration: 348620 \t Train: Acc=51%, Loss=0.6899283528327942 \t\t Validation: Acc=44%, Loss=0.6985244154930115\n",
            "Iteration: 348630 \t Train: Acc=50%, Loss=0.7042462825775146 \t\t Validation: Acc=50%, Loss=0.6910530924797058\n",
            "Iteration: 348640 \t Train: Acc=53%, Loss=0.6839318871498108 \t\t Validation: Acc=45%, Loss=0.6982651948928833\n",
            "Iteration: 348650 \t Train: Acc=49%, Loss=0.6900981664657593 \t\t Validation: Acc=47%, Loss=0.6975158452987671\n",
            "Iteration: 348660 \t Train: Acc=50%, Loss=0.6869022846221924 \t\t Validation: Acc=53%, Loss=0.6937533617019653\n",
            "Iteration: 348670 \t Train: Acc=52%, Loss=0.6939864158630371 \t\t Validation: Acc=57%, Loss=0.6872615814208984\n",
            "Iteration: 348680 \t Train: Acc=51%, Loss=0.6888676285743713 \t\t Validation: Acc=51%, Loss=0.6927451491355896\n",
            "Iteration: 348690 \t Train: Acc=52%, Loss=0.686794102191925 \t\t Validation: Acc=51%, Loss=0.692502498626709\n",
            "Iteration: 348700 \t Train: Acc=54%, Loss=0.689789354801178 \t\t Validation: Acc=50%, Loss=0.6912013292312622\n",
            "Iteration: 348710 \t Train: Acc=50%, Loss=0.6918646693229675 \t\t Validation: Acc=44%, Loss=0.7026839256286621\n",
            "Iteration: 348720 \t Train: Acc=48%, Loss=0.6983667016029358 \t\t Validation: Acc=52%, Loss=0.6940605640411377\n",
            "Iteration: 348730 \t Train: Acc=45%, Loss=0.6969196200370789 \t\t Validation: Acc=48%, Loss=0.691088080406189\n",
            "Iteration: 348740 \t Train: Acc=50%, Loss=0.694261908531189 \t\t Validation: Acc=43%, Loss=0.6921257972717285\n",
            "Iteration: 348750 \t Train: Acc=57%, Loss=0.686527669429779 \t\t Validation: Acc=47%, Loss=0.6947195529937744\n",
            "Iteration: 348760 \t Train: Acc=50%, Loss=0.6879149675369263 \t\t Validation: Acc=53%, Loss=0.6872104406356812\n",
            "Iteration: 348770 \t Train: Acc=51%, Loss=0.6926988363265991 \t\t Validation: Acc=46%, Loss=0.6998071074485779\n",
            "Iteration: 348780 \t Train: Acc=47%, Loss=0.7021371126174927 \t\t Validation: Acc=47%, Loss=0.6951773166656494\n",
            "Iteration: 348790 \t Train: Acc=48%, Loss=0.6946976184844971 \t\t Validation: Acc=48%, Loss=0.6959787607192993\n",
            "Iteration: 348800 \t Train: Acc=54%, Loss=0.6896336078643799 \t\t Validation: Acc=53%, Loss=0.697464644908905\n",
            "Iteration: 348810 \t Train: Acc=47%, Loss=0.6888466477394104 \t\t Validation: Acc=54%, Loss=0.6877555847167969\n",
            "Iteration: 348820 \t Train: Acc=50%, Loss=0.6857191920280457 \t\t Validation: Acc=55%, Loss=0.6888931393623352\n",
            "Iteration: 348830 \t Train: Acc=47%, Loss=0.7013766169548035 \t\t Validation: Acc=50%, Loss=0.6901119947433472\n",
            "Iteration: 348840 \t Train: Acc=58%, Loss=0.6875442266464233 \t\t Validation: Acc=51%, Loss=0.6944398283958435\n",
            "Iteration: 348850 \t Train: Acc=57%, Loss=0.6873900294303894 \t\t Validation: Acc=53%, Loss=0.6918368935585022\n",
            "Iteration: 348860 \t Train: Acc=54%, Loss=0.6913022994995117 \t\t Validation: Acc=51%, Loss=0.6921378970146179\n",
            "Iteration: 348870 \t Train: Acc=48%, Loss=0.698800265789032 \t\t Validation: Acc=52%, Loss=0.6871696710586548\n",
            "Iteration: 348880 \t Train: Acc=51%, Loss=0.693884015083313 \t\t Validation: Acc=57%, Loss=0.6912800073623657\n",
            "Iteration: 348890 \t Train: Acc=48%, Loss=0.6936948299407959 \t\t Validation: Acc=50%, Loss=0.6903915405273438\n",
            "Iteration: 348900 \t Train: Acc=52%, Loss=0.6870702505111694 \t\t Validation: Acc=50%, Loss=0.6919702887535095\n",
            "Iteration: 348910 \t Train: Acc=44%, Loss=0.6985921859741211 \t\t Validation: Acc=48%, Loss=0.6940877437591553\n",
            "Iteration: 348920 \t Train: Acc=55%, Loss=0.6870208382606506 \t\t Validation: Acc=53%, Loss=0.6914870142936707\n",
            "Iteration: 348930 \t Train: Acc=46%, Loss=0.6942906379699707 \t\t Validation: Acc=50%, Loss=0.6899502277374268\n",
            "Iteration: 348940 \t Train: Acc=44%, Loss=0.6952306032180786 \t\t Validation: Acc=48%, Loss=0.6934990286827087\n",
            "Iteration: 348950 \t Train: Acc=48%, Loss=0.6896779537200928 \t\t Validation: Acc=50%, Loss=0.6925532221794128\n",
            "Iteration: 348960 \t Train: Acc=47%, Loss=0.6958390474319458 \t\t Validation: Acc=49%, Loss=0.6946496367454529\n",
            "Iteration: 348970 \t Train: Acc=50%, Loss=0.6945222020149231 \t\t Validation: Acc=52%, Loss=0.6931066513061523\n",
            "Iteration: 348980 \t Train: Acc=57%, Loss=0.6868963837623596 \t\t Validation: Acc=49%, Loss=0.6929743885993958\n",
            "Iteration: 348990 \t Train: Acc=50%, Loss=0.6926431655883789 \t\t Validation: Acc=51%, Loss=0.6952304840087891\n",
            "Iteration: 349000 \t Train: Acc=51%, Loss=0.6915918588638306 \t\t Validation: Acc=57%, Loss=0.6881405115127563\n",
            "Iteration: 349010 \t Train: Acc=45%, Loss=0.6927712559700012 \t\t Validation: Acc=53%, Loss=0.6887160539627075\n",
            "Iteration: 349020 \t Train: Acc=49%, Loss=0.7000302076339722 \t\t Validation: Acc=50%, Loss=0.6852210760116577\n",
            "Iteration: 349030 \t Train: Acc=50%, Loss=0.6894744634628296 \t\t Validation: Acc=50%, Loss=0.696509838104248\n",
            "Iteration: 349040 \t Train: Acc=51%, Loss=0.6914200186729431 \t\t Validation: Acc=52%, Loss=0.6888234615325928\n",
            "Iteration: 349050 \t Train: Acc=55%, Loss=0.6909220218658447 \t\t Validation: Acc=48%, Loss=0.6986376047134399\n",
            "Iteration: 349060 \t Train: Acc=50%, Loss=0.6934215426445007 \t\t Validation: Acc=55%, Loss=0.6871271133422852\n",
            "Iteration: 349070 \t Train: Acc=44%, Loss=0.6959632039070129 \t\t Validation: Acc=49%, Loss=0.6950259804725647\n",
            "Iteration: 349080 \t Train: Acc=51%, Loss=0.6878666877746582 \t\t Validation: Acc=52%, Loss=0.693432629108429\n",
            "Iteration: 349090 \t Train: Acc=48%, Loss=0.6930293440818787 \t\t Validation: Acc=49%, Loss=0.6944984197616577\n",
            "Iteration: 349100 \t Train: Acc=51%, Loss=0.688508152961731 \t\t Validation: Acc=50%, Loss=0.6913243532180786\n",
            "Iteration: 349110 \t Train: Acc=57%, Loss=0.6871153116226196 \t\t Validation: Acc=53%, Loss=0.690788745880127\n",
            "Iteration: 349120 \t Train: Acc=52%, Loss=0.6861053109169006 \t\t Validation: Acc=51%, Loss=0.695713460445404\n",
            "Iteration: 349130 \t Train: Acc=48%, Loss=0.6953219771385193 \t\t Validation: Acc=46%, Loss=0.6988455057144165\n",
            "Iteration: 349140 \t Train: Acc=50%, Loss=0.6923090219497681 \t\t Validation: Acc=49%, Loss=0.6915205717086792\n",
            "Iteration: 349150 \t Train: Acc=54%, Loss=0.6888092756271362 \t\t Validation: Acc=49%, Loss=0.6893792748451233\n",
            "Iteration: 349160 \t Train: Acc=50%, Loss=0.6865218877792358 \t\t Validation: Acc=53%, Loss=0.6903923153877258\n",
            "Iteration: 349170 \t Train: Acc=54%, Loss=0.685110867023468 \t\t Validation: Acc=52%, Loss=0.68865567445755\n",
            "Iteration: 349180 \t Train: Acc=45%, Loss=0.705570638179779 \t\t Validation: Acc=51%, Loss=0.6881651282310486\n",
            "Iteration: 349190 \t Train: Acc=46%, Loss=0.6950165629386902 \t\t Validation: Acc=49%, Loss=0.6985889077186584\n",
            "Iteration: 349200 \t Train: Acc=53%, Loss=0.691895604133606 \t\t Validation: Acc=51%, Loss=0.6946086287498474\n",
            "Iteration: 349210 \t Train: Acc=50%, Loss=0.6956864595413208 \t\t Validation: Acc=46%, Loss=0.6963399648666382\n",
            "Iteration: 349220 \t Train: Acc=53%, Loss=0.6939836740493774 \t\t Validation: Acc=50%, Loss=0.6954860091209412\n",
            "Iteration: 349230 \t Train: Acc=50%, Loss=0.6925836801528931 \t\t Validation: Acc=50%, Loss=0.6884429454803467\n",
            "Iteration: 349240 \t Train: Acc=50%, Loss=0.6856527328491211 \t\t Validation: Acc=48%, Loss=0.6992402076721191\n",
            "Iteration: 349250 \t Train: Acc=44%, Loss=0.6958910822868347 \t\t Validation: Acc=48%, Loss=0.6899818181991577\n",
            "Iteration: 349260 \t Train: Acc=44%, Loss=0.697359561920166 \t\t Validation: Acc=53%, Loss=0.6870386600494385\n",
            "Iteration: 349270 \t Train: Acc=46%, Loss=0.696236252784729 \t\t Validation: Acc=50%, Loss=0.6959847211837769\n",
            "Iteration: 349280 \t Train: Acc=49%, Loss=0.6897254586219788 \t\t Validation: Acc=53%, Loss=0.687220573425293\n",
            "Iteration: 349290 \t Train: Acc=53%, Loss=0.6889168620109558 \t\t Validation: Acc=53%, Loss=0.6924020648002625\n",
            "Iteration: 349300 \t Train: Acc=53%, Loss=0.6904935836791992 \t\t Validation: Acc=52%, Loss=0.6890729665756226\n",
            "Iteration: 349310 \t Train: Acc=48%, Loss=0.6904232501983643 \t\t Validation: Acc=52%, Loss=0.69196617603302\n",
            "Iteration: 349320 \t Train: Acc=48%, Loss=0.6954362392425537 \t\t Validation: Acc=47%, Loss=0.6984608173370361\n",
            "Iteration: 349330 \t Train: Acc=46%, Loss=0.7017078995704651 \t\t Validation: Acc=54%, Loss=0.6858528852462769\n",
            "Iteration: 349340 \t Train: Acc=49%, Loss=0.6913176774978638 \t\t Validation: Acc=54%, Loss=0.6881072521209717\n",
            "Iteration: 349350 \t Train: Acc=53%, Loss=0.6909612417221069 \t\t Validation: Acc=48%, Loss=0.6934699416160583\n",
            "Iteration: 349360 \t Train: Acc=53%, Loss=0.6905087828636169 \t\t Validation: Acc=53%, Loss=0.6873803734779358\n",
            "Iteration: 349370 \t Train: Acc=52%, Loss=0.6925089955329895 \t\t Validation: Acc=51%, Loss=0.6931889057159424\n",
            "Iteration: 349380 \t Train: Acc=49%, Loss=0.69674152135849 \t\t Validation: Acc=45%, Loss=0.6940262913703918\n",
            "Iteration: 349390 \t Train: Acc=51%, Loss=0.6855580806732178 \t\t Validation: Acc=52%, Loss=0.6954181790351868\n",
            "Iteration: 349400 \t Train: Acc=50%, Loss=0.697007417678833 \t\t Validation: Acc=53%, Loss=0.6956067085266113\n",
            "Iteration: 349410 \t Train: Acc=48%, Loss=0.6956798434257507 \t\t Validation: Acc=57%, Loss=0.6859190464019775\n",
            "Iteration: 349420 \t Train: Acc=56%, Loss=0.6864800453186035 \t\t Validation: Acc=50%, Loss=0.6911705136299133\n",
            "Iteration: 349430 \t Train: Acc=46%, Loss=0.6940435171127319 \t\t Validation: Acc=49%, Loss=0.6934220790863037\n",
            "Iteration: 349440 \t Train: Acc=50%, Loss=0.6920173764228821 \t\t Validation: Acc=48%, Loss=0.6980855464935303\n",
            "Iteration: 349450 \t Train: Acc=53%, Loss=0.6912837624549866 \t\t Validation: Acc=51%, Loss=0.688348114490509\n",
            "Iteration: 349460 \t Train: Acc=49%, Loss=0.6948727965354919 \t\t Validation: Acc=53%, Loss=0.690812349319458\n",
            "Iteration: 349470 \t Train: Acc=53%, Loss=0.6778174638748169 \t\t Validation: Acc=46%, Loss=0.6934809684753418\n",
            "Iteration: 349480 \t Train: Acc=50%, Loss=0.688917338848114 \t\t Validation: Acc=47%, Loss=0.6991428136825562\n",
            "Iteration: 349490 \t Train: Acc=47%, Loss=0.6915196776390076 \t\t Validation: Acc=51%, Loss=0.6930463314056396\n",
            "Iteration: 349500 \t Train: Acc=57%, Loss=0.691241443157196 \t\t Validation: Acc=54%, Loss=0.6922950148582458\n",
            "Iteration: 349510 \t Train: Acc=53%, Loss=0.7028105854988098 \t\t Validation: Acc=50%, Loss=0.6919161677360535\n",
            "Iteration: 349520 \t Train: Acc=60%, Loss=0.6844180822372437 \t\t Validation: Acc=46%, Loss=0.6952736973762512\n",
            "Iteration: 349530 \t Train: Acc=56%, Loss=0.6861692070960999 \t\t Validation: Acc=52%, Loss=0.6918396353721619\n",
            "Iteration: 349540 \t Train: Acc=53%, Loss=0.6903303265571594 \t\t Validation: Acc=53%, Loss=0.6923705339431763\n",
            "Iteration: 349550 \t Train: Acc=52%, Loss=0.6900262832641602 \t\t Validation: Acc=49%, Loss=0.691347062587738\n",
            "Iteration: 349560 \t Train: Acc=50%, Loss=0.6795173287391663 \t\t Validation: Acc=57%, Loss=0.6864635944366455\n",
            "Iteration: 349570 \t Train: Acc=53%, Loss=0.6889727115631104 \t\t Validation: Acc=52%, Loss=0.6903166770935059\n",
            "Iteration: 349580 \t Train: Acc=46%, Loss=0.6993013620376587 \t\t Validation: Acc=47%, Loss=0.6959981322288513\n",
            "Iteration: 349590 \t Train: Acc=52%, Loss=0.6902062296867371 \t\t Validation: Acc=55%, Loss=0.68658846616745\n",
            "Iteration: 349600 \t Train: Acc=52%, Loss=0.6923907995223999 \t\t Validation: Acc=50%, Loss=0.6914046406745911\n",
            "Iteration: 349610 \t Train: Acc=52%, Loss=0.6877546906471252 \t\t Validation: Acc=51%, Loss=0.6905239224433899\n",
            "Iteration: 349620 \t Train: Acc=53%, Loss=0.6890185475349426 \t\t Validation: Acc=52%, Loss=0.6958026885986328\n",
            "Iteration: 349630 \t Train: Acc=47%, Loss=0.6886262893676758 \t\t Validation: Acc=50%, Loss=0.6950187683105469\n",
            "Iteration: 349640 \t Train: Acc=49%, Loss=0.6862542033195496 \t\t Validation: Acc=56%, Loss=0.6866834163665771\n",
            "Iteration: 349650 \t Train: Acc=48%, Loss=0.6996153593063354 \t\t Validation: Acc=46%, Loss=0.6931881308555603\n",
            "Iteration: 349660 \t Train: Acc=50%, Loss=0.6926870942115784 \t\t Validation: Acc=53%, Loss=0.6942735314369202\n",
            "Iteration: 349670 \t Train: Acc=47%, Loss=0.6864816546440125 \t\t Validation: Acc=52%, Loss=0.6881153583526611\n",
            "Iteration: 349680 \t Train: Acc=50%, Loss=0.6924717426300049 \t\t Validation: Acc=47%, Loss=0.6954935789108276\n",
            "Iteration: 349690 \t Train: Acc=48%, Loss=0.6931048035621643 \t\t Validation: Acc=53%, Loss=0.6901922821998596\n",
            "Iteration: 349700 \t Train: Acc=50%, Loss=0.6916781663894653 \t\t Validation: Acc=50%, Loss=0.691949188709259\n",
            "Iteration: 349710 \t Train: Acc=50%, Loss=0.700682520866394 \t\t Validation: Acc=50%, Loss=0.6936602592468262\n",
            "Iteration: 349720 \t Train: Acc=53%, Loss=0.6885559558868408 \t\t Validation: Acc=50%, Loss=0.6937142610549927\n",
            "Iteration: 349730 \t Train: Acc=51%, Loss=0.695184588432312 \t\t Validation: Acc=51%, Loss=0.6943456530570984\n",
            "Iteration: 349740 \t Train: Acc=52%, Loss=0.6900604963302612 \t\t Validation: Acc=46%, Loss=0.6972489356994629\n",
            "Iteration: 349750 \t Train: Acc=56%, Loss=0.6989641785621643 \t\t Validation: Acc=45%, Loss=0.6910732388496399\n",
            "Iteration: 349760 \t Train: Acc=46%, Loss=0.6984010934829712 \t\t Validation: Acc=49%, Loss=0.6913155317306519\n",
            "Iteration: 349770 \t Train: Acc=51%, Loss=0.6920099258422852 \t\t Validation: Acc=50%, Loss=0.6910420060157776\n",
            "Iteration: 349780 \t Train: Acc=48%, Loss=0.69310063123703 \t\t Validation: Acc=46%, Loss=0.693433403968811\n",
            "Iteration: 349790 \t Train: Acc=52%, Loss=0.690081000328064 \t\t Validation: Acc=50%, Loss=0.6913729906082153\n",
            "Iteration: 349800 \t Train: Acc=51%, Loss=0.6914370059967041 \t\t Validation: Acc=49%, Loss=0.6977257132530212\n",
            "Iteration: 349810 \t Train: Acc=53%, Loss=0.6907817721366882 \t\t Validation: Acc=48%, Loss=0.6941737532615662\n",
            "Iteration: 349820 \t Train: Acc=49%, Loss=0.6945353746414185 \t\t Validation: Acc=48%, Loss=0.6931565403938293\n",
            "Iteration: 349830 \t Train: Acc=53%, Loss=0.6903546452522278 \t\t Validation: Acc=50%, Loss=0.6945686340332031\n",
            "Iteration: 349840 \t Train: Acc=46%, Loss=0.6964772343635559 \t\t Validation: Acc=50%, Loss=0.6965171098709106\n",
            "Iteration: 349850 \t Train: Acc=53%, Loss=0.6897647976875305 \t\t Validation: Acc=50%, Loss=0.6916193962097168\n",
            "Iteration: 349860 \t Train: Acc=52%, Loss=0.6885181069374084 \t\t Validation: Acc=51%, Loss=0.6913962960243225\n",
            "Iteration: 349870 \t Train: Acc=43%, Loss=0.6953064203262329 \t\t Validation: Acc=47%, Loss=0.6927958726882935\n",
            "Iteration: 349880 \t Train: Acc=53%, Loss=0.6880214810371399 \t\t Validation: Acc=50%, Loss=0.6936410069465637\n",
            "Iteration: 349890 \t Train: Acc=51%, Loss=0.6997207403182983 \t\t Validation: Acc=46%, Loss=0.694061279296875\n",
            "Iteration: 349900 \t Train: Acc=49%, Loss=0.6926414966583252 \t\t Validation: Acc=54%, Loss=0.69402015209198\n",
            "Iteration: 349910 \t Train: Acc=55%, Loss=0.6876879930496216 \t\t Validation: Acc=50%, Loss=0.6962281465530396\n",
            "Iteration: 349920 \t Train: Acc=53%, Loss=0.6888734102249146 \t\t Validation: Acc=49%, Loss=0.6915128231048584\n",
            "Iteration: 349930 \t Train: Acc=50%, Loss=0.6916249990463257 \t\t Validation: Acc=48%, Loss=0.6924351453781128\n",
            "Iteration: 349940 \t Train: Acc=50%, Loss=0.6928545236587524 \t\t Validation: Acc=48%, Loss=0.6962530612945557\n",
            "Iteration: 349950 \t Train: Acc=50%, Loss=0.6975613832473755 \t\t Validation: Acc=53%, Loss=0.6950793862342834\n",
            "Iteration: 349960 \t Train: Acc=53%, Loss=0.6895936131477356 \t\t Validation: Acc=52%, Loss=0.6880530714988708\n",
            "Iteration: 349970 \t Train: Acc=50%, Loss=0.6900604963302612 \t\t Validation: Acc=50%, Loss=0.6899070143699646\n",
            "Iteration: 349980 \t Train: Acc=50%, Loss=0.6976967453956604 \t\t Validation: Acc=47%, Loss=0.6917965412139893\n",
            "Iteration: 349990 \t Train: Acc=56%, Loss=0.6872768998146057 \t\t Validation: Acc=48%, Loss=0.6939364671707153\n",
            "Iteration: 350000 \t Train: Acc=50%, Loss=0.6854888796806335 \t\t Validation: Acc=50%, Loss=0.6946084499359131\n",
            "Iteration: 350010 \t Train: Acc=47%, Loss=0.694065511226654 \t\t Validation: Acc=47%, Loss=0.6912105083465576\n",
            "Iteration: 350020 \t Train: Acc=52%, Loss=0.6832507848739624 \t\t Validation: Acc=50%, Loss=0.686985194683075\n",
            "Iteration: 350030 \t Train: Acc=48%, Loss=0.697368860244751 \t\t Validation: Acc=50%, Loss=0.6911287903785706\n",
            "Iteration: 350040 \t Train: Acc=50%, Loss=0.690328061580658 \t\t Validation: Acc=45%, Loss=0.6905121207237244\n",
            "Iteration: 350050 \t Train: Acc=52%, Loss=0.6959774494171143 \t\t Validation: Acc=50%, Loss=0.694831132888794\n",
            "Iteration: 350060 \t Train: Acc=50%, Loss=0.6886441111564636 \t\t Validation: Acc=55%, Loss=0.691068172454834\n",
            "Iteration: 350070 \t Train: Acc=51%, Loss=0.6907047033309937 \t\t Validation: Acc=51%, Loss=0.6891084313392639\n",
            "Iteration: 350080 \t Train: Acc=43%, Loss=0.697805643081665 \t\t Validation: Acc=53%, Loss=0.6963202953338623\n",
            "Iteration: 350090 \t Train: Acc=56%, Loss=0.6910637021064758 \t\t Validation: Acc=49%, Loss=0.6881852149963379\n",
            "Iteration: 350100 \t Train: Acc=43%, Loss=0.6967597007751465 \t\t Validation: Acc=49%, Loss=0.6924926042556763\n",
            "Iteration: 350110 \t Train: Acc=56%, Loss=0.6860204935073853 \t\t Validation: Acc=50%, Loss=0.6923841834068298\n",
            "Iteration: 350120 \t Train: Acc=56%, Loss=0.6874730587005615 \t\t Validation: Acc=50%, Loss=0.6933275461196899\n",
            "Iteration: 350130 \t Train: Acc=50%, Loss=0.6947183609008789 \t\t Validation: Acc=50%, Loss=0.692345142364502\n",
            "Iteration: 350140 \t Train: Acc=57%, Loss=0.6792104244232178 \t\t Validation: Acc=47%, Loss=0.6921131610870361\n",
            "Iteration: 350150 \t Train: Acc=52%, Loss=0.6908258199691772 \t\t Validation: Acc=53%, Loss=0.6880316138267517\n",
            "Iteration: 350160 \t Train: Acc=52%, Loss=0.6874496340751648 \t\t Validation: Acc=50%, Loss=0.6931139230728149\n",
            "Iteration: 350170 \t Train: Acc=53%, Loss=0.6901121735572815 \t\t Validation: Acc=50%, Loss=0.6938893795013428\n",
            "Iteration: 350180 \t Train: Acc=53%, Loss=0.6895846128463745 \t\t Validation: Acc=55%, Loss=0.6869782209396362\n",
            "Iteration: 350190 \t Train: Acc=53%, Loss=0.6910980939865112 \t\t Validation: Acc=50%, Loss=0.6912702322006226\n",
            "Iteration: 350200 \t Train: Acc=49%, Loss=0.6957337260246277 \t\t Validation: Acc=49%, Loss=0.6897681951522827\n",
            "Iteration: 350210 \t Train: Acc=50%, Loss=0.6968870162963867 \t\t Validation: Acc=47%, Loss=0.696026623249054\n",
            "Iteration: 350220 \t Train: Acc=53%, Loss=0.6898167133331299 \t\t Validation: Acc=50%, Loss=0.6896514296531677\n",
            "Iteration: 350230 \t Train: Acc=49%, Loss=0.6922531723976135 \t\t Validation: Acc=52%, Loss=0.6907909512519836\n",
            "Iteration: 350240 \t Train: Acc=50%, Loss=0.6925445795059204 \t\t Validation: Acc=51%, Loss=0.6886274218559265\n",
            "Iteration: 350250 \t Train: Acc=52%, Loss=0.6883589029312134 \t\t Validation: Acc=50%, Loss=0.6931682825088501\n",
            "Iteration: 350260 \t Train: Acc=53%, Loss=0.6835606694221497 \t\t Validation: Acc=55%, Loss=0.6872488260269165\n",
            "Iteration: 350270 \t Train: Acc=54%, Loss=0.6892927885055542 \t\t Validation: Acc=45%, Loss=0.6986916065216064\n",
            "Iteration: 350280 \t Train: Acc=48%, Loss=0.6934574842453003 \t\t Validation: Acc=49%, Loss=0.6987050771713257\n",
            "Iteration: 350290 \t Train: Acc=47%, Loss=0.6962155699729919 \t\t Validation: Acc=53%, Loss=0.6891627311706543\n",
            "Iteration: 350300 \t Train: Acc=47%, Loss=0.6945744156837463 \t\t Validation: Acc=56%, Loss=0.6905684471130371\n",
            "Iteration: 350310 \t Train: Acc=53%, Loss=0.6898974180221558 \t\t Validation: Acc=57%, Loss=0.6883461475372314\n",
            "Iteration: 350320 \t Train: Acc=48%, Loss=0.6942704916000366 \t\t Validation: Acc=53%, Loss=0.6883419752120972\n",
            "Iteration: 350330 \t Train: Acc=56%, Loss=0.6912910342216492 \t\t Validation: Acc=53%, Loss=0.6863926649093628\n",
            "Iteration: 350340 \t Train: Acc=42%, Loss=0.7013900876045227 \t\t Validation: Acc=50%, Loss=0.6908892393112183\n",
            "Iteration: 350350 \t Train: Acc=56%, Loss=0.6879280209541321 \t\t Validation: Acc=48%, Loss=0.692280650138855\n",
            "Iteration: 350360 \t Train: Acc=48%, Loss=0.694505512714386 \t\t Validation: Acc=57%, Loss=0.6870526671409607\n",
            "Iteration: 350370 \t Train: Acc=43%, Loss=0.6959589123725891 \t\t Validation: Acc=51%, Loss=0.691472053527832\n",
            "Iteration: 350380 \t Train: Acc=50%, Loss=0.6917418837547302 \t\t Validation: Acc=53%, Loss=0.6946277618408203\n",
            "Iteration: 350390 \t Train: Acc=52%, Loss=0.692897379398346 \t\t Validation: Acc=52%, Loss=0.6892704367637634\n",
            "Iteration: 350400 \t Train: Acc=52%, Loss=0.6836284399032593 \t\t Validation: Acc=53%, Loss=0.6913175582885742\n",
            "Iteration: 350410 \t Train: Acc=53%, Loss=0.6889044046401978 \t\t Validation: Acc=54%, Loss=0.6883493065834045\n",
            "Iteration: 350420 \t Train: Acc=47%, Loss=0.6911007165908813 \t\t Validation: Acc=51%, Loss=0.6881316900253296\n",
            "Iteration: 350430 \t Train: Acc=49%, Loss=0.6905841827392578 \t\t Validation: Acc=48%, Loss=0.6981872320175171\n",
            "Iteration: 350440 \t Train: Acc=53%, Loss=0.6909863948822021 \t\t Validation: Acc=42%, Loss=0.7024983167648315\n",
            "Iteration: 350450 \t Train: Acc=52%, Loss=0.6892844438552856 \t\t Validation: Acc=50%, Loss=0.6938830018043518\n",
            "Iteration: 350460 \t Train: Acc=56%, Loss=0.6922754645347595 \t\t Validation: Acc=51%, Loss=0.6915083527565002\n",
            "Iteration: 350470 \t Train: Acc=49%, Loss=0.6928444504737854 \t\t Validation: Acc=55%, Loss=0.6928771138191223\n",
            "Iteration: 350480 \t Train: Acc=45%, Loss=0.6898675560951233 \t\t Validation: Acc=53%, Loss=0.691408634185791\n",
            "Iteration: 350490 \t Train: Acc=45%, Loss=0.6957437992095947 \t\t Validation: Acc=50%, Loss=0.6949498653411865\n",
            "Iteration: 350500 \t Train: Acc=47%, Loss=0.6991491317749023 \t\t Validation: Acc=55%, Loss=0.6966869235038757\n",
            "Iteration: 350510 \t Train: Acc=57%, Loss=0.6908060908317566 \t\t Validation: Acc=50%, Loss=0.699499249458313\n",
            "Iteration: 350520 \t Train: Acc=47%, Loss=0.6993449926376343 \t\t Validation: Acc=53%, Loss=0.6891975998878479\n",
            "Iteration: 350530 \t Train: Acc=50%, Loss=0.6968439817428589 \t\t Validation: Acc=52%, Loss=0.6886692643165588\n",
            "Iteration: 350540 \t Train: Acc=46%, Loss=0.6961356997489929 \t\t Validation: Acc=51%, Loss=0.6924216747283936\n",
            "Iteration: 350550 \t Train: Acc=51%, Loss=0.6942897439002991 \t\t Validation: Acc=51%, Loss=0.6884620189666748\n",
            "Iteration: 350560 \t Train: Acc=53%, Loss=0.6896836161613464 \t\t Validation: Acc=50%, Loss=0.6935722231864929\n",
            "Iteration: 350570 \t Train: Acc=49%, Loss=0.6952648758888245 \t\t Validation: Acc=43%, Loss=0.7021715044975281\n",
            "Iteration: 350580 \t Train: Acc=47%, Loss=0.692227840423584 \t\t Validation: Acc=50%, Loss=0.6958380341529846\n",
            "Iteration: 350590 \t Train: Acc=46%, Loss=0.6942844390869141 \t\t Validation: Acc=53%, Loss=0.6878517866134644\n",
            "Iteration: 350600 \t Train: Acc=50%, Loss=0.6923274993896484 \t\t Validation: Acc=54%, Loss=0.689304530620575\n",
            "Iteration: 350610 \t Train: Acc=50%, Loss=0.6929124593734741 \t\t Validation: Acc=52%, Loss=0.6912077069282532\n",
            "Iteration: 350620 \t Train: Acc=50%, Loss=0.6994326114654541 \t\t Validation: Acc=55%, Loss=0.6928061842918396\n",
            "Iteration: 350630 \t Train: Acc=50%, Loss=0.688222348690033 \t\t Validation: Acc=50%, Loss=0.6826825141906738\n",
            "Iteration: 350640 \t Train: Acc=53%, Loss=0.6930357217788696 \t\t Validation: Acc=49%, Loss=0.6951518654823303\n",
            "Iteration: 350650 \t Train: Acc=53%, Loss=0.6909674406051636 \t\t Validation: Acc=55%, Loss=0.6872936487197876\n",
            "Iteration: 350660 \t Train: Acc=54%, Loss=0.6922754049301147 \t\t Validation: Acc=49%, Loss=0.6869227886199951\n",
            "Iteration: 350670 \t Train: Acc=57%, Loss=0.6808575391769409 \t\t Validation: Acc=53%, Loss=0.6894308924674988\n",
            "Iteration: 350680 \t Train: Acc=57%, Loss=0.6919960975646973 \t\t Validation: Acc=53%, Loss=0.6900734901428223\n",
            "Iteration: 350690 \t Train: Acc=53%, Loss=0.684302806854248 \t\t Validation: Acc=52%, Loss=0.6973051428794861\n",
            "Iteration: 350700 \t Train: Acc=50%, Loss=0.6940609812736511 \t\t Validation: Acc=49%, Loss=0.6974862217903137\n",
            "Iteration: 350710 \t Train: Acc=59%, Loss=0.682309091091156 \t\t Validation: Acc=51%, Loss=0.687611997127533\n",
            "Iteration: 350720 \t Train: Acc=52%, Loss=0.6918573975563049 \t\t Validation: Acc=44%, Loss=0.7064709067344666\n",
            "Iteration: 350730 \t Train: Acc=53%, Loss=0.6912620067596436 \t\t Validation: Acc=50%, Loss=0.6980269551277161\n",
            "Iteration: 350740 \t Train: Acc=47%, Loss=0.6958195567131042 \t\t Validation: Acc=50%, Loss=0.6911939978599548\n",
            "Iteration: 350750 \t Train: Acc=49%, Loss=0.6889148950576782 \t\t Validation: Acc=45%, Loss=0.6978567242622375\n",
            "Iteration: 350760 \t Train: Acc=55%, Loss=0.687388002872467 \t\t Validation: Acc=53%, Loss=0.6848892569541931\n",
            "Iteration: 350770 \t Train: Acc=47%, Loss=0.6947026252746582 \t\t Validation: Acc=57%, Loss=0.6854127049446106\n",
            "Iteration: 350780 \t Train: Acc=49%, Loss=0.6983200311660767 \t\t Validation: Acc=53%, Loss=0.6960833072662354\n",
            "Iteration: 350790 \t Train: Acc=47%, Loss=0.699107825756073 \t\t Validation: Acc=56%, Loss=0.6914767026901245\n",
            "Iteration: 350800 \t Train: Acc=57%, Loss=0.6996824741363525 \t\t Validation: Acc=51%, Loss=0.6958889961242676\n",
            "Iteration: 350810 \t Train: Acc=52%, Loss=0.6919798851013184 \t\t Validation: Acc=53%, Loss=0.691417396068573\n",
            "Iteration: 350820 \t Train: Acc=53%, Loss=0.6909164190292358 \t\t Validation: Acc=51%, Loss=0.6923221945762634\n",
            "Iteration: 350830 \t Train: Acc=50%, Loss=0.6896533966064453 \t\t Validation: Acc=51%, Loss=0.6908485889434814\n",
            "Iteration: 350840 \t Train: Acc=53%, Loss=0.6855856776237488 \t\t Validation: Acc=53%, Loss=0.6912221908569336\n",
            "Iteration: 350850 \t Train: Acc=47%, Loss=0.7026956081390381 \t\t Validation: Acc=49%, Loss=0.6984722018241882\n",
            "Iteration: 350860 \t Train: Acc=50%, Loss=0.69453364610672 \t\t Validation: Acc=50%, Loss=0.6915920376777649\n",
            "Iteration: 350870 \t Train: Acc=48%, Loss=0.6962878704071045 \t\t Validation: Acc=50%, Loss=0.6913179159164429\n",
            "Iteration: 350880 \t Train: Acc=53%, Loss=0.6862250566482544 \t\t Validation: Acc=50%, Loss=0.6946238279342651\n",
            "Iteration: 350890 \t Train: Acc=48%, Loss=0.6908029317855835 \t\t Validation: Acc=49%, Loss=0.6983715295791626\n",
            "Iteration: 350900 \t Train: Acc=47%, Loss=0.6884121894836426 \t\t Validation: Acc=49%, Loss=0.7018951773643494\n",
            "Iteration: 350910 \t Train: Acc=49%, Loss=0.6954920291900635 \t\t Validation: Acc=50%, Loss=0.6929768919944763\n",
            "Iteration: 350920 \t Train: Acc=52%, Loss=0.6863266825675964 \t\t Validation: Acc=46%, Loss=0.6987818479537964\n",
            "Iteration: 350930 \t Train: Acc=46%, Loss=0.6985875964164734 \t\t Validation: Acc=50%, Loss=0.691808819770813\n",
            "Iteration: 350940 \t Train: Acc=49%, Loss=0.6989316344261169 \t\t Validation: Acc=54%, Loss=0.6870203614234924\n",
            "Iteration: 350950 \t Train: Acc=52%, Loss=0.6941199898719788 \t\t Validation: Acc=53%, Loss=0.6914551854133606\n",
            "Iteration: 350960 \t Train: Acc=49%, Loss=0.6919869780540466 \t\t Validation: Acc=52%, Loss=0.6958383321762085\n",
            "Iteration: 350970 \t Train: Acc=45%, Loss=0.6988250613212585 \t\t Validation: Acc=50%, Loss=0.6952688694000244\n",
            "Iteration: 350980 \t Train: Acc=57%, Loss=0.6846417188644409 \t\t Validation: Acc=49%, Loss=0.6967179179191589\n",
            "Iteration: 350990 \t Train: Acc=53%, Loss=0.6954877972602844 \t\t Validation: Acc=56%, Loss=0.6891605854034424\n",
            "Iteration: 351000 \t Train: Acc=45%, Loss=0.6948165893554688 \t\t Validation: Acc=51%, Loss=0.6872326135635376\n",
            "Iteration: 351010 \t Train: Acc=60%, Loss=0.6820505857467651 \t\t Validation: Acc=51%, Loss=0.6935718059539795\n",
            "Iteration: 351020 \t Train: Acc=44%, Loss=0.6994487643241882 \t\t Validation: Acc=54%, Loss=0.6981252431869507\n",
            "Iteration: 351030 \t Train: Acc=50%, Loss=0.6931992173194885 \t\t Validation: Acc=52%, Loss=0.687172532081604\n",
            "Iteration: 351040 \t Train: Acc=47%, Loss=0.702335000038147 \t\t Validation: Acc=49%, Loss=0.6917508840560913\n",
            "Iteration: 351050 \t Train: Acc=52%, Loss=0.6864531636238098 \t\t Validation: Acc=52%, Loss=0.6925824880599976\n",
            "Iteration: 351060 \t Train: Acc=52%, Loss=0.6939848065376282 \t\t Validation: Acc=53%, Loss=0.69919353723526\n",
            "Iteration: 351070 \t Train: Acc=53%, Loss=0.6903916597366333 \t\t Validation: Acc=50%, Loss=0.6951165199279785\n",
            "Iteration: 351080 \t Train: Acc=46%, Loss=0.6881467700004578 \t\t Validation: Acc=50%, Loss=0.6884775161743164\n",
            "Iteration: 351090 \t Train: Acc=47%, Loss=0.6883198022842407 \t\t Validation: Acc=53%, Loss=0.6967985033988953\n",
            "Iteration: 351100 \t Train: Acc=55%, Loss=0.6855905055999756 \t\t Validation: Acc=52%, Loss=0.6922223567962646\n",
            "Iteration: 351110 \t Train: Acc=52%, Loss=0.6916404962539673 \t\t Validation: Acc=51%, Loss=0.6892945170402527\n",
            "Iteration: 351120 \t Train: Acc=48%, Loss=0.6953309774398804 \t\t Validation: Acc=49%, Loss=0.6950655579566956\n",
            "Iteration: 351130 \t Train: Acc=52%, Loss=0.6981841325759888 \t\t Validation: Acc=50%, Loss=0.6888566613197327\n",
            "Iteration: 351140 \t Train: Acc=47%, Loss=0.6868959069252014 \t\t Validation: Acc=56%, Loss=0.6890063881874084\n",
            "Iteration: 351150 \t Train: Acc=48%, Loss=0.6933141350746155 \t\t Validation: Acc=51%, Loss=0.6882781386375427\n",
            "Iteration: 351160 \t Train: Acc=49%, Loss=0.6920361518859863 \t\t Validation: Acc=52%, Loss=0.6940582394599915\n",
            "Iteration: 351170 \t Train: Acc=53%, Loss=0.6890665888786316 \t\t Validation: Acc=46%, Loss=0.6900497078895569\n",
            "Iteration: 351180 \t Train: Acc=57%, Loss=0.6880971193313599 \t\t Validation: Acc=51%, Loss=0.6912994384765625\n",
            "Iteration: 351190 \t Train: Acc=52%, Loss=0.6916733384132385 \t\t Validation: Acc=48%, Loss=0.7007291316986084\n",
            "Iteration: 351200 \t Train: Acc=54%, Loss=0.6888463497161865 \t\t Validation: Acc=48%, Loss=0.6937987804412842\n",
            "Iteration: 351210 \t Train: Acc=47%, Loss=0.6951128244400024 \t\t Validation: Acc=51%, Loss=0.6921746730804443\n",
            "Iteration: 351220 \t Train: Acc=50%, Loss=0.6908882260322571 \t\t Validation: Acc=51%, Loss=0.692020058631897\n",
            "Iteration: 351230 \t Train: Acc=53%, Loss=0.6967122554779053 \t\t Validation: Acc=54%, Loss=0.6891970634460449\n",
            "Iteration: 351240 \t Train: Acc=49%, Loss=0.6962951421737671 \t\t Validation: Acc=50%, Loss=0.7002590298652649\n",
            "Iteration: 351250 \t Train: Acc=47%, Loss=0.6909136772155762 \t\t Validation: Acc=52%, Loss=0.6939930319786072\n",
            "Iteration: 351260 \t Train: Acc=53%, Loss=0.6849455237388611 \t\t Validation: Acc=46%, Loss=0.6922351121902466\n",
            "Iteration: 351270 \t Train: Acc=49%, Loss=0.6939846873283386 \t\t Validation: Acc=52%, Loss=0.6856445074081421\n",
            "Iteration: 351280 \t Train: Acc=55%, Loss=0.6854673027992249 \t\t Validation: Acc=46%, Loss=0.7017264366149902\n",
            "Iteration: 351290 \t Train: Acc=48%, Loss=0.6923520565032959 \t\t Validation: Acc=50%, Loss=0.6944433450698853\n",
            "Iteration: 351300 \t Train: Acc=50%, Loss=0.6893772482872009 \t\t Validation: Acc=50%, Loss=0.6960816383361816\n",
            "Iteration: 351310 \t Train: Acc=48%, Loss=0.6959051489830017 \t\t Validation: Acc=50%, Loss=0.6963836550712585\n",
            "Iteration: 351320 \t Train: Acc=56%, Loss=0.6896746158599854 \t\t Validation: Acc=48%, Loss=0.7103592157363892\n",
            "Iteration: 351330 \t Train: Acc=53%, Loss=0.6937590837478638 \t\t Validation: Acc=50%, Loss=0.68886399269104\n",
            "Iteration: 351340 \t Train: Acc=53%, Loss=0.6871750950813293 \t\t Validation: Acc=54%, Loss=0.6883797645568848\n",
            "Iteration: 351350 \t Train: Acc=46%, Loss=0.6910699605941772 \t\t Validation: Acc=50%, Loss=0.6933807134628296\n",
            "Iteration: 351360 \t Train: Acc=46%, Loss=0.7029116153717041 \t\t Validation: Acc=50%, Loss=0.6940436959266663\n",
            "Iteration: 351370 \t Train: Acc=50%, Loss=0.6941294074058533 \t\t Validation: Acc=47%, Loss=0.6982699632644653\n",
            "Iteration: 351380 \t Train: Acc=53%, Loss=0.6950267553329468 \t\t Validation: Acc=52%, Loss=0.6914173364639282\n",
            "Iteration: 351390 \t Train: Acc=51%, Loss=0.6909657120704651 \t\t Validation: Acc=49%, Loss=0.690033495426178\n",
            "Iteration: 351400 \t Train: Acc=50%, Loss=0.6950761079788208 \t\t Validation: Acc=50%, Loss=0.6938198208808899\n",
            "Iteration: 351410 \t Train: Acc=54%, Loss=0.6957785487174988 \t\t Validation: Acc=51%, Loss=0.6994476914405823\n",
            "Iteration: 351420 \t Train: Acc=51%, Loss=0.6903770565986633 \t\t Validation: Acc=50%, Loss=0.6902918815612793\n",
            "Iteration: 351430 \t Train: Acc=55%, Loss=0.6901487112045288 \t\t Validation: Acc=52%, Loss=0.6893436908721924\n",
            "Iteration: 351440 \t Train: Acc=49%, Loss=0.6941061019897461 \t\t Validation: Acc=48%, Loss=0.6929998993873596\n",
            "Iteration: 351450 \t Train: Acc=52%, Loss=0.6919800043106079 \t\t Validation: Acc=49%, Loss=0.6967589855194092\n",
            "Iteration: 351460 \t Train: Acc=52%, Loss=0.7035179138183594 \t\t Validation: Acc=50%, Loss=0.6887226104736328\n",
            "Iteration: 351470 \t Train: Acc=52%, Loss=0.6934611797332764 \t\t Validation: Acc=51%, Loss=0.6905425786972046\n",
            "Iteration: 351480 \t Train: Acc=49%, Loss=0.6933416128158569 \t\t Validation: Acc=50%, Loss=0.6864635944366455\n",
            "Iteration: 351490 \t Train: Acc=55%, Loss=0.6865273714065552 \t\t Validation: Acc=46%, Loss=0.693067193031311\n",
            "Iteration: 351500 \t Train: Acc=47%, Loss=0.6959561109542847 \t\t Validation: Acc=54%, Loss=0.6891090869903564\n",
            "Iteration: 351510 \t Train: Acc=47%, Loss=0.6926307082176208 \t\t Validation: Acc=49%, Loss=0.696116030216217\n",
            "Iteration: 351520 \t Train: Acc=55%, Loss=0.6919572949409485 \t\t Validation: Acc=53%, Loss=0.6936163902282715\n",
            "Iteration: 351530 \t Train: Acc=50%, Loss=0.6894519925117493 \t\t Validation: Acc=56%, Loss=0.6866669654846191\n",
            "Iteration: 351540 \t Train: Acc=52%, Loss=0.6902247667312622 \t\t Validation: Acc=53%, Loss=0.696530818939209\n",
            "Iteration: 351550 \t Train: Acc=52%, Loss=0.6915884017944336 \t\t Validation: Acc=50%, Loss=0.69190913438797\n",
            "Iteration: 351560 \t Train: Acc=53%, Loss=0.6898126006126404 \t\t Validation: Acc=49%, Loss=0.6933991312980652\n",
            "Iteration: 351570 \t Train: Acc=53%, Loss=0.6910752654075623 \t\t Validation: Acc=50%, Loss=0.6960333585739136\n",
            "Iteration: 351580 \t Train: Acc=44%, Loss=0.6926512122154236 \t\t Validation: Acc=55%, Loss=0.6893852353096008\n",
            "Iteration: 351590 \t Train: Acc=49%, Loss=0.6916509866714478 \t\t Validation: Acc=51%, Loss=0.6926672458648682\n",
            "Iteration: 351600 \t Train: Acc=51%, Loss=0.6931056976318359 \t\t Validation: Acc=50%, Loss=0.6895943880081177\n",
            "Iteration: 351610 \t Train: Acc=50%, Loss=0.6916012763977051 \t\t Validation: Acc=51%, Loss=0.6872904300689697\n",
            "Iteration: 351620 \t Train: Acc=53%, Loss=0.6895559430122375 \t\t Validation: Acc=55%, Loss=0.6868166923522949\n",
            "Iteration: 351630 \t Train: Acc=53%, Loss=0.6876598000526428 \t\t Validation: Acc=53%, Loss=0.6878231763839722\n",
            "Iteration: 351640 \t Train: Acc=43%, Loss=0.695798933506012 \t\t Validation: Acc=53%, Loss=0.6861071586608887\n",
            "Iteration: 351650 \t Train: Acc=50%, Loss=0.6927076578140259 \t\t Validation: Acc=51%, Loss=0.6957038044929504\n",
            "Iteration: 351660 \t Train: Acc=53%, Loss=0.6910265684127808 \t\t Validation: Acc=50%, Loss=0.6936627626419067\n",
            "Iteration: 351670 \t Train: Acc=54%, Loss=0.6873100399971008 \t\t Validation: Acc=44%, Loss=0.6983047127723694\n",
            "Iteration: 351680 \t Train: Acc=51%, Loss=0.6917687058448792 \t\t Validation: Acc=46%, Loss=0.7001993656158447\n",
            "Iteration: 351690 \t Train: Acc=57%, Loss=0.6919049024581909 \t\t Validation: Acc=50%, Loss=0.6938875317573547\n",
            "Iteration: 351700 \t Train: Acc=56%, Loss=0.6885332465171814 \t\t Validation: Acc=50%, Loss=0.6869507431983948\n",
            "Iteration: 351710 \t Train: Acc=53%, Loss=0.693031907081604 \t\t Validation: Acc=52%, Loss=0.6890891790390015\n",
            "Iteration: 351720 \t Train: Acc=50%, Loss=0.6899603009223938 \t\t Validation: Acc=50%, Loss=0.6924193501472473\n",
            "Iteration: 351730 \t Train: Acc=53%, Loss=0.6862667798995972 \t\t Validation: Acc=48%, Loss=0.696628749370575\n",
            "Iteration: 351740 \t Train: Acc=52%, Loss=0.6901321411132812 \t\t Validation: Acc=47%, Loss=0.6908099055290222\n",
            "Iteration: 351750 \t Train: Acc=48%, Loss=0.6920201778411865 \t\t Validation: Acc=51%, Loss=0.6894510984420776\n",
            "Iteration: 351760 \t Train: Acc=52%, Loss=0.6891307234764099 \t\t Validation: Acc=51%, Loss=0.6922656297683716\n",
            "It's been too long since we last saved the model. Saving...\n",
            "Iteration: 351770 \t Train: Acc=44%, Loss=0.7006232738494873 \t\t Validation: Acc=46%, Loss=0.6951607465744019\n",
            "Iteration: 351780 \t Train: Acc=48%, Loss=0.6887038350105286 \t\t Validation: Acc=48%, Loss=0.6979690194129944\n",
            "Iteration: 351790 \t Train: Acc=49%, Loss=0.6904825568199158 \t\t Validation: Acc=46%, Loss=0.695040225982666\n",
            "Iteration: 351800 \t Train: Acc=50%, Loss=0.696344256401062 \t\t Validation: Acc=48%, Loss=0.6964382529258728\n",
            "Iteration: 351810 \t Train: Acc=53%, Loss=0.690518856048584 \t\t Validation: Acc=56%, Loss=0.6894928216934204\n",
            "Iteration: 351820 \t Train: Acc=48%, Loss=0.6941088438034058 \t\t Validation: Acc=49%, Loss=0.6951553821563721\n",
            "Iteration: 351830 \t Train: Acc=50%, Loss=0.6892197132110596 \t\t Validation: Acc=50%, Loss=0.6910054087638855\n",
            "Iteration: 351840 \t Train: Acc=53%, Loss=0.6912095546722412 \t\t Validation: Acc=54%, Loss=0.6892818808555603\n",
            "Iteration: 351850 \t Train: Acc=51%, Loss=0.695381224155426 \t\t Validation: Acc=53%, Loss=0.6900018453598022\n",
            "Iteration: 351860 \t Train: Acc=56%, Loss=0.6825578212738037 \t\t Validation: Acc=50%, Loss=0.6947643756866455\n",
            "Iteration: 351870 \t Train: Acc=53%, Loss=0.6894572973251343 \t\t Validation: Acc=52%, Loss=0.6887177228927612\n",
            "Iteration: 351880 \t Train: Acc=48%, Loss=0.6928991675376892 \t\t Validation: Acc=50%, Loss=0.6914246678352356\n",
            "Iteration: 351890 \t Train: Acc=52%, Loss=0.6889029741287231 \t\t Validation: Acc=50%, Loss=0.6973844170570374\n",
            "Iteration: 351900 \t Train: Acc=43%, Loss=0.7004696130752563 \t\t Validation: Acc=50%, Loss=0.6943234205245972\n",
            "Iteration: 351910 \t Train: Acc=54%, Loss=0.6862072348594666 \t\t Validation: Acc=46%, Loss=0.6992006301879883\n",
            "Iteration: 351920 \t Train: Acc=46%, Loss=0.6958527565002441 \t\t Validation: Acc=50%, Loss=0.6937288045883179\n",
            "Iteration: 351930 \t Train: Acc=50%, Loss=0.6931062936782837 \t\t Validation: Acc=47%, Loss=0.6915754675865173\n",
            "Iteration: 351940 \t Train: Acc=50%, Loss=0.6883801817893982 \t\t Validation: Acc=52%, Loss=0.6959949731826782\n",
            "Iteration: 351950 \t Train: Acc=53%, Loss=0.6933552622795105 \t\t Validation: Acc=50%, Loss=0.6895745396614075\n",
            "Iteration: 351960 \t Train: Acc=50%, Loss=0.6996225118637085 \t\t Validation: Acc=53%, Loss=0.6982177495956421\n",
            "Iteration: 351970 \t Train: Acc=50%, Loss=0.6865118145942688 \t\t Validation: Acc=48%, Loss=0.6938690543174744\n",
            "Iteration: 351980 \t Train: Acc=53%, Loss=0.6837856769561768 \t\t Validation: Acc=50%, Loss=0.6962456107139587\n",
            "Iteration: 351990 \t Train: Acc=50%, Loss=0.702335774898529 \t\t Validation: Acc=54%, Loss=0.6912263035774231\n",
            "Iteration: 352000 \t Train: Acc=57%, Loss=0.6880186796188354 \t\t Validation: Acc=49%, Loss=0.6908667087554932\n",
            "Iteration: 352010 \t Train: Acc=56%, Loss=0.6813001036643982 \t\t Validation: Acc=50%, Loss=0.689727783203125\n",
            "Iteration: 352020 \t Train: Acc=56%, Loss=0.6873317956924438 \t\t Validation: Acc=52%, Loss=0.6850773692131042\n",
            "Iteration: 352030 \t Train: Acc=46%, Loss=0.6930134892463684 \t\t Validation: Acc=53%, Loss=0.6933562755584717\n",
            "Iteration: 352040 \t Train: Acc=60%, Loss=0.6830856800079346 \t\t Validation: Acc=46%, Loss=0.6937346458435059\n",
            "Iteration: 352050 \t Train: Acc=56%, Loss=0.6917048692703247 \t\t Validation: Acc=50%, Loss=0.6916117072105408\n",
            "Iteration: 352060 \t Train: Acc=47%, Loss=0.6941822171211243 \t\t Validation: Acc=46%, Loss=0.6974507570266724\n",
            "Iteration: 352070 \t Train: Acc=44%, Loss=0.6969984173774719 \t\t Validation: Acc=48%, Loss=0.6902438402175903\n",
            "Iteration: 352080 \t Train: Acc=49%, Loss=0.6957195997238159 \t\t Validation: Acc=51%, Loss=0.6877810955047607\n",
            "Iteration: 352090 \t Train: Acc=54%, Loss=0.6966016888618469 \t\t Validation: Acc=50%, Loss=0.6924182176589966\n",
            "Iteration: 352100 \t Train: Acc=50%, Loss=0.6933993697166443 \t\t Validation: Acc=50%, Loss=0.685831606388092\n",
            "Iteration: 352110 \t Train: Acc=56%, Loss=0.6907680034637451 \t\t Validation: Acc=50%, Loss=0.6936037540435791\n",
            "Iteration: 352120 \t Train: Acc=50%, Loss=0.6886839866638184 \t\t Validation: Acc=52%, Loss=0.6901883482933044\n",
            "Iteration: 352130 \t Train: Acc=50%, Loss=0.6880947947502136 \t\t Validation: Acc=47%, Loss=0.6980509161949158\n",
            "Iteration: 352140 \t Train: Acc=49%, Loss=0.6918370723724365 \t\t Validation: Acc=53%, Loss=0.6898923516273499\n",
            "Iteration: 352150 \t Train: Acc=50%, Loss=0.6933557391166687 \t\t Validation: Acc=53%, Loss=0.6897475719451904\n",
            "Iteration: 352160 \t Train: Acc=48%, Loss=0.692871630191803 \t\t Validation: Acc=50%, Loss=0.6945494413375854\n",
            "Iteration: 352170 \t Train: Acc=57%, Loss=0.687600314617157 \t\t Validation: Acc=46%, Loss=0.6936339735984802\n",
            "Iteration: 352180 \t Train: Acc=53%, Loss=0.6894565224647522 \t\t Validation: Acc=52%, Loss=0.6930190324783325\n",
            "Iteration: 352190 \t Train: Acc=50%, Loss=0.6995291113853455 \t\t Validation: Acc=51%, Loss=0.6957951784133911\n",
            "Iteration: 352200 \t Train: Acc=46%, Loss=0.6947158575057983 \t\t Validation: Acc=48%, Loss=0.6975772976875305\n",
            "Iteration: 352210 \t Train: Acc=46%, Loss=0.6993399262428284 \t\t Validation: Acc=55%, Loss=0.6884109377861023\n",
            "Iteration: 352220 \t Train: Acc=56%, Loss=0.6896622776985168 \t\t Validation: Acc=50%, Loss=0.6908058524131775\n",
            "Iteration: 352230 \t Train: Acc=46%, Loss=0.6976056098937988 \t\t Validation: Acc=54%, Loss=0.6908342242240906\n",
            "Iteration: 352240 \t Train: Acc=49%, Loss=0.6904866695404053 \t\t Validation: Acc=53%, Loss=0.6901753544807434\n",
            "Iteration: 352250 \t Train: Acc=57%, Loss=0.6911534667015076 \t\t Validation: Acc=50%, Loss=0.6915634274482727\n",
            "Iteration: 352260 \t Train: Acc=57%, Loss=0.6898746490478516 \t\t Validation: Acc=52%, Loss=0.6960561871528625\n",
            "Iteration: 352270 \t Train: Acc=48%, Loss=0.695363461971283 \t\t Validation: Acc=50%, Loss=0.6950255036354065\n",
            "Iteration: 352280 \t Train: Acc=47%, Loss=0.6918216943740845 \t\t Validation: Acc=54%, Loss=0.6863217353820801\n",
            "Iteration: 352290 \t Train: Acc=53%, Loss=0.6909257173538208 \t\t Validation: Acc=46%, Loss=0.696079432964325\n",
            "Iteration: 352300 \t Train: Acc=53%, Loss=0.6911756992340088 \t\t Validation: Acc=50%, Loss=0.6972928643226624\n",
            "Iteration: 352310 \t Train: Acc=46%, Loss=0.6933667063713074 \t\t Validation: Acc=51%, Loss=0.6933501362800598\n",
            "Iteration: 352320 \t Train: Acc=53%, Loss=0.6911588907241821 \t\t Validation: Acc=49%, Loss=0.6996123790740967\n",
            "Iteration: 352330 \t Train: Acc=52%, Loss=0.6860133409500122 \t\t Validation: Acc=53%, Loss=0.6958138346672058\n",
            "Iteration: 352340 \t Train: Acc=59%, Loss=0.6837371587753296 \t\t Validation: Acc=50%, Loss=0.6881438493728638\n",
            "Iteration: 352350 \t Train: Acc=47%, Loss=0.6941590309143066 \t\t Validation: Acc=50%, Loss=0.7029389142990112\n",
            "Iteration: 352360 \t Train: Acc=46%, Loss=0.6894345283508301 \t\t Validation: Acc=50%, Loss=0.6889594793319702\n",
            "Iteration: 352370 \t Train: Acc=50%, Loss=0.6957870721817017 \t\t Validation: Acc=52%, Loss=0.687151312828064\n",
            "Iteration: 352380 \t Train: Acc=53%, Loss=0.689182698726654 \t\t Validation: Acc=48%, Loss=0.6953390836715698\n",
            "Iteration: 352390 \t Train: Acc=56%, Loss=0.6904694437980652 \t\t Validation: Acc=50%, Loss=0.6903592944145203\n",
            "Iteration: 352400 \t Train: Acc=46%, Loss=0.6957311034202576 \t\t Validation: Acc=50%, Loss=0.6887874603271484\n",
            "Iteration: 352410 \t Train: Acc=49%, Loss=0.6880366802215576 \t\t Validation: Acc=53%, Loss=0.6886022686958313\n",
            "Iteration: 352420 \t Train: Acc=53%, Loss=0.6925851702690125 \t\t Validation: Acc=46%, Loss=0.6983247995376587\n",
            "Iteration: 352430 \t Train: Acc=55%, Loss=0.6921277642250061 \t\t Validation: Acc=50%, Loss=0.6932289600372314\n",
            "Iteration: 352440 \t Train: Acc=56%, Loss=0.6882150769233704 \t\t Validation: Acc=49%, Loss=0.6933940052986145\n",
            "Iteration: 352450 \t Train: Acc=53%, Loss=0.6808798909187317 \t\t Validation: Acc=50%, Loss=0.6912200450897217\n",
            "Iteration: 352460 \t Train: Acc=50%, Loss=0.6921616792678833 \t\t Validation: Acc=50%, Loss=0.6989890336990356\n",
            "Iteration: 352470 \t Train: Acc=51%, Loss=0.6908529996871948 \t\t Validation: Acc=47%, Loss=0.6947569251060486\n",
            "Iteration: 352480 \t Train: Acc=51%, Loss=0.6911991834640503 \t\t Validation: Acc=55%, Loss=0.6885754466056824\n",
            "Iteration: 352490 \t Train: Acc=50%, Loss=0.6946022510528564 \t\t Validation: Acc=48%, Loss=0.6907157301902771\n",
            "Iteration: 352500 \t Train: Acc=50%, Loss=0.6962804198265076 \t\t Validation: Acc=54%, Loss=0.6866940259933472\n",
            "Iteration: 352510 \t Train: Acc=52%, Loss=0.690609335899353 \t\t Validation: Acc=54%, Loss=0.6864241361618042\n",
            "Iteration: 352520 \t Train: Acc=55%, Loss=0.6895343065261841 \t\t Validation: Acc=50%, Loss=0.6874275803565979\n",
            "Iteration: 352530 \t Train: Acc=50%, Loss=0.6925131678581238 \t\t Validation: Acc=49%, Loss=0.6882929801940918\n",
            "Iteration: 352540 \t Train: Acc=50%, Loss=0.6969550251960754 \t\t Validation: Acc=48%, Loss=0.6909003257751465\n",
            "Iteration: 352550 \t Train: Acc=57%, Loss=0.682062029838562 \t\t Validation: Acc=50%, Loss=0.6951839327812195\n",
            "Iteration: 352560 \t Train: Acc=47%, Loss=0.6989094614982605 \t\t Validation: Acc=52%, Loss=0.6961560249328613\n",
            "Iteration: 352570 \t Train: Acc=49%, Loss=0.6939454674720764 \t\t Validation: Acc=55%, Loss=0.6914094686508179\n",
            "Iteration: 352580 \t Train: Acc=51%, Loss=0.6908565759658813 \t\t Validation: Acc=52%, Loss=0.6852688789367676\n",
            "Iteration: 352590 \t Train: Acc=53%, Loss=0.6892255544662476 \t\t Validation: Acc=52%, Loss=0.689085066318512\n",
            "Iteration: 352600 \t Train: Acc=47%, Loss=0.6942787170410156 \t\t Validation: Acc=51%, Loss=0.6951313018798828\n",
            "Iteration: 352610 \t Train: Acc=49%, Loss=0.689274787902832 \t\t Validation: Acc=51%, Loss=0.6876338124275208\n",
            "Iteration: 352620 \t Train: Acc=50%, Loss=0.6915655136108398 \t\t Validation: Acc=51%, Loss=0.6940190196037292\n",
            "Iteration: 352630 \t Train: Acc=51%, Loss=0.6895796656608582 \t\t Validation: Acc=52%, Loss=0.6880511045455933\n",
            "Iteration: 352640 \t Train: Acc=43%, Loss=0.7030973434448242 \t\t Validation: Acc=49%, Loss=0.6902766227722168\n",
            "Iteration: 352650 \t Train: Acc=46%, Loss=0.6962597370147705 \t\t Validation: Acc=47%, Loss=0.6917837858200073\n",
            "Iteration: 352660 \t Train: Acc=54%, Loss=0.6847651600837708 \t\t Validation: Acc=50%, Loss=0.6919525861740112\n",
            "Iteration: 352670 \t Train: Acc=51%, Loss=0.687546968460083 \t\t Validation: Acc=47%, Loss=0.7003058791160583\n",
            "Iteration: 352680 \t Train: Acc=49%, Loss=0.6871300339698792 \t\t Validation: Acc=52%, Loss=0.6872382760047913\n",
            "Iteration: 352690 \t Train: Acc=55%, Loss=0.680742621421814 \t\t Validation: Acc=52%, Loss=0.6845681071281433\n",
            "Iteration: 352700 \t Train: Acc=56%, Loss=0.6857205033302307 \t\t Validation: Acc=48%, Loss=0.6930434703826904\n",
            "Iteration: 352710 \t Train: Acc=48%, Loss=0.6943931579589844 \t\t Validation: Acc=50%, Loss=0.6861546039581299\n",
            "Iteration: 352720 \t Train: Acc=48%, Loss=0.6898593306541443 \t\t Validation: Acc=48%, Loss=0.7003584504127502\n",
            "Iteration: 352730 \t Train: Acc=50%, Loss=0.6910743117332458 \t\t Validation: Acc=45%, Loss=0.7048184871673584\n",
            "Iteration: 352740 \t Train: Acc=48%, Loss=0.6920140385627747 \t\t Validation: Acc=48%, Loss=0.704054594039917\n",
            "Iteration: 352750 \t Train: Acc=47%, Loss=0.6973096132278442 \t\t Validation: Acc=50%, Loss=0.7047442197799683\n",
            "Iteration: 352760 \t Train: Acc=47%, Loss=0.6962252259254456 \t\t Validation: Acc=51%, Loss=0.6943215131759644\n",
            "Iteration: 352770 \t Train: Acc=50%, Loss=0.6901258826255798 \t\t Validation: Acc=50%, Loss=0.6959633827209473\n",
            "Iteration: 352780 \t Train: Acc=55%, Loss=0.6905211210250854 \t\t Validation: Acc=50%, Loss=0.6906346082687378\n",
            "Iteration: 352790 \t Train: Acc=47%, Loss=0.6975306868553162 \t\t Validation: Acc=50%, Loss=0.6956506967544556\n",
            "Iteration: 352800 \t Train: Acc=50%, Loss=0.6879929304122925 \t\t Validation: Acc=50%, Loss=0.6889529228210449\n",
            "Iteration: 352810 \t Train: Acc=48%, Loss=0.6904367804527283 \t\t Validation: Acc=51%, Loss=0.6931105256080627\n",
            "Iteration: 352820 \t Train: Acc=51%, Loss=0.6910229921340942 \t\t Validation: Acc=52%, Loss=0.684039831161499\n",
            "Iteration: 352830 \t Train: Acc=50%, Loss=0.6894724369049072 \t\t Validation: Acc=51%, Loss=0.6954934000968933\n",
            "Iteration: 352840 \t Train: Acc=54%, Loss=0.689480721950531 \t\t Validation: Acc=49%, Loss=0.6849505305290222\n",
            "Iteration: 352850 \t Train: Acc=52%, Loss=0.6854682564735413 \t\t Validation: Acc=51%, Loss=0.7040843963623047\n",
            "Iteration: 352860 \t Train: Acc=48%, Loss=0.6924847364425659 \t\t Validation: Acc=52%, Loss=0.6963615417480469\n",
            "Iteration: 352870 \t Train: Acc=59%, Loss=0.6882491707801819 \t\t Validation: Acc=51%, Loss=0.698683500289917\n",
            "Iteration: 352880 \t Train: Acc=49%, Loss=0.699001669883728 \t\t Validation: Acc=51%, Loss=0.6934432983398438\n",
            "Iteration: 352890 \t Train: Acc=50%, Loss=0.6961206793785095 \t\t Validation: Acc=50%, Loss=0.6902889609336853\n",
            "Iteration: 352900 \t Train: Acc=50%, Loss=0.6898229122161865 \t\t Validation: Acc=50%, Loss=0.7060109972953796\n",
            "Iteration: 352910 \t Train: Acc=49%, Loss=0.6928346157073975 \t\t Validation: Acc=50%, Loss=0.6967429518699646\n",
            "Iteration: 352920 \t Train: Acc=48%, Loss=0.6962689757347107 \t\t Validation: Acc=49%, Loss=0.6930753588676453\n",
            "Iteration: 352930 \t Train: Acc=51%, Loss=0.6907042860984802 \t\t Validation: Acc=50%, Loss=0.7013190984725952\n",
            "Iteration: 352940 \t Train: Acc=56%, Loss=0.682336688041687 \t\t Validation: Acc=50%, Loss=0.7007209658622742\n",
            "Iteration: 352950 \t Train: Acc=53%, Loss=0.6883375644683838 \t\t Validation: Acc=51%, Loss=0.6957509517669678\n",
            "Iteration: 352960 \t Train: Acc=50%, Loss=0.6957637071609497 \t\t Validation: Acc=50%, Loss=0.6896793842315674\n",
            "Iteration: 352970 \t Train: Acc=50%, Loss=0.692065417766571 \t\t Validation: Acc=52%, Loss=0.6886524558067322\n",
            "Iteration: 352980 \t Train: Acc=58%, Loss=0.6849771738052368 \t\t Validation: Acc=48%, Loss=0.700495183467865\n",
            "Iteration: 352990 \t Train: Acc=53%, Loss=0.6898264288902283 \t\t Validation: Acc=48%, Loss=0.6980600357055664\n",
            "Iteration: 353000 \t Train: Acc=50%, Loss=0.6911656856536865 \t\t Validation: Acc=50%, Loss=0.6965209245681763\n",
            "Iteration: 353010 \t Train: Acc=45%, Loss=0.703831672668457 \t\t Validation: Acc=50%, Loss=0.6899186372756958\n",
            "Iteration: 353020 \t Train: Acc=51%, Loss=0.6962428092956543 \t\t Validation: Acc=50%, Loss=0.6889025568962097\n",
            "Iteration: 353030 \t Train: Acc=46%, Loss=0.6948121190071106 \t\t Validation: Acc=50%, Loss=0.7071950435638428\n",
            "Iteration: 353040 \t Train: Acc=45%, Loss=0.7039456367492676 \t\t Validation: Acc=50%, Loss=0.688927173614502\n",
            "Iteration: 353050 \t Train: Acc=43%, Loss=0.6947360634803772 \t\t Validation: Acc=49%, Loss=0.6950849890708923\n",
            "Iteration: 353060 \t Train: Acc=53%, Loss=0.6885173916816711 \t\t Validation: Acc=52%, Loss=0.6909196376800537\n",
            "Iteration: 353070 \t Train: Acc=51%, Loss=0.6987561583518982 \t\t Validation: Acc=50%, Loss=0.681928813457489\n",
            "Iteration: 353080 \t Train: Acc=59%, Loss=0.6825417876243591 \t\t Validation: Acc=50%, Loss=0.7011421918869019\n",
            "Iteration: 353090 \t Train: Acc=45%, Loss=0.6966821551322937 \t\t Validation: Acc=47%, Loss=0.696442186832428\n",
            "Iteration: 353100 \t Train: Acc=47%, Loss=0.6944478154182434 \t\t Validation: Acc=50%, Loss=0.6790587902069092\n",
            "Iteration: 353110 \t Train: Acc=54%, Loss=0.6975462436676025 \t\t Validation: Acc=49%, Loss=0.6958889961242676\n",
            "Iteration: 353120 \t Train: Acc=48%, Loss=0.6971918940544128 \t\t Validation: Acc=50%, Loss=0.700421929359436\n",
            "Iteration: 353130 \t Train: Acc=50%, Loss=0.6903921961784363 \t\t Validation: Acc=48%, Loss=0.7061052322387695\n",
            "Iteration: 353140 \t Train: Acc=44%, Loss=0.6988123059272766 \t\t Validation: Acc=50%, Loss=0.6914196014404297\n",
            "Iteration: 353150 \t Train: Acc=50%, Loss=0.6932979226112366 \t\t Validation: Acc=50%, Loss=0.6926286220550537\n",
            "Iteration: 353160 \t Train: Acc=46%, Loss=0.6976280808448792 \t\t Validation: Acc=50%, Loss=0.688892662525177\n",
            "Iteration: 353170 \t Train: Acc=44%, Loss=0.6971360445022583 \t\t Validation: Acc=49%, Loss=0.6946298480033875\n",
            "Iteration: 353180 \t Train: Acc=50%, Loss=0.6930261850357056 \t\t Validation: Acc=46%, Loss=0.7029250264167786\n",
            "Iteration: 353190 \t Train: Acc=51%, Loss=0.6913054585456848 \t\t Validation: Acc=49%, Loss=0.699164628982544\n",
            "Iteration: 353200 \t Train: Acc=55%, Loss=0.6919387578964233 \t\t Validation: Acc=50%, Loss=0.6971532106399536\n",
            "Iteration: 353210 \t Train: Acc=50%, Loss=0.697380781173706 \t\t Validation: Acc=50%, Loss=0.6925156116485596\n",
            "Iteration: 353220 \t Train: Acc=52%, Loss=0.6832742691040039 \t\t Validation: Acc=49%, Loss=0.6973544359207153\n",
            "Iteration: 353230 \t Train: Acc=49%, Loss=0.6925332546234131 \t\t Validation: Acc=51%, Loss=0.6876766085624695\n",
            "Iteration: 353240 \t Train: Acc=52%, Loss=0.6899608969688416 \t\t Validation: Acc=47%, Loss=0.6894433498382568\n",
            "Iteration: 353250 \t Train: Acc=51%, Loss=0.6928088068962097 \t\t Validation: Acc=50%, Loss=0.6932029724121094\n",
            "Iteration: 353260 \t Train: Acc=48%, Loss=0.6942765712738037 \t\t Validation: Acc=50%, Loss=0.6949023604393005\n",
            "Iteration: 353270 \t Train: Acc=50%, Loss=0.6909297704696655 \t\t Validation: Acc=50%, Loss=0.7001882195472717\n",
            "Iteration: 353280 \t Train: Acc=50%, Loss=0.6902811527252197 \t\t Validation: Acc=47%, Loss=0.7036222219467163\n",
            "Iteration: 353290 \t Train: Acc=55%, Loss=0.6837928891181946 \t\t Validation: Acc=51%, Loss=0.6900356411933899\n",
            "Iteration: 353300 \t Train: Acc=56%, Loss=0.6856441497802734 \t\t Validation: Acc=52%, Loss=0.6943542957305908\n",
            "Iteration: 353310 \t Train: Acc=48%, Loss=0.6972988843917847 \t\t Validation: Acc=50%, Loss=0.7004163861274719\n",
            "Iteration: 353320 \t Train: Acc=51%, Loss=0.6881747245788574 \t\t Validation: Acc=50%, Loss=0.6933921575546265\n",
            "Iteration: 353330 \t Train: Acc=49%, Loss=0.6992557048797607 \t\t Validation: Acc=49%, Loss=0.6948163509368896\n",
            "Iteration: 353340 \t Train: Acc=55%, Loss=0.6868610978126526 \t\t Validation: Acc=50%, Loss=0.6935546398162842\n",
            "Iteration: 353350 \t Train: Acc=53%, Loss=0.6947106122970581 \t\t Validation: Acc=50%, Loss=0.6978495717048645\n",
            "Iteration: 353360 \t Train: Acc=57%, Loss=0.6848162412643433 \t\t Validation: Acc=50%, Loss=0.6932076215744019\n",
            "Iteration: 353370 \t Train: Acc=52%, Loss=0.6807166337966919 \t\t Validation: Acc=47%, Loss=0.6999272108078003\n",
            "Iteration: 353380 \t Train: Acc=48%, Loss=0.6904129981994629 \t\t Validation: Acc=50%, Loss=0.6957070827484131\n",
            "Iteration: 353390 \t Train: Acc=51%, Loss=0.6917996406555176 \t\t Validation: Acc=51%, Loss=0.6920406818389893\n",
            "Iteration: 353400 \t Train: Acc=43%, Loss=0.6973204016685486 \t\t Validation: Acc=50%, Loss=0.6877143979072571\n",
            "Iteration: 353410 \t Train: Acc=47%, Loss=0.6888377070426941 \t\t Validation: Acc=51%, Loss=0.6836736798286438\n",
            "Iteration: 353420 \t Train: Acc=50%, Loss=0.6930009722709656 \t\t Validation: Acc=52%, Loss=0.6978290677070618\n",
            "Iteration: 353430 \t Train: Acc=55%, Loss=0.6823431849479675 \t\t Validation: Acc=49%, Loss=0.6868985891342163\n",
            "Iteration: 353440 \t Train: Acc=57%, Loss=0.687400221824646 \t\t Validation: Acc=48%, Loss=0.6950826644897461\n",
            "Iteration: 353450 \t Train: Acc=49%, Loss=0.6942564249038696 \t\t Validation: Acc=50%, Loss=0.6976736783981323\n",
            "Iteration: 353460 \t Train: Acc=50%, Loss=0.6907660365104675 \t\t Validation: Acc=46%, Loss=0.7077478170394897\n",
            "Iteration: 353470 \t Train: Acc=49%, Loss=0.6933360695838928 \t\t Validation: Acc=52%, Loss=0.6865330934524536\n",
            "Iteration: 353480 \t Train: Acc=56%, Loss=0.68839430809021 \t\t Validation: Acc=51%, Loss=0.6874866485595703\n",
            "Iteration: 353490 \t Train: Acc=52%, Loss=0.690876305103302 \t\t Validation: Acc=50%, Loss=0.7029001712799072\n",
            "Iteration: 353500 \t Train: Acc=51%, Loss=0.6917896866798401 \t\t Validation: Acc=50%, Loss=0.6951943039894104\n",
            "Iteration: 353510 \t Train: Acc=50%, Loss=0.6925278306007385 \t\t Validation: Acc=51%, Loss=0.6912720203399658\n",
            "Iteration: 353520 \t Train: Acc=50%, Loss=0.6926303505897522 \t\t Validation: Acc=51%, Loss=0.6930855512619019\n",
            "Iteration: 353530 \t Train: Acc=49%, Loss=0.6933961510658264 \t\t Validation: Acc=48%, Loss=0.7045290470123291\n",
            "Iteration: 353540 \t Train: Acc=52%, Loss=0.6900752186775208 \t\t Validation: Acc=51%, Loss=0.7013395428657532\n",
            "Iteration: 353550 \t Train: Acc=49%, Loss=0.6904546022415161 \t\t Validation: Acc=50%, Loss=0.7055423855781555\n",
            "Iteration: 353560 \t Train: Acc=52%, Loss=0.6933654546737671 \t\t Validation: Acc=50%, Loss=0.6861236691474915\n",
            "Iteration: 353570 \t Train: Acc=57%, Loss=0.6888335347175598 \t\t Validation: Acc=51%, Loss=0.6903212666511536\n",
            "Iteration: 353580 \t Train: Acc=48%, Loss=0.6959319710731506 \t\t Validation: Acc=53%, Loss=0.6977697014808655\n",
            "Iteration: 353590 \t Train: Acc=58%, Loss=0.6933229565620422 \t\t Validation: Acc=47%, Loss=0.7000024914741516\n",
            "Iteration: 353600 \t Train: Acc=58%, Loss=0.6844815611839294 \t\t Validation: Acc=50%, Loss=0.6909469962120056\n",
            "Iteration: 353610 \t Train: Acc=51%, Loss=0.6882249712944031 \t\t Validation: Acc=50%, Loss=0.7019391059875488\n",
            "Iteration: 353620 \t Train: Acc=48%, Loss=0.696625828742981 \t\t Validation: Acc=50%, Loss=0.6938029527664185\n",
            "Iteration: 353630 \t Train: Acc=49%, Loss=0.6996856331825256 \t\t Validation: Acc=50%, Loss=0.6893126964569092\n",
            "Iteration: 353640 \t Train: Acc=49%, Loss=0.6907477378845215 \t\t Validation: Acc=51%, Loss=0.6896852850914001\n",
            "Iteration: 353650 \t Train: Acc=55%, Loss=0.691763162612915 \t\t Validation: Acc=48%, Loss=0.6879399418830872\n",
            "Iteration: 353660 \t Train: Acc=42%, Loss=0.6992067694664001 \t\t Validation: Acc=47%, Loss=0.691533625125885\n",
            "Iteration: 353670 \t Train: Acc=54%, Loss=0.6866926550865173 \t\t Validation: Acc=46%, Loss=0.704335629940033\n",
            "Iteration: 353680 \t Train: Acc=54%, Loss=0.684524416923523 \t\t Validation: Acc=46%, Loss=0.6983148455619812\n",
            "Iteration: 353690 \t Train: Acc=50%, Loss=0.6942435503005981 \t\t Validation: Acc=52%, Loss=0.6958053112030029\n",
            "Iteration: 353700 \t Train: Acc=46%, Loss=0.6928529143333435 \t\t Validation: Acc=46%, Loss=0.6962180137634277\n",
            "Iteration: 353710 \t Train: Acc=53%, Loss=0.6890654563903809 \t\t Validation: Acc=49%, Loss=0.695317268371582\n",
            "Iteration: 353720 \t Train: Acc=45%, Loss=0.6964236497879028 \t\t Validation: Acc=50%, Loss=0.6893810033798218\n",
            "Iteration: 353730 \t Train: Acc=52%, Loss=0.6922484636306763 \t\t Validation: Acc=53%, Loss=0.6757248044013977\n",
            "Iteration: 353740 \t Train: Acc=46%, Loss=0.7034322619438171 \t\t Validation: Acc=53%, Loss=0.6868765950202942\n",
            "Iteration: 353750 \t Train: Acc=52%, Loss=0.6916630864143372 \t\t Validation: Acc=50%, Loss=0.69257652759552\n",
            "Iteration: 353760 \t Train: Acc=47%, Loss=0.6863648891448975 \t\t Validation: Acc=47%, Loss=0.6954182982444763\n",
            "Iteration: 353770 \t Train: Acc=51%, Loss=0.690233588218689 \t\t Validation: Acc=48%, Loss=0.6901039481163025\n",
            "Iteration: 353780 \t Train: Acc=48%, Loss=0.6943511366844177 \t\t Validation: Acc=50%, Loss=0.6974038481712341\n",
            "Iteration: 353790 \t Train: Acc=50%, Loss=0.6926847696304321 \t\t Validation: Acc=48%, Loss=0.7021555304527283\n",
            "Iteration: 353800 \t Train: Acc=49%, Loss=0.6989794373512268 \t\t Validation: Acc=50%, Loss=0.6934531927108765\n",
            "Iteration: 353810 \t Train: Acc=50%, Loss=0.6871242523193359 \t\t Validation: Acc=50%, Loss=0.6928768754005432\n",
            "Iteration: 353820 \t Train: Acc=59%, Loss=0.6925947666168213 \t\t Validation: Acc=50%, Loss=0.6898099780082703\n",
            "Iteration: 353830 \t Train: Acc=55%, Loss=0.6882080435752869 \t\t Validation: Acc=52%, Loss=0.6872756481170654\n",
            "Iteration: 353840 \t Train: Acc=52%, Loss=0.689452052116394 \t\t Validation: Acc=48%, Loss=0.6925216317176819\n",
            "Iteration: 353850 \t Train: Acc=51%, Loss=0.6898356080055237 \t\t Validation: Acc=53%, Loss=0.6931579113006592\n",
            "Iteration: 353860 \t Train: Acc=53%, Loss=0.6920337080955505 \t\t Validation: Acc=50%, Loss=0.6963909864425659\n",
            "Iteration: 353870 \t Train: Acc=49%, Loss=0.6829755902290344 \t\t Validation: Acc=51%, Loss=0.6975032091140747\n",
            "Iteration: 353880 \t Train: Acc=54%, Loss=0.6890712380409241 \t\t Validation: Acc=51%, Loss=0.7010869383811951\n",
            "Iteration: 353890 \t Train: Acc=52%, Loss=0.6838737726211548 \t\t Validation: Acc=49%, Loss=0.690443754196167\n",
            "Iteration: 353900 \t Train: Acc=50%, Loss=0.6897779703140259 \t\t Validation: Acc=51%, Loss=0.696631908416748\n",
            "Iteration: 353910 \t Train: Acc=50%, Loss=0.6974157094955444 \t\t Validation: Acc=47%, Loss=0.689071774482727\n",
            "Iteration: 353920 \t Train: Acc=50%, Loss=0.6931732892990112 \t\t Validation: Acc=50%, Loss=0.697365403175354\n",
            "Iteration: 353930 \t Train: Acc=55%, Loss=0.6867513656616211 \t\t Validation: Acc=48%, Loss=0.6880310773849487\n",
            "Iteration: 353940 \t Train: Acc=46%, Loss=0.6926542520523071 \t\t Validation: Acc=50%, Loss=0.6968207955360413\n",
            "Iteration: 353950 \t Train: Acc=56%, Loss=0.6867009997367859 \t\t Validation: Acc=48%, Loss=0.7022983431816101\n",
            "Iteration: 353960 \t Train: Acc=53%, Loss=0.6859467625617981 \t\t Validation: Acc=49%, Loss=0.6942681670188904\n",
            "Iteration: 353970 \t Train: Acc=53%, Loss=0.6920852661132812 \t\t Validation: Acc=49%, Loss=0.6937791109085083\n",
            "Iteration: 353980 \t Train: Acc=46%, Loss=0.6903008222579956 \t\t Validation: Acc=50%, Loss=0.6903895139694214\n",
            "Iteration: 353990 \t Train: Acc=49%, Loss=0.691188395023346 \t\t Validation: Acc=51%, Loss=0.6880402565002441\n",
            "Iteration: 354000 \t Train: Acc=46%, Loss=0.6986699104309082 \t\t Validation: Acc=50%, Loss=0.7009472250938416\n",
            "Iteration: 354010 \t Train: Acc=51%, Loss=0.6924595236778259 \t\t Validation: Acc=49%, Loss=0.6885749101638794\n",
            "Iteration: 354020 \t Train: Acc=53%, Loss=0.6925356388092041 \t\t Validation: Acc=51%, Loss=0.6939021348953247\n",
            "Iteration: 354030 \t Train: Acc=51%, Loss=0.6975265145301819 \t\t Validation: Acc=50%, Loss=0.6929330229759216\n",
            "Iteration: 354040 \t Train: Acc=50%, Loss=0.6910929679870605 \t\t Validation: Acc=51%, Loss=0.6955031156539917\n",
            "Iteration: 354050 \t Train: Acc=50%, Loss=0.6929796934127808 \t\t Validation: Acc=53%, Loss=0.6941990256309509\n",
            "Iteration: 354060 \t Train: Acc=53%, Loss=0.6902855038642883 \t\t Validation: Acc=51%, Loss=0.7062381505966187\n",
            "Iteration: 354070 \t Train: Acc=53%, Loss=0.6905773282051086 \t\t Validation: Acc=48%, Loss=0.6967902779579163\n",
            "Iteration: 354080 \t Train: Acc=53%, Loss=0.685822069644928 \t\t Validation: Acc=53%, Loss=0.690087616443634\n",
            "Iteration: 354090 \t Train: Acc=57%, Loss=0.6848096251487732 \t\t Validation: Acc=49%, Loss=0.6982890963554382\n",
            "Iteration: 354100 \t Train: Acc=46%, Loss=0.6970654726028442 \t\t Validation: Acc=50%, Loss=0.701905369758606\n",
            "Iteration: 354110 \t Train: Acc=53%, Loss=0.683735191822052 \t\t Validation: Acc=52%, Loss=0.6999329924583435\n",
            "Iteration: 354120 \t Train: Acc=45%, Loss=0.6927977800369263 \t\t Validation: Acc=53%, Loss=0.6883692741394043\n",
            "Iteration: 354130 \t Train: Acc=54%, Loss=0.6965047121047974 \t\t Validation: Acc=50%, Loss=0.6844712495803833\n",
            "Iteration: 354140 \t Train: Acc=52%, Loss=0.689933180809021 \t\t Validation: Acc=50%, Loss=0.6956601142883301\n",
            "Iteration: 354150 \t Train: Acc=46%, Loss=0.6952348351478577 \t\t Validation: Acc=47%, Loss=0.6926500797271729\n",
            "Iteration: 354160 \t Train: Acc=49%, Loss=0.6910403370857239 \t\t Validation: Acc=50%, Loss=0.6871919631958008\n",
            "Iteration: 354170 \t Train: Acc=54%, Loss=0.6872872114181519 \t\t Validation: Acc=52%, Loss=0.698798656463623\n",
            "Iteration: 354180 \t Train: Acc=51%, Loss=0.684944748878479 \t\t Validation: Acc=48%, Loss=0.6980911493301392\n",
            "Iteration: 354190 \t Train: Acc=55%, Loss=0.6878772974014282 \t\t Validation: Acc=55%, Loss=0.6832171082496643\n",
            "Iteration: 354200 \t Train: Acc=48%, Loss=0.6904062628746033 \t\t Validation: Acc=52%, Loss=0.6909734010696411\n",
            "Iteration: 354210 \t Train: Acc=53%, Loss=0.6863808631896973 \t\t Validation: Acc=52%, Loss=0.6873555779457092\n",
            "Iteration: 354220 \t Train: Acc=45%, Loss=0.6905710697174072 \t\t Validation: Acc=51%, Loss=0.6928592324256897\n",
            "Iteration: 354230 \t Train: Acc=51%, Loss=0.6866375803947449 \t\t Validation: Acc=48%, Loss=0.697330892086029\n",
            "Iteration: 354240 \t Train: Acc=50%, Loss=0.6950923204421997 \t\t Validation: Acc=50%, Loss=0.6920857429504395\n",
            "Iteration: 354250 \t Train: Acc=43%, Loss=0.7036025524139404 \t\t Validation: Acc=49%, Loss=0.6960847973823547\n",
            "Iteration: 354260 \t Train: Acc=46%, Loss=0.6922938823699951 \t\t Validation: Acc=53%, Loss=0.683574914932251\n",
            "Iteration: 354270 \t Train: Acc=50%, Loss=0.6898248195648193 \t\t Validation: Acc=50%, Loss=0.6907795071601868\n",
            "Iteration: 354280 \t Train: Acc=57%, Loss=0.6857046484947205 \t\t Validation: Acc=47%, Loss=0.6976410150527954\n",
            "Iteration: 354290 \t Train: Acc=49%, Loss=0.6924425363540649 \t\t Validation: Acc=50%, Loss=0.6934506893157959\n",
            "Iteration: 354300 \t Train: Acc=49%, Loss=0.6891924142837524 \t\t Validation: Acc=50%, Loss=0.6907501816749573\n",
            "Iteration: 354310 \t Train: Acc=52%, Loss=0.6863032579421997 \t\t Validation: Acc=53%, Loss=0.683087944984436\n",
            "Iteration: 354320 \t Train: Acc=56%, Loss=0.6861069798469543 \t\t Validation: Acc=50%, Loss=0.6909977793693542\n",
            "Iteration: 354330 \t Train: Acc=51%, Loss=0.7002732753753662 \t\t Validation: Acc=49%, Loss=0.6946356892585754\n",
            "Iteration: 354340 \t Train: Acc=53%, Loss=0.6874644756317139 \t\t Validation: Acc=50%, Loss=0.6910932064056396\n",
            "Iteration: 354350 \t Train: Acc=53%, Loss=0.6845730543136597 \t\t Validation: Acc=50%, Loss=0.6977192163467407\n",
            "Iteration: 354360 \t Train: Acc=58%, Loss=0.693525493144989 \t\t Validation: Acc=51%, Loss=0.6938610076904297\n",
            "Iteration: 354370 \t Train: Acc=50%, Loss=0.6899439692497253 \t\t Validation: Acc=50%, Loss=0.6855737566947937\n",
            "Iteration: 354380 \t Train: Acc=54%, Loss=0.6944739818572998 \t\t Validation: Acc=55%, Loss=0.695037305355072\n",
            "Iteration: 354390 \t Train: Acc=57%, Loss=0.6847279667854309 \t\t Validation: Acc=50%, Loss=0.6977718472480774\n",
            "Iteration: 354400 \t Train: Acc=49%, Loss=0.6926531791687012 \t\t Validation: Acc=49%, Loss=0.6932234764099121\n",
            "Iteration: 354410 \t Train: Acc=45%, Loss=0.6955127716064453 \t\t Validation: Acc=49%, Loss=0.6898406147956848\n",
            "Iteration: 354420 \t Train: Acc=50%, Loss=0.6871458292007446 \t\t Validation: Acc=48%, Loss=0.7004315853118896\n",
            "Iteration: 354430 \t Train: Acc=46%, Loss=0.6943696141242981 \t\t Validation: Acc=50%, Loss=0.6897732615470886\n",
            "Iteration: 354440 \t Train: Acc=47%, Loss=0.6926845908164978 \t\t Validation: Acc=51%, Loss=0.6900690197944641\n",
            "Iteration: 354450 \t Train: Acc=51%, Loss=0.6954399943351746 \t\t Validation: Acc=49%, Loss=0.6964051723480225\n",
            "Iteration: 354460 \t Train: Acc=51%, Loss=0.6937577724456787 \t\t Validation: Acc=53%, Loss=0.689441978931427\n",
            "Iteration: 354470 \t Train: Acc=48%, Loss=0.6922932863235474 \t\t Validation: Acc=50%, Loss=0.6982631087303162\n",
            "Iteration: 354480 \t Train: Acc=51%, Loss=0.6919494271278381 \t\t Validation: Acc=51%, Loss=0.6979299783706665\n",
            "Iteration: 354490 \t Train: Acc=47%, Loss=0.6879658699035645 \t\t Validation: Acc=50%, Loss=0.7028759717941284\n",
            "Iteration: 354500 \t Train: Acc=49%, Loss=0.6920003890991211 \t\t Validation: Acc=53%, Loss=0.7029532194137573\n",
            "Iteration: 354510 \t Train: Acc=51%, Loss=0.6893497705459595 \t\t Validation: Acc=48%, Loss=0.6960409283638\n",
            "Iteration: 354520 \t Train: Acc=47%, Loss=0.6971059441566467 \t\t Validation: Acc=50%, Loss=0.694831132888794\n",
            "Iteration: 354530 \t Train: Acc=49%, Loss=0.6921359896659851 \t\t Validation: Acc=50%, Loss=0.6949474811553955\n",
            "Iteration: 354540 \t Train: Acc=57%, Loss=0.6908404231071472 \t\t Validation: Acc=49%, Loss=0.6914032101631165\n",
            "Iteration: 354550 \t Train: Acc=53%, Loss=0.6892026662826538 \t\t Validation: Acc=52%, Loss=0.6883366107940674\n",
            "Iteration: 354560 \t Train: Acc=53%, Loss=0.698136568069458 \t\t Validation: Acc=51%, Loss=0.695469081401825\n",
            "Iteration: 354570 \t Train: Acc=51%, Loss=0.6944639682769775 \t\t Validation: Acc=49%, Loss=0.6901485323905945\n",
            "Iteration: 354580 \t Train: Acc=49%, Loss=0.6885122656822205 \t\t Validation: Acc=52%, Loss=0.6907331347465515\n",
            "Iteration: 354590 \t Train: Acc=44%, Loss=0.6933913826942444 \t\t Validation: Acc=50%, Loss=0.6927611231803894\n",
            "Iteration: 354600 \t Train: Acc=46%, Loss=0.6996310353279114 \t\t Validation: Acc=49%, Loss=0.697202742099762\n",
            "Iteration: 354610 \t Train: Acc=48%, Loss=0.6877660751342773 \t\t Validation: Acc=50%, Loss=0.694743275642395\n",
            "Iteration: 354620 \t Train: Acc=56%, Loss=0.6808415651321411 \t\t Validation: Acc=51%, Loss=0.6919413208961487\n",
            "Iteration: 354630 \t Train: Acc=46%, Loss=0.7006497383117676 \t\t Validation: Acc=50%, Loss=0.6982393264770508\n",
            "Iteration: 354640 \t Train: Acc=55%, Loss=0.6928220391273499 \t\t Validation: Acc=50%, Loss=0.6854617595672607\n",
            "Iteration: 354650 \t Train: Acc=50%, Loss=0.6903629899024963 \t\t Validation: Acc=51%, Loss=0.6863121390342712\n",
            "Iteration: 354660 \t Train: Acc=58%, Loss=0.6822295188903809 \t\t Validation: Acc=47%, Loss=0.6954705119132996\n",
            "Iteration: 354670 \t Train: Acc=57%, Loss=0.6776536703109741 \t\t Validation: Acc=48%, Loss=0.6985474228858948\n",
            "Iteration: 354680 \t Train: Acc=49%, Loss=0.6949341893196106 \t\t Validation: Acc=49%, Loss=0.7055224776268005\n",
            "Iteration: 354690 \t Train: Acc=53%, Loss=0.6912630200386047 \t\t Validation: Acc=53%, Loss=0.683583676815033\n",
            "Iteration: 354700 \t Train: Acc=52%, Loss=0.6898581981658936 \t\t Validation: Acc=50%, Loss=0.6847476959228516\n",
            "Iteration: 354710 \t Train: Acc=57%, Loss=0.6843789219856262 \t\t Validation: Acc=50%, Loss=0.6912407875061035\n",
            "Iteration: 354720 \t Train: Acc=50%, Loss=0.6866039633750916 \t\t Validation: Acc=50%, Loss=0.6928243041038513\n",
            "Iteration: 354730 \t Train: Acc=53%, Loss=0.6873330473899841 \t\t Validation: Acc=49%, Loss=0.7016462087631226\n",
            "Iteration: 354740 \t Train: Acc=49%, Loss=0.6885479092597961 \t\t Validation: Acc=49%, Loss=0.6994166970252991\n",
            "Iteration: 354750 \t Train: Acc=48%, Loss=0.6888219118118286 \t\t Validation: Acc=49%, Loss=0.700295627117157\n",
            "Iteration: 354760 \t Train: Acc=53%, Loss=0.681086003780365 \t\t Validation: Acc=51%, Loss=0.6953473687171936\n",
            "Iteration: 354770 \t Train: Acc=50%, Loss=0.6970070600509644 \t\t Validation: Acc=49%, Loss=0.7013186812400818\n",
            "Iteration: 354780 \t Train: Acc=47%, Loss=0.6935299038887024 \t\t Validation: Acc=49%, Loss=0.6897022724151611\n",
            "Iteration: 354790 \t Train: Acc=53%, Loss=0.6850745677947998 \t\t Validation: Acc=49%, Loss=0.6953353881835938\n",
            "Iteration: 354800 \t Train: Acc=48%, Loss=0.7064030766487122 \t\t Validation: Acc=48%, Loss=0.6922740936279297\n",
            "Iteration: 354810 \t Train: Acc=48%, Loss=0.6902848482131958 \t\t Validation: Acc=49%, Loss=0.6980476379394531\n",
            "Iteration: 354820 \t Train: Acc=49%, Loss=0.6938756108283997 \t\t Validation: Acc=52%, Loss=0.6919161677360535\n",
            "Iteration: 354830 \t Train: Acc=51%, Loss=0.6894347667694092 \t\t Validation: Acc=51%, Loss=0.6897438764572144\n",
            "Iteration: 354840 \t Train: Acc=44%, Loss=0.6934753656387329 \t\t Validation: Acc=50%, Loss=0.6983814239501953\n",
            "Iteration: 354850 \t Train: Acc=51%, Loss=0.688739538192749 \t\t Validation: Acc=51%, Loss=0.6970163583755493\n",
            "Iteration: 354860 \t Train: Acc=56%, Loss=0.6907990574836731 \t\t Validation: Acc=52%, Loss=0.6896296143531799\n",
            "Iteration: 354870 \t Train: Acc=50%, Loss=0.6980751156806946 \t\t Validation: Acc=47%, Loss=0.6978186368942261\n",
            "Iteration: 354880 \t Train: Acc=50%, Loss=0.6796073317527771 \t\t Validation: Acc=50%, Loss=0.7006461024284363\n",
            "Iteration: 354890 \t Train: Acc=52%, Loss=0.6799797415733337 \t\t Validation: Acc=53%, Loss=0.6816397905349731\n",
            "Iteration: 354900 \t Train: Acc=49%, Loss=0.6942745447158813 \t\t Validation: Acc=46%, Loss=0.7019901871681213\n",
            "Iteration: 354910 \t Train: Acc=49%, Loss=0.6899234652519226 \t\t Validation: Acc=50%, Loss=0.6917807459831238\n",
            "Iteration: 354920 \t Train: Acc=50%, Loss=0.692643404006958 \t\t Validation: Acc=50%, Loss=0.6983943581581116\n",
            "Iteration: 354930 \t Train: Acc=53%, Loss=0.6884567737579346 \t\t Validation: Acc=50%, Loss=0.6934231519699097\n",
            "Iteration: 354940 \t Train: Acc=53%, Loss=0.686229944229126 \t\t Validation: Acc=50%, Loss=0.6857578754425049\n",
            "Iteration: 354950 \t Train: Acc=50%, Loss=0.6938887238502502 \t\t Validation: Acc=50%, Loss=0.6986091732978821\n",
            "Iteration: 354960 \t Train: Acc=50%, Loss=0.6909873485565186 \t\t Validation: Acc=45%, Loss=0.6987088322639465\n",
            "Iteration: 354970 \t Train: Acc=47%, Loss=0.6940672993659973 \t\t Validation: Acc=52%, Loss=0.6890410780906677\n",
            "Iteration: 354980 \t Train: Acc=51%, Loss=0.6923742294311523 \t\t Validation: Acc=48%, Loss=0.7026532292366028\n",
            "Iteration: 354990 \t Train: Acc=51%, Loss=0.6974694728851318 \t\t Validation: Acc=50%, Loss=0.6911918520927429\n",
            "Iteration: 355000 \t Train: Acc=56%, Loss=0.6889303922653198 \t\t Validation: Acc=50%, Loss=0.675971508026123\n",
            "Iteration: 355010 \t Train: Acc=56%, Loss=0.692245364189148 \t\t Validation: Acc=50%, Loss=0.6902633309364319\n",
            "Iteration: 355020 \t Train: Acc=54%, Loss=0.6856562495231628 \t\t Validation: Acc=50%, Loss=0.7010337710380554\n",
            "Iteration: 355030 \t Train: Acc=47%, Loss=0.6858974695205688 \t\t Validation: Acc=48%, Loss=0.7055864334106445\n",
            "Iteration: 355040 \t Train: Acc=50%, Loss=0.6901628971099854 \t\t Validation: Acc=47%, Loss=0.7063730359077454\n",
            "Iteration: 355050 \t Train: Acc=49%, Loss=0.6910426616668701 \t\t Validation: Acc=50%, Loss=0.6843032240867615\n",
            "Iteration: 355060 \t Train: Acc=49%, Loss=0.6948854923248291 \t\t Validation: Acc=51%, Loss=0.6961697340011597\n",
            "Iteration: 355070 \t Train: Acc=50%, Loss=0.6912289261817932 \t\t Validation: Acc=50%, Loss=0.6923628449440002\n",
            "Iteration: 355080 \t Train: Acc=50%, Loss=0.6879819631576538 \t\t Validation: Acc=50%, Loss=0.6974896192550659\n",
            "Iteration: 355090 \t Train: Acc=53%, Loss=0.6915589570999146 \t\t Validation: Acc=50%, Loss=0.6917387843132019\n",
            "Iteration: 355100 \t Train: Acc=54%, Loss=0.6880286335945129 \t\t Validation: Acc=50%, Loss=0.6948528289794922\n",
            "Iteration: 355110 \t Train: Acc=53%, Loss=0.6850564479827881 \t\t Validation: Acc=52%, Loss=0.6936494708061218\n",
            "Iteration: 355120 \t Train: Acc=51%, Loss=0.6954084038734436 \t\t Validation: Acc=50%, Loss=0.6926525235176086\n",
            "Iteration: 355130 \t Train: Acc=50%, Loss=0.6940364837646484 \t\t Validation: Acc=50%, Loss=0.6905499696731567\n",
            "Iteration: 355140 \t Train: Acc=52%, Loss=0.6948550343513489 \t\t Validation: Acc=49%, Loss=0.6891724467277527\n",
            "Iteration: 355150 \t Train: Acc=55%, Loss=0.6864087581634521 \t\t Validation: Acc=46%, Loss=0.6827625036239624\n",
            "Iteration: 355160 \t Train: Acc=52%, Loss=0.6895597577095032 \t\t Validation: Acc=50%, Loss=0.6944949626922607\n",
            "Iteration: 355170 \t Train: Acc=53%, Loss=0.6858803033828735 \t\t Validation: Acc=52%, Loss=0.7062898278236389\n",
            "Iteration: 355180 \t Train: Acc=50%, Loss=0.6915290355682373 \t\t Validation: Acc=50%, Loss=0.6902577877044678\n",
            "Iteration: 355190 \t Train: Acc=51%, Loss=0.6916995048522949 \t\t Validation: Acc=50%, Loss=0.6875142455101013\n",
            "Iteration: 355200 \t Train: Acc=52%, Loss=0.69044029712677 \t\t Validation: Acc=50%, Loss=0.689216136932373\n",
            "Iteration: 355210 \t Train: Acc=57%, Loss=0.6887894868850708 \t\t Validation: Acc=48%, Loss=0.6937906742095947\n",
            "Iteration: 355220 \t Train: Acc=51%, Loss=0.6945755481719971 \t\t Validation: Acc=50%, Loss=0.6911816000938416\n",
            "Iteration: 355230 \t Train: Acc=47%, Loss=0.6953282356262207 \t\t Validation: Acc=49%, Loss=0.691633403301239\n",
            "Iteration: 355240 \t Train: Acc=49%, Loss=0.6965053081512451 \t\t Validation: Acc=50%, Loss=0.690970778465271\n",
            "Iteration: 355250 \t Train: Acc=44%, Loss=0.6972064971923828 \t\t Validation: Acc=49%, Loss=0.6903542876243591\n",
            "Iteration: 355260 \t Train: Acc=55%, Loss=0.6850472688674927 \t\t Validation: Acc=50%, Loss=0.6972437500953674\n",
            "Iteration: 355270 \t Train: Acc=45%, Loss=0.6928291320800781 \t\t Validation: Acc=47%, Loss=0.6984613537788391\n",
            "Iteration: 355280 \t Train: Acc=51%, Loss=0.6931688785552979 \t\t Validation: Acc=50%, Loss=0.6986281871795654\n",
            "Iteration: 355290 \t Train: Acc=52%, Loss=0.6928088068962097 \t\t Validation: Acc=51%, Loss=0.6908621788024902\n",
            "Iteration: 355300 \t Train: Acc=53%, Loss=0.6875269412994385 \t\t Validation: Acc=50%, Loss=0.6935088634490967\n",
            "Iteration: 355310 \t Train: Acc=55%, Loss=0.685782253742218 \t\t Validation: Acc=50%, Loss=0.694259524345398\n",
            "Iteration: 355320 \t Train: Acc=50%, Loss=0.687861979007721 \t\t Validation: Acc=48%, Loss=0.6916674375534058\n",
            "Iteration: 355330 \t Train: Acc=56%, Loss=0.6911094188690186 \t\t Validation: Acc=49%, Loss=0.7079560160636902\n",
            "Iteration: 355340 \t Train: Acc=50%, Loss=0.6935505867004395 \t\t Validation: Acc=50%, Loss=0.6984492540359497\n",
            "Iteration: 355350 \t Train: Acc=48%, Loss=0.6944537162780762 \t\t Validation: Acc=49%, Loss=0.6901026964187622\n",
            "Iteration: 355360 \t Train: Acc=52%, Loss=0.6857349872589111 \t\t Validation: Acc=46%, Loss=0.6976799964904785\n",
            "Iteration: 355370 \t Train: Acc=47%, Loss=0.6886214017868042 \t\t Validation: Acc=53%, Loss=0.6860882639884949\n",
            "Iteration: 355380 \t Train: Acc=44%, Loss=0.6968486309051514 \t\t Validation: Acc=49%, Loss=0.698292076587677\n",
            "Iteration: 355390 \t Train: Acc=49%, Loss=0.6945428848266602 \t\t Validation: Acc=49%, Loss=0.6938472986221313\n",
            "Iteration: 355400 \t Train: Acc=51%, Loss=0.6922191381454468 \t\t Validation: Acc=50%, Loss=0.6926122307777405\n",
            "Iteration: 355410 \t Train: Acc=53%, Loss=0.691261887550354 \t\t Validation: Acc=50%, Loss=0.6923491954803467\n",
            "Iteration: 355420 \t Train: Acc=56%, Loss=0.6883131861686707 \t\t Validation: Acc=51%, Loss=0.6929501891136169\n",
            "Iteration: 355430 \t Train: Acc=50%, Loss=0.685531497001648 \t\t Validation: Acc=51%, Loss=0.6948443651199341\n",
            "Iteration: 355440 \t Train: Acc=46%, Loss=0.6978895664215088 \t\t Validation: Acc=51%, Loss=0.6918824911117554\n",
            "Iteration: 355450 \t Train: Acc=50%, Loss=0.6954382658004761 \t\t Validation: Acc=48%, Loss=0.6972609758377075\n",
            "Iteration: 355460 \t Train: Acc=46%, Loss=0.6905620098114014 \t\t Validation: Acc=50%, Loss=0.6863686442375183\n",
            "Iteration: 355470 \t Train: Acc=47%, Loss=0.6891602277755737 \t\t Validation: Acc=53%, Loss=0.6885682344436646\n",
            "Iteration: 355480 \t Train: Acc=49%, Loss=0.694094181060791 \t\t Validation: Acc=50%, Loss=0.7043960690498352\n",
            "Iteration: 355490 \t Train: Acc=50%, Loss=0.6937510967254639 \t\t Validation: Acc=50%, Loss=0.693873405456543\n",
            "Iteration: 355500 \t Train: Acc=50%, Loss=0.6896533370018005 \t\t Validation: Acc=50%, Loss=0.6985872387886047\n",
            "Iteration: 355510 \t Train: Acc=44%, Loss=0.6894106864929199 \t\t Validation: Acc=50%, Loss=0.6984376907348633\n",
            "Iteration: 355520 \t Train: Acc=50%, Loss=0.6931512951850891 \t\t Validation: Acc=50%, Loss=0.6958175897598267\n",
            "Iteration: 355530 \t Train: Acc=57%, Loss=0.6872984170913696 \t\t Validation: Acc=50%, Loss=0.6826146841049194\n",
            "Iteration: 355540 \t Train: Acc=46%, Loss=0.7013515830039978 \t\t Validation: Acc=50%, Loss=0.7013657093048096\n",
            "Iteration: 355550 \t Train: Acc=51%, Loss=0.692824125289917 \t\t Validation: Acc=50%, Loss=0.6966791749000549\n",
            "Iteration: 355560 \t Train: Acc=56%, Loss=0.6883590817451477 \t\t Validation: Acc=50%, Loss=0.6878350973129272\n",
            "Iteration: 355570 \t Train: Acc=50%, Loss=0.6911250352859497 \t\t Validation: Acc=50%, Loss=0.688598096370697\n",
            "Iteration: 355580 \t Train: Acc=53%, Loss=0.6867908239364624 \t\t Validation: Acc=48%, Loss=0.706969141960144\n",
            "Iteration: 355590 \t Train: Acc=50%, Loss=0.6913922429084778 \t\t Validation: Acc=50%, Loss=0.6930047273635864\n",
            "Iteration: 355600 \t Train: Acc=56%, Loss=0.6954638957977295 \t\t Validation: Acc=51%, Loss=0.704969584941864\n",
            "Iteration: 355610 \t Train: Acc=59%, Loss=0.6941589117050171 \t\t Validation: Acc=49%, Loss=0.6962305903434753\n",
            "Iteration: 355620 \t Train: Acc=44%, Loss=0.6950007081031799 \t\t Validation: Acc=50%, Loss=0.6983562111854553\n",
            "Iteration: 355630 \t Train: Acc=53%, Loss=0.687935471534729 \t\t Validation: Acc=52%, Loss=0.6933647394180298\n",
            "Iteration: 355640 \t Train: Acc=53%, Loss=0.6947835683822632 \t\t Validation: Acc=48%, Loss=0.69505774974823\n",
            "Iteration: 355650 \t Train: Acc=54%, Loss=0.6861565113067627 \t\t Validation: Acc=48%, Loss=0.6975874900817871\n",
            "Iteration: 355660 \t Train: Acc=46%, Loss=0.6980816125869751 \t\t Validation: Acc=49%, Loss=0.6965576410293579\n",
            "Iteration: 355670 \t Train: Acc=46%, Loss=0.6913384199142456 \t\t Validation: Acc=49%, Loss=0.6928703188896179\n",
            "Iteration: 355680 \t Train: Acc=53%, Loss=0.6951457858085632 \t\t Validation: Acc=50%, Loss=0.6981504559516907\n",
            "Iteration: 355690 \t Train: Acc=53%, Loss=0.6854168176651001 \t\t Validation: Acc=50%, Loss=0.7010082006454468\n",
            "Iteration: 355700 \t Train: Acc=48%, Loss=0.6954956650733948 \t\t Validation: Acc=52%, Loss=0.6871837377548218\n",
            "Iteration: 355710 \t Train: Acc=55%, Loss=0.6900798082351685 \t\t Validation: Acc=48%, Loss=0.6976310610771179\n",
            "Iteration: 355720 \t Train: Acc=51%, Loss=0.6908427476882935 \t\t Validation: Acc=50%, Loss=0.6932361125946045\n",
            "Iteration: 355730 \t Train: Acc=51%, Loss=0.6921820044517517 \t\t Validation: Acc=47%, Loss=0.6995812654495239\n",
            "Iteration: 355740 \t Train: Acc=51%, Loss=0.6901613473892212 \t\t Validation: Acc=50%, Loss=0.6839355826377869\n",
            "Iteration: 355750 \t Train: Acc=52%, Loss=0.6935045719146729 \t\t Validation: Acc=50%, Loss=0.6864656209945679\n",
            "Iteration: 355760 \t Train: Acc=50%, Loss=0.6904346346855164 \t\t Validation: Acc=50%, Loss=0.7028269171714783\n",
            "Iteration: 355770 \t Train: Acc=47%, Loss=0.7013412117958069 \t\t Validation: Acc=50%, Loss=0.6984837651252747\n",
            "Iteration: 355780 \t Train: Acc=50%, Loss=0.6962597370147705 \t\t Validation: Acc=49%, Loss=0.6897954344749451\n",
            "Iteration: 355790 \t Train: Acc=50%, Loss=0.696685254573822 \t\t Validation: Acc=53%, Loss=0.6932701468467712\n",
            "Iteration: 355800 \t Train: Acc=46%, Loss=0.6932193040847778 \t\t Validation: Acc=50%, Loss=0.6952602863311768\n",
            "Iteration: 355810 \t Train: Acc=52%, Loss=0.6892240047454834 \t\t Validation: Acc=50%, Loss=0.6963842511177063\n",
            "Iteration: 355820 \t Train: Acc=52%, Loss=0.689424455165863 \t\t Validation: Acc=52%, Loss=0.6932870149612427\n",
            "Iteration: 355830 \t Train: Acc=46%, Loss=0.7068065404891968 \t\t Validation: Acc=50%, Loss=0.6924052238464355\n",
            "Iteration: 355840 \t Train: Acc=50%, Loss=0.6934258341789246 \t\t Validation: Acc=51%, Loss=0.6940833330154419\n",
            "Iteration: 355850 \t Train: Acc=55%, Loss=0.6856642365455627 \t\t Validation: Acc=51%, Loss=0.6918703317642212\n",
            "Iteration: 355860 \t Train: Acc=53%, Loss=0.6928203105926514 \t\t Validation: Acc=50%, Loss=0.6880193948745728\n",
            "Iteration: 355870 \t Train: Acc=57%, Loss=0.6913613080978394 \t\t Validation: Acc=49%, Loss=0.6937978863716125\n",
            "Iteration: 355880 \t Train: Acc=50%, Loss=0.7067791223526001 \t\t Validation: Acc=48%, Loss=0.6985331773757935\n",
            "Iteration: 355890 \t Train: Acc=50%, Loss=0.6914151310920715 \t\t Validation: Acc=50%, Loss=0.687291145324707\n",
            "Iteration: 355900 \t Train: Acc=48%, Loss=0.6920777559280396 \t\t Validation: Acc=50%, Loss=0.694386899471283\n",
            "Iteration: 355910 \t Train: Acc=50%, Loss=0.6871302723884583 \t\t Validation: Acc=46%, Loss=0.6901428699493408\n",
            "Iteration: 355920 \t Train: Acc=49%, Loss=0.6905263662338257 \t\t Validation: Acc=50%, Loss=0.6954586505889893\n",
            "Iteration: 355930 \t Train: Acc=50%, Loss=0.6924477219581604 \t\t Validation: Acc=50%, Loss=0.7012686133384705\n",
            "Iteration: 355940 \t Train: Acc=53%, Loss=0.6906033158302307 \t\t Validation: Acc=51%, Loss=0.6944257020950317\n",
            "Iteration: 355950 \t Train: Acc=49%, Loss=0.690787136554718 \t\t Validation: Acc=50%, Loss=0.6904679536819458\n",
            "Iteration: 355960 \t Train: Acc=47%, Loss=0.6877425312995911 \t\t Validation: Acc=49%, Loss=0.7052524089813232\n",
            "Iteration: 355970 \t Train: Acc=52%, Loss=0.6923038363456726 \t\t Validation: Acc=53%, Loss=0.6992127299308777\n",
            "Iteration: 355980 \t Train: Acc=47%, Loss=0.6972819566726685 \t\t Validation: Acc=49%, Loss=0.6969215869903564\n",
            "Iteration: 355990 \t Train: Acc=57%, Loss=0.6862970590591431 \t\t Validation: Acc=50%, Loss=0.6986696124076843\n",
            "Iteration: 356000 \t Train: Acc=53%, Loss=0.6842749714851379 \t\t Validation: Acc=47%, Loss=0.6973752379417419\n",
            "Iteration: 356010 \t Train: Acc=53%, Loss=0.686837911605835 \t\t Validation: Acc=47%, Loss=0.6960173845291138\n",
            "Iteration: 356020 \t Train: Acc=53%, Loss=0.6899194717407227 \t\t Validation: Acc=51%, Loss=0.6933030486106873\n",
            "Iteration: 356030 \t Train: Acc=48%, Loss=0.6942712664604187 \t\t Validation: Acc=53%, Loss=0.6865465044975281\n",
            "Iteration: 356040 \t Train: Acc=49%, Loss=0.697706401348114 \t\t Validation: Acc=53%, Loss=0.6851716041564941\n",
            "Iteration: 356050 \t Train: Acc=48%, Loss=0.6932240724563599 \t\t Validation: Acc=51%, Loss=0.6874063014984131\n",
            "Iteration: 356060 \t Train: Acc=50%, Loss=0.6942347288131714 \t\t Validation: Acc=49%, Loss=0.7009607553482056\n",
            "Iteration: 356070 \t Train: Acc=54%, Loss=0.6872208714485168 \t\t Validation: Acc=49%, Loss=0.6944625377655029\n",
            "Iteration: 356080 \t Train: Acc=48%, Loss=0.6892274022102356 \t\t Validation: Acc=51%, Loss=0.6951311826705933\n",
            "Iteration: 356090 \t Train: Acc=56%, Loss=0.683724045753479 \t\t Validation: Acc=51%, Loss=0.6919318437576294\n",
            "Iteration: 356100 \t Train: Acc=53%, Loss=0.6966479420661926 \t\t Validation: Acc=48%, Loss=0.6924520134925842\n",
            "Iteration: 356110 \t Train: Acc=50%, Loss=0.6927065849304199 \t\t Validation: Acc=51%, Loss=0.6947426199913025\n",
            "Iteration: 356120 \t Train: Acc=49%, Loss=0.6955966353416443 \t\t Validation: Acc=49%, Loss=0.6945371627807617\n",
            "Iteration: 356130 \t Train: Acc=50%, Loss=0.6909404993057251 \t\t Validation: Acc=50%, Loss=0.6928045749664307\n",
            "Iteration: 356140 \t Train: Acc=46%, Loss=0.6986530423164368 \t\t Validation: Acc=52%, Loss=0.6927195191383362\n",
            "Iteration: 356150 \t Train: Acc=53%, Loss=0.6925946474075317 \t\t Validation: Acc=49%, Loss=0.6940075159072876\n",
            "Iteration: 356160 \t Train: Acc=49%, Loss=0.6955520510673523 \t\t Validation: Acc=49%, Loss=0.6911329627037048\n",
            "Iteration: 356170 \t Train: Acc=48%, Loss=0.6914278864860535 \t\t Validation: Acc=50%, Loss=0.6963611841201782\n",
            "Iteration: 356180 \t Train: Acc=50%, Loss=0.6907162666320801 \t\t Validation: Acc=50%, Loss=0.6924912929534912\n",
            "Iteration: 356190 \t Train: Acc=49%, Loss=0.6996825337409973 \t\t Validation: Acc=49%, Loss=0.6926591396331787\n",
            "Iteration: 356200 \t Train: Acc=54%, Loss=0.695690929889679 \t\t Validation: Acc=50%, Loss=0.6964336633682251\n",
            "Iteration: 356210 \t Train: Acc=53%, Loss=0.6889801621437073 \t\t Validation: Acc=48%, Loss=0.6936614513397217\n",
            "Iteration: 356220 \t Train: Acc=48%, Loss=0.6900486946105957 \t\t Validation: Acc=49%, Loss=0.6876281499862671\n",
            "Iteration: 356230 \t Train: Acc=50%, Loss=0.6937304735183716 \t\t Validation: Acc=50%, Loss=0.6952639222145081\n",
            "Iteration: 356240 \t Train: Acc=54%, Loss=0.6926496028900146 \t\t Validation: Acc=50%, Loss=0.6898279786109924\n",
            "Iteration: 356250 \t Train: Acc=43%, Loss=0.6951173543930054 \t\t Validation: Acc=53%, Loss=0.6905984878540039\n",
            "Iteration: 356260 \t Train: Acc=47%, Loss=0.6965285539627075 \t\t Validation: Acc=50%, Loss=0.6891674995422363\n",
            "Iteration: 356270 \t Train: Acc=56%, Loss=0.6899022459983826 \t\t Validation: Acc=50%, Loss=0.692623496055603\n",
            "Iteration: 356280 \t Train: Acc=51%, Loss=0.6908423900604248 \t\t Validation: Acc=51%, Loss=0.6994075179100037\n",
            "Iteration: 356290 \t Train: Acc=55%, Loss=0.6880182027816772 \t\t Validation: Acc=50%, Loss=0.6964127421379089\n",
            "Iteration: 356300 \t Train: Acc=47%, Loss=0.6950016617774963 \t\t Validation: Acc=51%, Loss=0.6961463093757629\n",
            "Iteration: 356310 \t Train: Acc=57%, Loss=0.6876399517059326 \t\t Validation: Acc=52%, Loss=0.6920334100723267\n",
            "Iteration: 356320 \t Train: Acc=51%, Loss=0.6920100450515747 \t\t Validation: Acc=49%, Loss=0.6975655555725098\n",
            "Iteration: 356330 \t Train: Acc=53%, Loss=0.6823912262916565 \t\t Validation: Acc=50%, Loss=0.6887438893318176\n",
            "Iteration: 356340 \t Train: Acc=59%, Loss=0.6842235326766968 \t\t Validation: Acc=50%, Loss=0.6892496347427368\n",
            "Iteration: 356350 \t Train: Acc=50%, Loss=0.6955305933952332 \t\t Validation: Acc=48%, Loss=0.6938297748565674\n",
            "Iteration: 356360 \t Train: Acc=54%, Loss=0.6892691850662231 \t\t Validation: Acc=47%, Loss=0.6906781792640686\n",
            "Iteration: 356370 \t Train: Acc=56%, Loss=0.6897600293159485 \t\t Validation: Acc=50%, Loss=0.6951116323471069\n",
            "Iteration: 356380 \t Train: Acc=51%, Loss=0.691977322101593 \t\t Validation: Acc=53%, Loss=0.6886094808578491\n",
            "Iteration: 356390 \t Train: Acc=50%, Loss=0.6894366145133972 \t\t Validation: Acc=50%, Loss=0.6885852217674255\n",
            "Iteration: 356400 \t Train: Acc=57%, Loss=0.6926217675209045 \t\t Validation: Acc=51%, Loss=0.6915457844734192\n",
            "Iteration: 356410 \t Train: Acc=48%, Loss=0.6922681331634521 \t\t Validation: Acc=50%, Loss=0.6980643272399902\n",
            "Iteration: 356420 \t Train: Acc=49%, Loss=0.6883162260055542 \t\t Validation: Acc=48%, Loss=0.6898306608200073\n",
            "Iteration: 356430 \t Train: Acc=48%, Loss=0.6931778192520142 \t\t Validation: Acc=51%, Loss=0.696958065032959\n",
            "Iteration: 356440 \t Train: Acc=46%, Loss=0.7014769911766052 \t\t Validation: Acc=49%, Loss=0.6913164854049683\n",
            "Iteration: 356450 \t Train: Acc=53%, Loss=0.690571665763855 \t\t Validation: Acc=48%, Loss=0.6969523429870605\n",
            "Iteration: 356460 \t Train: Acc=46%, Loss=0.6976205110549927 \t\t Validation: Acc=46%, Loss=0.6948772072792053\n",
            "Iteration: 356470 \t Train: Acc=50%, Loss=0.6848688721656799 \t\t Validation: Acc=50%, Loss=0.6930425763130188\n",
            "Iteration: 356480 \t Train: Acc=47%, Loss=0.6953389644622803 \t\t Validation: Acc=51%, Loss=0.6902585029602051\n",
            "Iteration: 356490 \t Train: Acc=53%, Loss=0.6936659812927246 \t\t Validation: Acc=50%, Loss=0.6930963397026062\n",
            "Iteration: 356500 \t Train: Acc=54%, Loss=0.6903584003448486 \t\t Validation: Acc=48%, Loss=0.6917882561683655\n",
            "Iteration: 356510 \t Train: Acc=54%, Loss=0.6840692758560181 \t\t Validation: Acc=49%, Loss=0.6883058547973633\n",
            "Iteration: 356520 \t Train: Acc=51%, Loss=0.6904250383377075 \t\t Validation: Acc=52%, Loss=0.6921917200088501\n",
            "Iteration: 356530 \t Train: Acc=50%, Loss=0.6874358654022217 \t\t Validation: Acc=49%, Loss=0.6942152380943298\n",
            "Iteration: 356540 \t Train: Acc=50%, Loss=0.6881918907165527 \t\t Validation: Acc=50%, Loss=0.6905688047409058\n",
            "Iteration: 356550 \t Train: Acc=46%, Loss=0.6938638091087341 \t\t Validation: Acc=50%, Loss=0.687427282333374\n",
            "Iteration: 356560 \t Train: Acc=49%, Loss=0.6964722871780396 \t\t Validation: Acc=49%, Loss=0.6896806359291077\n",
            "Iteration: 356570 \t Train: Acc=60%, Loss=0.6786398887634277 \t\t Validation: Acc=48%, Loss=0.6916412115097046\n",
            "Iteration: 356580 \t Train: Acc=51%, Loss=0.6866083741188049 \t\t Validation: Acc=50%, Loss=0.7000436186790466\n",
            "Iteration: 356590 \t Train: Acc=51%, Loss=0.6977651119232178 \t\t Validation: Acc=53%, Loss=0.6965328454971313\n",
            "Iteration: 356600 \t Train: Acc=47%, Loss=0.691640317440033 \t\t Validation: Acc=51%, Loss=0.6925797462463379\n",
            "Iteration: 356610 \t Train: Acc=48%, Loss=0.6931456327438354 \t\t Validation: Acc=52%, Loss=0.6887458562850952\n",
            "Iteration: 356620 \t Train: Acc=45%, Loss=0.6916422843933105 \t\t Validation: Acc=51%, Loss=0.6925936341285706\n",
            "Iteration: 356630 \t Train: Acc=51%, Loss=0.693422794342041 \t\t Validation: Acc=51%, Loss=0.6987205743789673\n",
            "Iteration: 356640 \t Train: Acc=53%, Loss=0.6886303424835205 \t\t Validation: Acc=50%, Loss=0.6963791847229004\n",
            "Iteration: 356650 \t Train: Acc=57%, Loss=0.6848503947257996 \t\t Validation: Acc=49%, Loss=0.6950597763061523\n",
            "Iteration: 356660 \t Train: Acc=51%, Loss=0.6892662644386292 \t\t Validation: Acc=50%, Loss=0.6967366933822632\n",
            "Iteration: 356670 \t Train: Acc=51%, Loss=0.6896485686302185 \t\t Validation: Acc=52%, Loss=0.6921390295028687\n",
            "Iteration: 356680 \t Train: Acc=56%, Loss=0.6844651699066162 \t\t Validation: Acc=50%, Loss=0.694869339466095\n",
            "Iteration: 356690 \t Train: Acc=52%, Loss=0.686751127243042 \t\t Validation: Acc=48%, Loss=0.69200599193573\n",
            "Iteration: 356700 \t Train: Acc=46%, Loss=0.6962880492210388 \t\t Validation: Acc=50%, Loss=0.6939283609390259\n",
            "Iteration: 356710 \t Train: Acc=53%, Loss=0.6849743127822876 \t\t Validation: Acc=50%, Loss=0.6932559609413147\n",
            "Iteration: 356720 \t Train: Acc=57%, Loss=0.6810769438743591 \t\t Validation: Acc=45%, Loss=0.6932730078697205\n",
            "Iteration: 356730 \t Train: Acc=53%, Loss=0.6897428035736084 \t\t Validation: Acc=55%, Loss=0.6946594715118408\n",
            "Iteration: 356740 \t Train: Acc=50%, Loss=0.6929316520690918 \t\t Validation: Acc=49%, Loss=0.6934062838554382\n",
            "Iteration: 356750 \t Train: Acc=53%, Loss=0.6798465847969055 \t\t Validation: Acc=55%, Loss=0.6883289813995361\n",
            "Iteration: 356760 \t Train: Acc=55%, Loss=0.6869503259658813 \t\t Validation: Acc=52%, Loss=0.692824125289917\n",
            "Iteration: 356770 \t Train: Acc=49%, Loss=0.6903076171875 \t\t Validation: Acc=53%, Loss=0.6870872378349304\n",
            "Iteration: 356780 \t Train: Acc=50%, Loss=0.6931601762771606 \t\t Validation: Acc=46%, Loss=0.6977360844612122\n",
            "Iteration: 356790 \t Train: Acc=50%, Loss=0.6938807368278503 \t\t Validation: Acc=51%, Loss=0.6901284456253052\n",
            "Iteration: 356800 \t Train: Acc=57%, Loss=0.6867924928665161 \t\t Validation: Acc=47%, Loss=0.6951262950897217\n",
            "Iteration: 356810 \t Train: Acc=44%, Loss=0.7043861746788025 \t\t Validation: Acc=50%, Loss=0.6919010877609253\n",
            "Iteration: 356820 \t Train: Acc=49%, Loss=0.6893497109413147 \t\t Validation: Acc=49%, Loss=0.6893556714057922\n",
            "Iteration: 356830 \t Train: Acc=57%, Loss=0.6832080483436584 \t\t Validation: Acc=52%, Loss=0.692040205001831\n",
            "Iteration: 356840 \t Train: Acc=47%, Loss=0.6931486129760742 \t\t Validation: Acc=54%, Loss=0.6900651454925537\n",
            "Iteration: 356850 \t Train: Acc=52%, Loss=0.6960306167602539 \t\t Validation: Acc=48%, Loss=0.6918421983718872\n",
            "Iteration: 356860 \t Train: Acc=53%, Loss=0.6907921433448792 \t\t Validation: Acc=46%, Loss=0.6936742663383484\n",
            "Iteration: 356870 \t Train: Acc=51%, Loss=0.6946018934249878 \t\t Validation: Acc=53%, Loss=0.6950020790100098\n",
            "Iteration: 356880 \t Train: Acc=53%, Loss=0.6904781460762024 \t\t Validation: Acc=50%, Loss=0.6928577423095703\n",
            "Iteration: 356890 \t Train: Acc=46%, Loss=0.690880537033081 \t\t Validation: Acc=51%, Loss=0.691507875919342\n",
            "Iteration: 356900 \t Train: Acc=56%, Loss=0.6919030547142029 \t\t Validation: Acc=49%, Loss=0.6895395517349243\n",
            "Iteration: 356910 \t Train: Acc=51%, Loss=0.694292426109314 \t\t Validation: Acc=46%, Loss=0.6961987018585205\n",
            "Iteration: 356920 \t Train: Acc=47%, Loss=0.6961585879325867 \t\t Validation: Acc=53%, Loss=0.6920596957206726\n",
            "Iteration: 356930 \t Train: Acc=39%, Loss=0.7050421833992004 \t\t Validation: Acc=53%, Loss=0.6900027990341187\n",
            "Iteration: 356940 \t Train: Acc=48%, Loss=0.6902628540992737 \t\t Validation: Acc=53%, Loss=0.6958171129226685\n",
            "Iteration: 356950 \t Train: Acc=53%, Loss=0.6892628073692322 \t\t Validation: Acc=51%, Loss=0.6988036632537842\n",
            "Iteration: 356960 \t Train: Acc=54%, Loss=0.6860144138336182 \t\t Validation: Acc=51%, Loss=0.6902581453323364\n",
            "Iteration: 356970 \t Train: Acc=52%, Loss=0.693770706653595 \t\t Validation: Acc=53%, Loss=0.6917396783828735\n",
            "Iteration: 356980 \t Train: Acc=53%, Loss=0.6843239665031433 \t\t Validation: Acc=53%, Loss=0.6969791650772095\n",
            "Iteration: 356990 \t Train: Acc=50%, Loss=0.6899540424346924 \t\t Validation: Acc=50%, Loss=0.6901984214782715\n",
            "Iteration: 357000 \t Train: Acc=60%, Loss=0.6820221543312073 \t\t Validation: Acc=50%, Loss=0.6917189359664917\n",
            "Iteration: 357010 \t Train: Acc=51%, Loss=0.6903786063194275 \t\t Validation: Acc=51%, Loss=0.6912189722061157\n",
            "Iteration: 357020 \t Train: Acc=53%, Loss=0.6961168050765991 \t\t Validation: Acc=52%, Loss=0.6857433319091797\n",
            "Iteration: 357030 \t Train: Acc=54%, Loss=0.6833235025405884 \t\t Validation: Acc=46%, Loss=0.6958500146865845\n",
            "Iteration: 357040 \t Train: Acc=46%, Loss=0.6915396451950073 \t\t Validation: Acc=51%, Loss=0.6943279504776001\n",
            "Iteration: 357050 \t Train: Acc=50%, Loss=0.6908371448516846 \t\t Validation: Acc=48%, Loss=0.6969671249389648\n",
            "Iteration: 357060 \t Train: Acc=53%, Loss=0.6830117702484131 \t\t Validation: Acc=51%, Loss=0.6912193894386292\n",
            "Iteration: 357070 \t Train: Acc=53%, Loss=0.690555989742279 \t\t Validation: Acc=50%, Loss=0.6938599944114685\n",
            "Iteration: 357080 \t Train: Acc=56%, Loss=0.6877858638763428 \t\t Validation: Acc=51%, Loss=0.6856691837310791\n",
            "Iteration: 357090 \t Train: Acc=53%, Loss=0.699657678604126 \t\t Validation: Acc=52%, Loss=0.6912740468978882\n",
            "Iteration: 357100 \t Train: Acc=50%, Loss=0.6853111982345581 \t\t Validation: Acc=49%, Loss=0.7003505229949951\n",
            "Iteration: 357110 \t Train: Acc=53%, Loss=0.6928707361221313 \t\t Validation: Acc=50%, Loss=0.6950054168701172\n",
            "Iteration: 357120 \t Train: Acc=54%, Loss=0.688030481338501 \t\t Validation: Acc=49%, Loss=0.6924903392791748\n",
            "Iteration: 357130 \t Train: Acc=49%, Loss=0.6930357813835144 \t\t Validation: Acc=47%, Loss=0.6955054998397827\n",
            "Iteration: 357140 \t Train: Acc=56%, Loss=0.6854448914527893 \t\t Validation: Acc=48%, Loss=0.6962457895278931\n",
            "Iteration: 357150 \t Train: Acc=52%, Loss=0.691403329372406 \t\t Validation: Acc=50%, Loss=0.6936599016189575\n",
            "Iteration: 357160 \t Train: Acc=47%, Loss=0.6971682906150818 \t\t Validation: Acc=46%, Loss=0.6931651830673218\n",
            "Iteration: 357170 \t Train: Acc=46%, Loss=0.6931337714195251 \t\t Validation: Acc=49%, Loss=0.6970030069351196\n",
            "Iteration: 357180 \t Train: Acc=57%, Loss=0.6820752024650574 \t\t Validation: Acc=52%, Loss=0.6934871673583984\n",
            "Iteration: 357190 \t Train: Acc=46%, Loss=0.693987250328064 \t\t Validation: Acc=54%, Loss=0.6901086568832397\n",
            "Iteration: 357200 \t Train: Acc=53%, Loss=0.6842629909515381 \t\t Validation: Acc=53%, Loss=0.6874498724937439\n",
            "Iteration: 357210 \t Train: Acc=50%, Loss=0.7021918892860413 \t\t Validation: Acc=52%, Loss=0.6935116052627563\n",
            "Iteration: 357220 \t Train: Acc=52%, Loss=0.6951601505279541 \t\t Validation: Acc=52%, Loss=0.6864157319068909\n",
            "Iteration: 357230 \t Train: Acc=44%, Loss=0.6982028484344482 \t\t Validation: Acc=53%, Loss=0.6908642649650574\n",
            "Iteration: 357240 \t Train: Acc=50%, Loss=0.6908203363418579 \t\t Validation: Acc=51%, Loss=0.6927535533905029\n",
            "Iteration: 357250 \t Train: Acc=52%, Loss=0.6857423782348633 \t\t Validation: Acc=47%, Loss=0.6942888498306274\n",
            "Iteration: 357260 \t Train: Acc=57%, Loss=0.6835496425628662 \t\t Validation: Acc=48%, Loss=0.6915169954299927\n",
            "Iteration: 357270 \t Train: Acc=53%, Loss=0.6876872777938843 \t\t Validation: Acc=46%, Loss=0.6977459788322449\n",
            "Iteration: 357280 \t Train: Acc=49%, Loss=0.6961154341697693 \t\t Validation: Acc=49%, Loss=0.6934696435928345\n",
            "Iteration: 357290 \t Train: Acc=56%, Loss=0.6880486011505127 \t\t Validation: Acc=53%, Loss=0.6876014471054077\n",
            "Iteration: 357300 \t Train: Acc=55%, Loss=0.6899307370185852 \t\t Validation: Acc=50%, Loss=0.6972346901893616\n",
            "Iteration: 357310 \t Train: Acc=53%, Loss=0.6878916621208191 \t\t Validation: Acc=48%, Loss=0.6946507692337036\n",
            "Iteration: 357320 \t Train: Acc=51%, Loss=0.6925941109657288 \t\t Validation: Acc=50%, Loss=0.6902010440826416\n",
            "Iteration: 357330 \t Train: Acc=58%, Loss=0.6846274733543396 \t\t Validation: Acc=53%, Loss=0.6921182870864868\n",
            "Iteration: 357340 \t Train: Acc=57%, Loss=0.6850042939186096 \t\t Validation: Acc=47%, Loss=0.6986346244812012\n",
            "Iteration: 357350 \t Train: Acc=56%, Loss=0.6924812197685242 \t\t Validation: Acc=52%, Loss=0.6924462914466858\n",
            "Iteration: 357360 \t Train: Acc=50%, Loss=0.6927695870399475 \t\t Validation: Acc=50%, Loss=0.6923153400421143\n",
            "Iteration: 357370 \t Train: Acc=49%, Loss=0.6881080865859985 \t\t Validation: Acc=47%, Loss=0.6927768588066101\n",
            "Iteration: 357380 \t Train: Acc=45%, Loss=0.6934946775436401 \t\t Validation: Acc=46%, Loss=0.7012190818786621\n",
            "Iteration: 357390 \t Train: Acc=52%, Loss=0.7004505395889282 \t\t Validation: Acc=47%, Loss=0.6916296482086182\n",
            "Iteration: 357400 \t Train: Acc=52%, Loss=0.6999903321266174 \t\t Validation: Acc=50%, Loss=0.6877439022064209\n",
            "Iteration: 357410 \t Train: Acc=48%, Loss=0.683266282081604 \t\t Validation: Acc=52%, Loss=0.6925058960914612\n",
            "Iteration: 357420 \t Train: Acc=46%, Loss=0.6988382339477539 \t\t Validation: Acc=53%, Loss=0.6893622279167175\n",
            "Iteration: 357430 \t Train: Acc=47%, Loss=0.6952535510063171 \t\t Validation: Acc=47%, Loss=0.6965358853340149\n",
            "Iteration: 357440 \t Train: Acc=54%, Loss=0.6975273489952087 \t\t Validation: Acc=53%, Loss=0.6904282569885254\n",
            "Iteration: 357450 \t Train: Acc=47%, Loss=0.6921931505203247 \t\t Validation: Acc=51%, Loss=0.6973024606704712\n",
            "Iteration: 357460 \t Train: Acc=54%, Loss=0.6920842528343201 \t\t Validation: Acc=48%, Loss=0.6925514936447144\n",
            "Iteration: 357470 \t Train: Acc=51%, Loss=0.688201904296875 \t\t Validation: Acc=48%, Loss=0.6922972798347473\n",
            "Iteration: 357480 \t Train: Acc=51%, Loss=0.6877018213272095 \t\t Validation: Acc=49%, Loss=0.693958580493927\n",
            "Iteration: 357490 \t Train: Acc=55%, Loss=0.6885048747062683 \t\t Validation: Acc=49%, Loss=0.6944935321807861\n",
            "Iteration: 357500 \t Train: Acc=46%, Loss=0.6994026899337769 \t\t Validation: Acc=50%, Loss=0.694091260433197\n",
            "Iteration: 357510 \t Train: Acc=50%, Loss=0.6933701634407043 \t\t Validation: Acc=50%, Loss=0.6930220723152161\n",
            "Iteration: 357520 \t Train: Acc=52%, Loss=0.6996616721153259 \t\t Validation: Acc=46%, Loss=0.6996374726295471\n",
            "Iteration: 357530 \t Train: Acc=55%, Loss=0.6896231770515442 \t\t Validation: Acc=49%, Loss=0.7002018094062805\n",
            "Iteration: 357540 \t Train: Acc=47%, Loss=0.6945313215255737 \t\t Validation: Acc=53%, Loss=0.6898202896118164\n",
            "Iteration: 357550 \t Train: Acc=50%, Loss=0.6940164566040039 \t\t Validation: Acc=52%, Loss=0.6895620822906494\n",
            "Iteration: 357560 \t Train: Acc=53%, Loss=0.6899909973144531 \t\t Validation: Acc=52%, Loss=0.6916921138763428\n",
            "Iteration: 357570 \t Train: Acc=53%, Loss=0.6903642416000366 \t\t Validation: Acc=50%, Loss=0.6980581879615784\n",
            "Iteration: 357580 \t Train: Acc=46%, Loss=0.7024438381195068 \t\t Validation: Acc=50%, Loss=0.7026848196983337\n",
            "Iteration: 357590 \t Train: Acc=48%, Loss=0.7006459832191467 \t\t Validation: Acc=50%, Loss=0.6926998496055603\n",
            "Iteration: 357600 \t Train: Acc=50%, Loss=0.6892969608306885 \t\t Validation: Acc=51%, Loss=0.6880365610122681\n",
            "Iteration: 357610 \t Train: Acc=50%, Loss=0.6958097815513611 \t\t Validation: Acc=48%, Loss=0.691886842250824\n",
            "Iteration: 357620 \t Train: Acc=49%, Loss=0.6849038600921631 \t\t Validation: Acc=50%, Loss=0.6903550028800964\n",
            "Iteration: 357630 \t Train: Acc=46%, Loss=0.6923104524612427 \t\t Validation: Acc=51%, Loss=0.6884167194366455\n",
            "Iteration: 357640 \t Train: Acc=53%, Loss=0.6868114471435547 \t\t Validation: Acc=49%, Loss=0.6966214179992676\n",
            "Iteration: 357650 \t Train: Acc=45%, Loss=0.699722945690155 \t\t Validation: Acc=51%, Loss=0.6953651309013367\n",
            "Iteration: 357660 \t Train: Acc=54%, Loss=0.6917127370834351 \t\t Validation: Acc=51%, Loss=0.6922137141227722\n",
            "Iteration: 357670 \t Train: Acc=51%, Loss=0.6912064552307129 \t\t Validation: Acc=50%, Loss=0.6975533366203308\n",
            "Iteration: 357680 \t Train: Acc=51%, Loss=0.6916916370391846 \t\t Validation: Acc=50%, Loss=0.697007954120636\n",
            "Iteration: 357690 \t Train: Acc=44%, Loss=0.7052714228630066 \t\t Validation: Acc=50%, Loss=0.6924947500228882\n",
            "Iteration: 357700 \t Train: Acc=50%, Loss=0.6908051371574402 \t\t Validation: Acc=51%, Loss=0.6902915239334106\n",
            "Iteration: 357710 \t Train: Acc=47%, Loss=0.6943917274475098 \t\t Validation: Acc=53%, Loss=0.6980747580528259\n",
            "Iteration: 357720 \t Train: Acc=51%, Loss=0.6839901208877563 \t\t Validation: Acc=50%, Loss=0.6901921033859253\n",
            "Iteration: 357730 \t Train: Acc=50%, Loss=0.6918792724609375 \t\t Validation: Acc=53%, Loss=0.6956800222396851\n",
            "Iteration: 357740 \t Train: Acc=57%, Loss=0.6859397888183594 \t\t Validation: Acc=50%, Loss=0.6937112808227539\n",
            "Iteration: 357750 \t Train: Acc=53%, Loss=0.6920363306999207 \t\t Validation: Acc=53%, Loss=0.6958315968513489\n",
            "Iteration: 357760 \t Train: Acc=46%, Loss=0.6936115026473999 \t\t Validation: Acc=50%, Loss=0.6921740770339966\n",
            "Iteration: 357770 \t Train: Acc=53%, Loss=0.6904745101928711 \t\t Validation: Acc=50%, Loss=0.6913928985595703\n",
            "Iteration: 357780 \t Train: Acc=49%, Loss=0.693121612071991 \t\t Validation: Acc=53%, Loss=0.6871334314346313\n",
            "Iteration: 357790 \t Train: Acc=46%, Loss=0.70055091381073 \t\t Validation: Acc=50%, Loss=0.6923060417175293\n",
            "Iteration: 357800 \t Train: Acc=50%, Loss=0.6942276954650879 \t\t Validation: Acc=50%, Loss=0.6947379112243652\n",
            "Iteration: 357810 \t Train: Acc=56%, Loss=0.6873911619186401 \t\t Validation: Acc=51%, Loss=0.6856740117073059\n",
            "Iteration: 357820 \t Train: Acc=51%, Loss=0.6862397193908691 \t\t Validation: Acc=53%, Loss=0.6910072565078735\n",
            "Iteration: 357830 \t Train: Acc=44%, Loss=0.6967335343360901 \t\t Validation: Acc=53%, Loss=0.6884199380874634\n",
            "Iteration: 357840 \t Train: Acc=48%, Loss=0.6859982013702393 \t\t Validation: Acc=51%, Loss=0.6909157037734985\n",
            "Iteration: 357850 \t Train: Acc=52%, Loss=0.6892596483230591 \t\t Validation: Acc=45%, Loss=0.695540189743042\n",
            "Iteration: 357860 \t Train: Acc=49%, Loss=0.6949918270111084 \t\t Validation: Acc=49%, Loss=0.692071795463562\n",
            "Iteration: 357870 \t Train: Acc=50%, Loss=0.6906457543373108 \t\t Validation: Acc=49%, Loss=0.6925754547119141\n",
            "Iteration: 357880 \t Train: Acc=50%, Loss=0.6896625757217407 \t\t Validation: Acc=51%, Loss=0.6923503279685974\n",
            "Iteration: 357890 \t Train: Acc=51%, Loss=0.6866703033447266 \t\t Validation: Acc=50%, Loss=0.6920335292816162\n",
            "Iteration: 357900 \t Train: Acc=46%, Loss=0.7017762660980225 \t\t Validation: Acc=46%, Loss=0.6912296414375305\n",
            "Iteration: 357910 \t Train: Acc=55%, Loss=0.6833082437515259 \t\t Validation: Acc=50%, Loss=0.690684974193573\n",
            "Iteration: 357920 \t Train: Acc=55%, Loss=0.6853142976760864 \t\t Validation: Acc=50%, Loss=0.7001927495002747\n",
            "Iteration: 357930 \t Train: Acc=45%, Loss=0.6908324956893921 \t\t Validation: Acc=50%, Loss=0.6930212378501892\n",
            "Iteration: 357940 \t Train: Acc=53%, Loss=0.6881964206695557 \t\t Validation: Acc=48%, Loss=0.6998275518417358\n",
            "Iteration: 357950 \t Train: Acc=57%, Loss=0.6855573058128357 \t\t Validation: Acc=48%, Loss=0.6853222846984863\n",
            "Iteration: 357960 \t Train: Acc=55%, Loss=0.6972389221191406 \t\t Validation: Acc=50%, Loss=0.6909502744674683\n",
            "Iteration: 357970 \t Train: Acc=53%, Loss=0.6904961466789246 \t\t Validation: Acc=48%, Loss=0.6939041018486023\n",
            "Iteration: 357980 \t Train: Acc=50%, Loss=0.6853391528129578 \t\t Validation: Acc=51%, Loss=0.6935359835624695\n",
            "Iteration: 357990 \t Train: Acc=54%, Loss=0.6822476387023926 \t\t Validation: Acc=49%, Loss=0.6945589780807495\n",
            "Iteration: 358000 \t Train: Acc=48%, Loss=0.6920061111450195 \t\t Validation: Acc=50%, Loss=0.6967210173606873\n",
            "Iteration: 358010 \t Train: Acc=50%, Loss=0.6923350095748901 \t\t Validation: Acc=50%, Loss=0.6923110485076904\n",
            "Iteration: 358020 \t Train: Acc=51%, Loss=0.694243311882019 \t\t Validation: Acc=51%, Loss=0.6893834471702576\n",
            "Iteration: 358030 \t Train: Acc=56%, Loss=0.6882672309875488 \t\t Validation: Acc=51%, Loss=0.6935982704162598\n",
            "Iteration: 358040 \t Train: Acc=51%, Loss=0.694091796875 \t\t Validation: Acc=50%, Loss=0.699706494808197\n",
            "Iteration: 358050 \t Train: Acc=53%, Loss=0.6866433620452881 \t\t Validation: Acc=43%, Loss=0.6969730854034424\n",
            "Iteration: 358060 \t Train: Acc=50%, Loss=0.6955252885818481 \t\t Validation: Acc=52%, Loss=0.6887891292572021\n",
            "Iteration: 358070 \t Train: Acc=50%, Loss=0.6955312490463257 \t\t Validation: Acc=50%, Loss=0.6943807005882263\n",
            "Iteration: 358080 \t Train: Acc=46%, Loss=0.6976161003112793 \t\t Validation: Acc=49%, Loss=0.6912707090377808\n",
            "Iteration: 358090 \t Train: Acc=50%, Loss=0.6847641468048096 \t\t Validation: Acc=46%, Loss=0.6970727443695068\n",
            "Iteration: 358100 \t Train: Acc=58%, Loss=0.6813062429428101 \t\t Validation: Acc=50%, Loss=0.69417804479599\n",
            "Iteration: 358110 \t Train: Acc=52%, Loss=0.6919558048248291 \t\t Validation: Acc=50%, Loss=0.6896181702613831\n",
            "Iteration: 358120 \t Train: Acc=50%, Loss=0.681036651134491 \t\t Validation: Acc=50%, Loss=0.6931803822517395\n",
            "Iteration: 358130 \t Train: Acc=52%, Loss=0.6901621222496033 \t\t Validation: Acc=51%, Loss=0.6907272338867188\n",
            "Iteration: 358140 \t Train: Acc=45%, Loss=0.6951454281806946 \t\t Validation: Acc=50%, Loss=0.6888772249221802\n",
            "Iteration: 358150 \t Train: Acc=52%, Loss=0.6891348361968994 \t\t Validation: Acc=53%, Loss=0.6897128820419312\n",
            "Iteration: 358160 \t Train: Acc=52%, Loss=0.6964789032936096 \t\t Validation: Acc=50%, Loss=0.6882814168930054\n",
            "Iteration: 358170 \t Train: Acc=54%, Loss=0.6908820867538452 \t\t Validation: Acc=48%, Loss=0.6957343816757202\n",
            "Iteration: 358180 \t Train: Acc=51%, Loss=0.6947637796401978 \t\t Validation: Acc=50%, Loss=0.6904925107955933\n",
            "Iteration: 358190 \t Train: Acc=50%, Loss=0.6910004615783691 \t\t Validation: Acc=50%, Loss=0.6958290934562683\n",
            "Iteration: 358200 \t Train: Acc=52%, Loss=0.6910350322723389 \t\t Validation: Acc=47%, Loss=0.7012364864349365\n",
            "Iteration: 358210 \t Train: Acc=47%, Loss=0.6928310990333557 \t\t Validation: Acc=47%, Loss=0.6952298283576965\n",
            "Iteration: 358220 \t Train: Acc=55%, Loss=0.6837694644927979 \t\t Validation: Acc=46%, Loss=0.6934616565704346\n",
            "Iteration: 358230 \t Train: Acc=45%, Loss=0.6983784437179565 \t\t Validation: Acc=52%, Loss=0.6890242695808411\n",
            "Iteration: 358240 \t Train: Acc=53%, Loss=0.6895015239715576 \t\t Validation: Acc=50%, Loss=0.6982228755950928\n",
            "Iteration: 358250 \t Train: Acc=50%, Loss=0.6906487345695496 \t\t Validation: Acc=51%, Loss=0.6956166625022888\n",
            "Iteration: 358260 \t Train: Acc=46%, Loss=0.6926106214523315 \t\t Validation: Acc=50%, Loss=0.692977786064148\n",
            "Iteration: 358270 \t Train: Acc=43%, Loss=0.7008246183395386 \t\t Validation: Acc=49%, Loss=0.6925774812698364\n",
            "Iteration: 358280 \t Train: Acc=54%, Loss=0.6933656930923462 \t\t Validation: Acc=48%, Loss=0.6922062039375305\n",
            "Iteration: 358290 \t Train: Acc=51%, Loss=0.6905134916305542 \t\t Validation: Acc=49%, Loss=0.6897549629211426\n",
            "Iteration: 358300 \t Train: Acc=55%, Loss=0.6929383277893066 \t\t Validation: Acc=48%, Loss=0.6957129836082458\n",
            "Iteration: 358310 \t Train: Acc=53%, Loss=0.6924560070037842 \t\t Validation: Acc=48%, Loss=0.6947021484375\n",
            "Iteration: 358320 \t Train: Acc=55%, Loss=0.6765283942222595 \t\t Validation: Acc=52%, Loss=0.6950994729995728\n",
            "Iteration: 358330 \t Train: Acc=50%, Loss=0.6856095790863037 \t\t Validation: Acc=52%, Loss=0.6919500231742859\n",
            "Iteration: 358340 \t Train: Acc=49%, Loss=0.6965659260749817 \t\t Validation: Acc=48%, Loss=0.6936453580856323\n",
            "Iteration: 358350 \t Train: Acc=48%, Loss=0.6868529915809631 \t\t Validation: Acc=50%, Loss=0.6916073560714722\n",
            "Iteration: 358360 \t Train: Acc=54%, Loss=0.6898694038391113 \t\t Validation: Acc=55%, Loss=0.6954987049102783\n",
            "Iteration: 358370 \t Train: Acc=52%, Loss=0.6987875699996948 \t\t Validation: Acc=53%, Loss=0.6946251392364502\n",
            "Iteration: 358380 \t Train: Acc=55%, Loss=0.6861802935600281 \t\t Validation: Acc=52%, Loss=0.6918917894363403\n",
            "Iteration: 358390 \t Train: Acc=50%, Loss=0.7056949138641357 \t\t Validation: Acc=53%, Loss=0.6874930262565613\n",
            "Iteration: 358400 \t Train: Acc=53%, Loss=0.6873855590820312 \t\t Validation: Acc=49%, Loss=0.6931946873664856\n",
            "Iteration: 358410 \t Train: Acc=54%, Loss=0.6915621161460876 \t\t Validation: Acc=50%, Loss=0.6933226585388184\n",
            "Iteration: 358420 \t Train: Acc=48%, Loss=0.6966676115989685 \t\t Validation: Acc=49%, Loss=0.6934918165206909\n",
            "Iteration: 358430 \t Train: Acc=46%, Loss=0.6887595653533936 \t\t Validation: Acc=53%, Loss=0.6902045011520386\n",
            "Iteration: 358440 \t Train: Acc=48%, Loss=0.6919522881507874 \t\t Validation: Acc=52%, Loss=0.6985496878623962\n",
            "Iteration: 358450 \t Train: Acc=49%, Loss=0.6963796615600586 \t\t Validation: Acc=44%, Loss=0.6935622692108154\n",
            "Iteration: 358460 \t Train: Acc=60%, Loss=0.6832812428474426 \t\t Validation: Acc=51%, Loss=0.6921318769454956\n",
            "Iteration: 358470 \t Train: Acc=52%, Loss=0.693524956703186 \t\t Validation: Acc=48%, Loss=0.696243166923523\n",
            "Iteration: 358480 \t Train: Acc=51%, Loss=0.6904286742210388 \t\t Validation: Acc=51%, Loss=0.6994365453720093\n",
            "Iteration: 358490 \t Train: Acc=52%, Loss=0.6944218873977661 \t\t Validation: Acc=51%, Loss=0.6947340965270996\n",
            "Iteration: 358500 \t Train: Acc=50%, Loss=0.6931447386741638 \t\t Validation: Acc=46%, Loss=0.6931252479553223\n",
            "Iteration: 358510 \t Train: Acc=57%, Loss=0.6800317168235779 \t\t Validation: Acc=50%, Loss=0.6947903633117676\n",
            "Iteration: 358520 \t Train: Acc=46%, Loss=0.6937642097473145 \t\t Validation: Acc=48%, Loss=0.6915223598480225\n",
            "Iteration: 358530 \t Train: Acc=42%, Loss=0.6975147128105164 \t\t Validation: Acc=50%, Loss=0.6933602094650269\n",
            "Iteration: 358540 \t Train: Acc=48%, Loss=0.6916744112968445 \t\t Validation: Acc=47%, Loss=0.6962047815322876\n",
            "Iteration: 358550 \t Train: Acc=50%, Loss=0.695419192314148 \t\t Validation: Acc=50%, Loss=0.6875054240226746\n",
            "Iteration: 358560 \t Train: Acc=50%, Loss=0.696753203868866 \t\t Validation: Acc=50%, Loss=0.6931315660476685\n",
            "Iteration: 358570 \t Train: Acc=47%, Loss=0.6908731460571289 \t\t Validation: Acc=53%, Loss=0.6928340792655945\n",
            "Iteration: 358580 \t Train: Acc=46%, Loss=0.7002535462379456 \t\t Validation: Acc=50%, Loss=0.6926043629646301\n",
            "Iteration: 358590 \t Train: Acc=60%, Loss=0.6873726844787598 \t\t Validation: Acc=51%, Loss=0.6970611810684204\n",
            "Iteration: 358600 \t Train: Acc=49%, Loss=0.6875031590461731 \t\t Validation: Acc=50%, Loss=0.6972724199295044\n",
            "Iteration: 358610 \t Train: Acc=50%, Loss=0.6871063709259033 \t\t Validation: Acc=49%, Loss=0.6920500993728638\n",
            "Iteration: 358620 \t Train: Acc=53%, Loss=0.687193751335144 \t\t Validation: Acc=49%, Loss=0.6879335045814514\n",
            "Iteration: 358630 \t Train: Acc=53%, Loss=0.6890168786048889 \t\t Validation: Acc=52%, Loss=0.6939794421195984\n",
            "Iteration: 358640 \t Train: Acc=51%, Loss=0.7006802558898926 \t\t Validation: Acc=50%, Loss=0.6981601715087891\n",
            "Iteration: 358650 \t Train: Acc=44%, Loss=0.7002190351486206 \t\t Validation: Acc=51%, Loss=0.6909947395324707\n",
            "Iteration: 358660 \t Train: Acc=53%, Loss=0.6850811243057251 \t\t Validation: Acc=50%, Loss=0.6926731467247009\n",
            "Iteration: 358670 \t Train: Acc=53%, Loss=0.6900985836982727 \t\t Validation: Acc=46%, Loss=0.6978204846382141\n",
            "Iteration: 358680 \t Train: Acc=52%, Loss=0.6882100105285645 \t\t Validation: Acc=50%, Loss=0.6974678635597229\n",
            "Iteration: 358690 \t Train: Acc=53%, Loss=0.6884591579437256 \t\t Validation: Acc=50%, Loss=0.6917918920516968\n",
            "Iteration: 358700 \t Train: Acc=56%, Loss=0.6886031031608582 \t\t Validation: Acc=52%, Loss=0.6912596225738525\n",
            "Iteration: 358710 \t Train: Acc=52%, Loss=0.6907374262809753 \t\t Validation: Acc=50%, Loss=0.6950645446777344\n",
            "Iteration: 358720 \t Train: Acc=50%, Loss=0.6936506032943726 \t\t Validation: Acc=50%, Loss=0.6893692016601562\n",
            "Iteration: 358730 \t Train: Acc=49%, Loss=0.6937334537506104 \t\t Validation: Acc=50%, Loss=0.6928359270095825\n",
            "Iteration: 358740 \t Train: Acc=49%, Loss=0.6946813464164734 \t\t Validation: Acc=48%, Loss=0.6969630122184753\n",
            "Iteration: 358750 \t Train: Acc=48%, Loss=0.6950685381889343 \t\t Validation: Acc=53%, Loss=0.6914641857147217\n",
            "Iteration: 358760 \t Train: Acc=44%, Loss=0.6958445906639099 \t\t Validation: Acc=49%, Loss=0.6936097145080566\n",
            "Iteration: 358770 \t Train: Acc=52%, Loss=0.6960340142250061 \t\t Validation: Acc=53%, Loss=0.6918097734451294\n",
            "Iteration: 358780 \t Train: Acc=52%, Loss=0.6912190318107605 \t\t Validation: Acc=51%, Loss=0.6853866577148438\n",
            "Iteration: 358790 \t Train: Acc=53%, Loss=0.685379147529602 \t\t Validation: Acc=53%, Loss=0.6888515949249268\n",
            "Iteration: 358800 \t Train: Acc=53%, Loss=0.6985502243041992 \t\t Validation: Acc=50%, Loss=0.6910269856452942\n",
            "Iteration: 358810 \t Train: Acc=53%, Loss=0.6882288455963135 \t\t Validation: Acc=50%, Loss=0.6881416440010071\n",
            "Iteration: 358820 \t Train: Acc=56%, Loss=0.692621111869812 \t\t Validation: Acc=49%, Loss=0.6976568102836609\n",
            "Iteration: 358830 \t Train: Acc=47%, Loss=0.6988773941993713 \t\t Validation: Acc=47%, Loss=0.6992712020874023\n",
            "Iteration: 358840 \t Train: Acc=51%, Loss=0.6950981616973877 \t\t Validation: Acc=48%, Loss=0.6960065364837646\n",
            "Iteration: 358850 \t Train: Acc=51%, Loss=0.6901699900627136 \t\t Validation: Acc=53%, Loss=0.6910616159439087\n",
            "Iteration: 358860 \t Train: Acc=51%, Loss=0.690494179725647 \t\t Validation: Acc=48%, Loss=0.6915396451950073\n",
            "Iteration: 358870 \t Train: Acc=49%, Loss=0.6885921955108643 \t\t Validation: Acc=52%, Loss=0.6969759464263916\n",
            "Iteration: 358880 \t Train: Acc=50%, Loss=0.6971691846847534 \t\t Validation: Acc=48%, Loss=0.6945396065711975\n",
            "Iteration: 358890 \t Train: Acc=45%, Loss=0.699047863483429 \t\t Validation: Acc=46%, Loss=0.694922685623169\n",
            "Iteration: 358900 \t Train: Acc=53%, Loss=0.694840669631958 \t\t Validation: Acc=53%, Loss=0.690887451171875\n",
            "Iteration: 358910 \t Train: Acc=47%, Loss=0.6924262642860413 \t\t Validation: Acc=53%, Loss=0.6872244477272034\n",
            "Iteration: 358920 \t Train: Acc=55%, Loss=0.6877591609954834 \t\t Validation: Acc=49%, Loss=0.6921712756156921\n",
            "Iteration: 358930 \t Train: Acc=47%, Loss=0.7002090215682983 \t\t Validation: Acc=45%, Loss=0.6975703239440918\n",
            "Iteration: 358940 \t Train: Acc=53%, Loss=0.6871985197067261 \t\t Validation: Acc=47%, Loss=0.6943480968475342\n",
            "Iteration: 358950 \t Train: Acc=49%, Loss=0.7070149779319763 \t\t Validation: Acc=50%, Loss=0.6991872191429138\n",
            "Iteration: 358960 \t Train: Acc=57%, Loss=0.6867383122444153 \t\t Validation: Acc=50%, Loss=0.6963856220245361\n",
            "Iteration: 358970 \t Train: Acc=53%, Loss=0.6893570423126221 \t\t Validation: Acc=51%, Loss=0.689578652381897\n",
            "Iteration: 358980 \t Train: Acc=59%, Loss=0.6821329593658447 \t\t Validation: Acc=51%, Loss=0.6967893838882446\n",
            "Iteration: 358990 \t Train: Acc=46%, Loss=0.6819473505020142 \t\t Validation: Acc=53%, Loss=0.690689742565155\n",
            "Iteration: 359000 \t Train: Acc=55%, Loss=0.6848275661468506 \t\t Validation: Acc=51%, Loss=0.6930750608444214\n",
            "Iteration: 359010 \t Train: Acc=49%, Loss=0.69317626953125 \t\t Validation: Acc=50%, Loss=0.6893084049224854\n",
            "Iteration: 359020 \t Train: Acc=50%, Loss=0.6892019510269165 \t\t Validation: Acc=50%, Loss=0.6935348510742188\n",
            "Iteration: 359030 \t Train: Acc=47%, Loss=0.6946111917495728 \t\t Validation: Acc=50%, Loss=0.6919507384300232\n",
            "Iteration: 359040 \t Train: Acc=51%, Loss=0.7009217143058777 \t\t Validation: Acc=53%, Loss=0.6894411444664001\n",
            "Iteration: 359050 \t Train: Acc=47%, Loss=0.6993287205696106 \t\t Validation: Acc=50%, Loss=0.6893717050552368\n",
            "Iteration: 359060 \t Train: Acc=49%, Loss=0.6919025778770447 \t\t Validation: Acc=55%, Loss=0.6852707862854004\n",
            "Iteration: 359070 \t Train: Acc=44%, Loss=0.6982821226119995 \t\t Validation: Acc=46%, Loss=0.6966996192932129\n",
            "Iteration: 359080 \t Train: Acc=44%, Loss=0.6963335275650024 \t\t Validation: Acc=49%, Loss=0.6971331238746643\n",
            "Iteration: 359090 \t Train: Acc=51%, Loss=0.6867495179176331 \t\t Validation: Acc=50%, Loss=0.6863499283790588\n",
            "Iteration: 359100 \t Train: Acc=49%, Loss=0.689342737197876 \t\t Validation: Acc=49%, Loss=0.6906450986862183\n",
            "Iteration: 359110 \t Train: Acc=42%, Loss=0.69399094581604 \t\t Validation: Acc=52%, Loss=0.6921061277389526\n",
            "Iteration: 359120 \t Train: Acc=48%, Loss=0.6907987594604492 \t\t Validation: Acc=47%, Loss=0.6971338987350464\n",
            "Iteration: 359130 \t Train: Acc=52%, Loss=0.6817331910133362 \t\t Validation: Acc=49%, Loss=0.6930862069129944\n",
            "Iteration: 359140 \t Train: Acc=57%, Loss=0.6853880882263184 \t\t Validation: Acc=49%, Loss=0.6979570388793945\n",
            "Iteration: 359150 \t Train: Acc=51%, Loss=0.6848748922348022 \t\t Validation: Acc=50%, Loss=0.6865274310112\n",
            "Iteration: 359160 \t Train: Acc=50%, Loss=0.682421863079071 \t\t Validation: Acc=52%, Loss=0.6923291683197021\n",
            "Iteration: 359170 \t Train: Acc=54%, Loss=0.695156991481781 \t\t Validation: Acc=48%, Loss=0.6889703869819641\n",
            "Iteration: 359180 \t Train: Acc=50%, Loss=0.6918792128562927 \t\t Validation: Acc=51%, Loss=0.6899533271789551\n",
            "Iteration: 359190 \t Train: Acc=56%, Loss=0.6811793446540833 \t\t Validation: Acc=49%, Loss=0.6951018571853638\n",
            "Iteration: 359200 \t Train: Acc=51%, Loss=0.6873356699943542 \t\t Validation: Acc=51%, Loss=0.6928814053535461\n",
            "Iteration: 359210 \t Train: Acc=50%, Loss=0.7010340690612793 \t\t Validation: Acc=50%, Loss=0.691237211227417\n",
            "Iteration: 359220 \t Train: Acc=51%, Loss=0.694751501083374 \t\t Validation: Acc=52%, Loss=0.6939321756362915\n",
            "Iteration: 359230 \t Train: Acc=57%, Loss=0.6888769865036011 \t\t Validation: Acc=50%, Loss=0.6924999952316284\n",
            "Iteration: 359240 \t Train: Acc=52%, Loss=0.6907506585121155 \t\t Validation: Acc=47%, Loss=0.695168673992157\n",
            "Iteration: 359250 \t Train: Acc=55%, Loss=0.688512921333313 \t\t Validation: Acc=50%, Loss=0.6928219795227051\n",
            "Iteration: 359260 \t Train: Acc=54%, Loss=0.684357762336731 \t\t Validation: Acc=50%, Loss=0.6900471448898315\n",
            "Iteration: 359270 \t Train: Acc=52%, Loss=0.6881585717201233 \t\t Validation: Acc=50%, Loss=0.6954125165939331\n",
            "Iteration: 359280 \t Train: Acc=56%, Loss=0.6857693791389465 \t\t Validation: Acc=48%, Loss=0.6892333030700684\n",
            "Iteration: 359290 \t Train: Acc=53%, Loss=0.6900323629379272 \t\t Validation: Acc=47%, Loss=0.6998220682144165\n",
            "Iteration: 359300 \t Train: Acc=54%, Loss=0.6880813241004944 \t\t Validation: Acc=51%, Loss=0.692801833152771\n",
            "Iteration: 359310 \t Train: Acc=55%, Loss=0.6856969594955444 \t\t Validation: Acc=43%, Loss=0.7001926898956299\n",
            "Iteration: 359320 \t Train: Acc=60%, Loss=0.689486563205719 \t\t Validation: Acc=51%, Loss=0.6935468316078186\n",
            "Iteration: 359330 \t Train: Acc=53%, Loss=0.6805193424224854 \t\t Validation: Acc=51%, Loss=0.6904117465019226\n",
            "Iteration: 359340 \t Train: Acc=48%, Loss=0.691615641117096 \t\t Validation: Acc=48%, Loss=0.6931189298629761\n",
            "Iteration: 359350 \t Train: Acc=49%, Loss=0.6977099776268005 \t\t Validation: Acc=49%, Loss=0.692650556564331\n",
            "Iteration: 359360 \t Train: Acc=53%, Loss=0.7001515626907349 \t\t Validation: Acc=49%, Loss=0.6875995397567749\n",
            "Iteration: 359370 \t Train: Acc=50%, Loss=0.6961054801940918 \t\t Validation: Acc=52%, Loss=0.6925129294395447\n",
            "Iteration: 359380 \t Train: Acc=50%, Loss=0.6868375539779663 \t\t Validation: Acc=50%, Loss=0.6923782229423523\n",
            "Iteration: 359390 \t Train: Acc=48%, Loss=0.6977249383926392 \t\t Validation: Acc=53%, Loss=0.6925070881843567\n",
            "Iteration: 359400 \t Train: Acc=54%, Loss=0.6938360333442688 \t\t Validation: Acc=50%, Loss=0.6960734128952026\n",
            "Iteration: 359410 \t Train: Acc=53%, Loss=0.6867867708206177 \t\t Validation: Acc=50%, Loss=0.6942442059516907\n",
            "Iteration: 359420 \t Train: Acc=52%, Loss=0.6887866258621216 \t\t Validation: Acc=53%, Loss=0.6902561783790588\n",
            "Iteration: 359430 \t Train: Acc=47%, Loss=0.6978389620780945 \t\t Validation: Acc=51%, Loss=0.6932249665260315\n",
            "Iteration: 359440 \t Train: Acc=56%, Loss=0.684269368648529 \t\t Validation: Acc=49%, Loss=0.6941506266593933\n",
            "Iteration: 359450 \t Train: Acc=53%, Loss=0.6853314638137817 \t\t Validation: Acc=50%, Loss=0.6875462532043457\n",
            "Iteration: 359460 \t Train: Acc=50%, Loss=0.691887378692627 \t\t Validation: Acc=49%, Loss=0.6903684139251709\n",
            "Iteration: 359470 \t Train: Acc=50%, Loss=0.6911381483078003 \t\t Validation: Acc=49%, Loss=0.7046160697937012\n",
            "Iteration: 359480 \t Train: Acc=57%, Loss=0.6829787492752075 \t\t Validation: Acc=51%, Loss=0.6930208802223206\n",
            "Iteration: 359490 \t Train: Acc=51%, Loss=0.6884135603904724 \t\t Validation: Acc=52%, Loss=0.6946509480476379\n",
            "Iteration: 359500 \t Train: Acc=58%, Loss=0.6810008883476257 \t\t Validation: Acc=53%, Loss=0.6946417689323425\n",
            "Iteration: 359510 \t Train: Acc=50%, Loss=0.6923660039901733 \t\t Validation: Acc=55%, Loss=0.6903708577156067\n",
            "Iteration: 359520 \t Train: Acc=51%, Loss=0.6932311654090881 \t\t Validation: Acc=52%, Loss=0.6948255896568298\n",
            "Iteration: 359530 \t Train: Acc=44%, Loss=0.6966963410377502 \t\t Validation: Acc=54%, Loss=0.6947832703590393\n",
            "Iteration: 359540 \t Train: Acc=50%, Loss=0.7007902264595032 \t\t Validation: Acc=48%, Loss=0.6916853785514832\n",
            "Iteration: 359550 \t Train: Acc=56%, Loss=0.6800302267074585 \t\t Validation: Acc=50%, Loss=0.6909978985786438\n",
            "Iteration: 359560 \t Train: Acc=54%, Loss=0.6897273063659668 \t\t Validation: Acc=50%, Loss=0.694810688495636\n",
            "Iteration: 359570 \t Train: Acc=57%, Loss=0.6889450550079346 \t\t Validation: Acc=49%, Loss=0.6974912881851196\n",
            "Iteration: 359580 \t Train: Acc=49%, Loss=0.6898493766784668 \t\t Validation: Acc=46%, Loss=0.7080475091934204\n",
            "Iteration: 359590 \t Train: Acc=56%, Loss=0.6861119270324707 \t\t Validation: Acc=49%, Loss=0.6957486867904663\n",
            "Iteration: 359600 \t Train: Acc=55%, Loss=0.6866140961647034 \t\t Validation: Acc=48%, Loss=0.6949652433395386\n",
            "Iteration: 359610 \t Train: Acc=53%, Loss=0.6916362047195435 \t\t Validation: Acc=50%, Loss=0.6986548900604248\n",
            "Iteration: 359620 \t Train: Acc=50%, Loss=0.6823195219039917 \t\t Validation: Acc=50%, Loss=0.6976891160011292\n",
            "Iteration: 359630 \t Train: Acc=46%, Loss=0.6940611600875854 \t\t Validation: Acc=46%, Loss=0.6971135139465332\n",
            "Iteration: 359640 \t Train: Acc=53%, Loss=0.6795740127563477 \t\t Validation: Acc=51%, Loss=0.6844820976257324\n",
            "Iteration: 359650 \t Train: Acc=48%, Loss=0.6914520263671875 \t\t Validation: Acc=46%, Loss=0.6960968375205994\n",
            "Iteration: 359660 \t Train: Acc=50%, Loss=0.6949236392974854 \t\t Validation: Acc=47%, Loss=0.6999072432518005\n",
            "Iteration: 359670 \t Train: Acc=52%, Loss=0.6879497170448303 \t\t Validation: Acc=52%, Loss=0.6913139820098877\n",
            "Iteration: 359680 \t Train: Acc=53%, Loss=0.686423659324646 \t\t Validation: Acc=51%, Loss=0.6973721981048584\n",
            "Iteration: 359690 \t Train: Acc=51%, Loss=0.691307783126831 \t\t Validation: Acc=49%, Loss=0.6959742307662964\n",
            "Iteration: 359700 \t Train: Acc=42%, Loss=0.7001120448112488 \t\t Validation: Acc=50%, Loss=0.6970080137252808\n",
            "Iteration: 359710 \t Train: Acc=44%, Loss=0.7032151818275452 \t\t Validation: Acc=51%, Loss=0.694564163684845\n",
            "Iteration: 359720 \t Train: Acc=50%, Loss=0.6873573064804077 \t\t Validation: Acc=50%, Loss=0.7007880806922913\n",
            "Iteration: 359730 \t Train: Acc=50%, Loss=0.685483992099762 \t\t Validation: Acc=49%, Loss=0.6965926885604858\n",
            "Iteration: 359740 \t Train: Acc=48%, Loss=0.6918243169784546 \t\t Validation: Acc=48%, Loss=0.6859177350997925\n",
            "Iteration: 359750 \t Train: Acc=46%, Loss=0.6916611790657043 \t\t Validation: Acc=50%, Loss=0.6808153986930847\n",
            "Iteration: 359760 \t Train: Acc=47%, Loss=0.6999608278274536 \t\t Validation: Acc=52%, Loss=0.6943750977516174\n",
            "Iteration: 359770 \t Train: Acc=51%, Loss=0.7096821069717407 \t\t Validation: Acc=52%, Loss=0.6950019001960754\n",
            "Iteration: 359780 \t Train: Acc=48%, Loss=0.6962431073188782 \t\t Validation: Acc=48%, Loss=0.6926717162132263\n",
            "Iteration: 359790 \t Train: Acc=51%, Loss=0.6853030323982239 \t\t Validation: Acc=52%, Loss=0.6932358741760254\n",
            "Iteration: 359800 \t Train: Acc=46%, Loss=0.6876617670059204 \t\t Validation: Acc=52%, Loss=0.6973685622215271\n",
            "Iteration: 359810 \t Train: Acc=48%, Loss=0.6914646625518799 \t\t Validation: Acc=50%, Loss=0.6991029381752014\n",
            "Iteration: 359820 \t Train: Acc=50%, Loss=0.6893576383590698 \t\t Validation: Acc=49%, Loss=0.6868420243263245\n",
            "Iteration: 359830 \t Train: Acc=48%, Loss=0.6984353065490723 \t\t Validation: Acc=49%, Loss=0.6881914138793945\n",
            "Iteration: 359840 \t Train: Acc=45%, Loss=0.6943932175636292 \t\t Validation: Acc=48%, Loss=0.6873730421066284\n",
            "Iteration: 359850 \t Train: Acc=45%, Loss=0.7049477100372314 \t\t Validation: Acc=48%, Loss=0.696428656578064\n",
            "Iteration: 359860 \t Train: Acc=54%, Loss=0.6886807680130005 \t\t Validation: Acc=48%, Loss=0.6972216963768005\n",
            "Iteration: 359870 \t Train: Acc=52%, Loss=0.6974186897277832 \t\t Validation: Acc=48%, Loss=0.6929625272750854\n",
            "Iteration: 359880 \t Train: Acc=55%, Loss=0.6847360134124756 \t\t Validation: Acc=50%, Loss=0.6933191418647766\n",
            "Iteration: 359890 \t Train: Acc=48%, Loss=0.694403350353241 \t\t Validation: Acc=51%, Loss=0.6951476335525513\n",
            "Iteration: 359900 \t Train: Acc=53%, Loss=0.6937973499298096 \t\t Validation: Acc=50%, Loss=0.6904096007347107\n",
            "Iteration: 359910 \t Train: Acc=51%, Loss=0.6831084489822388 \t\t Validation: Acc=53%, Loss=0.685339093208313\n",
            "Iteration: 359920 \t Train: Acc=50%, Loss=0.6841601133346558 \t\t Validation: Acc=50%, Loss=0.6962871551513672\n",
            "Iteration: 359930 \t Train: Acc=53%, Loss=0.6907878518104553 \t\t Validation: Acc=48%, Loss=0.6994410157203674\n",
            "Iteration: 359940 \t Train: Acc=51%, Loss=0.7012540102005005 \t\t Validation: Acc=53%, Loss=0.7000501155853271\n",
            "Iteration: 359950 \t Train: Acc=50%, Loss=0.6812906861305237 \t\t Validation: Acc=51%, Loss=0.6949489116668701\n",
            "It's been too long since we last saved the model. Saving...\n",
            "Iteration: 359960 \t Train: Acc=46%, Loss=0.6926991939544678 \t\t Validation: Acc=48%, Loss=0.6936392784118652\n",
            "Iteration: 359970 \t Train: Acc=50%, Loss=0.6895566582679749 \t\t Validation: Acc=50%, Loss=0.697102963924408\n",
            "Iteration: 359980 \t Train: Acc=51%, Loss=0.6850749850273132 \t\t Validation: Acc=54%, Loss=0.6919909119606018\n",
            "Iteration: 359990 \t Train: Acc=53%, Loss=0.6933290958404541 \t\t Validation: Acc=46%, Loss=0.6953496336936951\n",
            "Iteration: 360000 \t Train: Acc=53%, Loss=0.702826201915741 \t\t Validation: Acc=51%, Loss=0.6911157369613647\n",
            "Iteration: 360010 \t Train: Acc=48%, Loss=0.6942130327224731 \t\t Validation: Acc=49%, Loss=0.6985377073287964\n",
            "Iteration: 360020 \t Train: Acc=52%, Loss=0.6935495138168335 \t\t Validation: Acc=46%, Loss=0.6961339712142944\n",
            "Iteration: 360030 \t Train: Acc=46%, Loss=0.6924852132797241 \t\t Validation: Acc=49%, Loss=0.6923357248306274\n",
            "Iteration: 360040 \t Train: Acc=46%, Loss=0.6948684453964233 \t\t Validation: Acc=50%, Loss=0.6997684836387634\n",
            "Iteration: 360050 \t Train: Acc=52%, Loss=0.6902728080749512 \t\t Validation: Acc=50%, Loss=0.6919857859611511\n",
            "Iteration: 360060 \t Train: Acc=51%, Loss=0.6952464580535889 \t\t Validation: Acc=51%, Loss=0.6925889253616333\n",
            "Iteration: 360070 \t Train: Acc=50%, Loss=0.6859597563743591 \t\t Validation: Acc=50%, Loss=0.6979845762252808\n",
            "Iteration: 360080 \t Train: Acc=47%, Loss=0.6904678344726562 \t\t Validation: Acc=47%, Loss=0.6917160153388977\n",
            "Iteration: 360090 \t Train: Acc=52%, Loss=0.6879600286483765 \t\t Validation: Acc=46%, Loss=0.6968529224395752\n",
            "Iteration: 360100 \t Train: Acc=47%, Loss=0.6935005187988281 \t\t Validation: Acc=50%, Loss=0.6927922368049622\n",
            "Iteration: 360110 \t Train: Acc=49%, Loss=0.6919253468513489 \t\t Validation: Acc=52%, Loss=0.6937090754508972\n",
            "Iteration: 360120 \t Train: Acc=44%, Loss=0.7009075880050659 \t\t Validation: Acc=50%, Loss=0.6915210485458374\n",
            "Iteration: 360130 \t Train: Acc=51%, Loss=0.6858386993408203 \t\t Validation: Acc=51%, Loss=0.6932001113891602\n",
            "Iteration: 360140 \t Train: Acc=50%, Loss=0.6895048022270203 \t\t Validation: Acc=53%, Loss=0.6904117465019226\n",
            "Iteration: 360150 \t Train: Acc=49%, Loss=0.6949814558029175 \t\t Validation: Acc=50%, Loss=0.6925070881843567\n",
            "Iteration: 360160 \t Train: Acc=53%, Loss=0.6921141743659973 \t\t Validation: Acc=51%, Loss=0.6951068043708801\n",
            "Iteration: 360170 \t Train: Acc=49%, Loss=0.6963093876838684 \t\t Validation: Acc=46%, Loss=0.6913187503814697\n",
            "Iteration: 360180 \t Train: Acc=47%, Loss=0.6925023794174194 \t\t Validation: Acc=50%, Loss=0.693244218826294\n",
            "Iteration: 360190 \t Train: Acc=53%, Loss=0.6912445425987244 \t\t Validation: Acc=47%, Loss=0.6946552395820618\n",
            "Iteration: 360200 \t Train: Acc=52%, Loss=0.6983597874641418 \t\t Validation: Acc=50%, Loss=0.6954761743545532\n",
            "Iteration: 360210 \t Train: Acc=57%, Loss=0.6824994683265686 \t\t Validation: Acc=47%, Loss=0.6919835209846497\n",
            "Iteration: 360220 \t Train: Acc=58%, Loss=0.6886377930641174 \t\t Validation: Acc=50%, Loss=0.6964571475982666\n",
            "Iteration: 360230 \t Train: Acc=52%, Loss=0.685695469379425 \t\t Validation: Acc=53%, Loss=0.690255343914032\n",
            "Iteration: 360240 \t Train: Acc=46%, Loss=0.6925500631332397 \t\t Validation: Acc=47%, Loss=0.690934956073761\n",
            "Iteration: 360250 \t Train: Acc=51%, Loss=0.6869238018989563 \t\t Validation: Acc=50%, Loss=0.6914586424827576\n",
            "Iteration: 360260 \t Train: Acc=54%, Loss=0.6922606229782104 \t\t Validation: Acc=50%, Loss=0.6910866498947144\n",
            "Iteration: 360270 \t Train: Acc=47%, Loss=0.695443332195282 \t\t Validation: Acc=50%, Loss=0.6993395686149597\n",
            "Iteration: 360280 \t Train: Acc=50%, Loss=0.6854344010353088 \t\t Validation: Acc=49%, Loss=0.6964947581291199\n",
            "Iteration: 360290 \t Train: Acc=52%, Loss=0.6846621036529541 \t\t Validation: Acc=50%, Loss=0.692792534828186\n",
            "Iteration: 360300 \t Train: Acc=57%, Loss=0.6973615288734436 \t\t Validation: Acc=48%, Loss=0.6972659230232239\n",
            "Iteration: 360310 \t Train: Acc=54%, Loss=0.6753664612770081 \t\t Validation: Acc=50%, Loss=0.6941555738449097\n",
            "Iteration: 360320 \t Train: Acc=50%, Loss=0.695917010307312 \t\t Validation: Acc=50%, Loss=0.6970431804656982\n",
            "Iteration: 360330 \t Train: Acc=46%, Loss=0.6889420747756958 \t\t Validation: Acc=48%, Loss=0.6911742687225342\n",
            "Iteration: 360340 \t Train: Acc=46%, Loss=0.6883243918418884 \t\t Validation: Acc=50%, Loss=0.69215989112854\n",
            "Iteration: 360350 \t Train: Acc=50%, Loss=0.6943901777267456 \t\t Validation: Acc=46%, Loss=0.699277400970459\n",
            "Iteration: 360360 \t Train: Acc=50%, Loss=0.6922997236251831 \t\t Validation: Acc=48%, Loss=0.7016034126281738\n",
            "Iteration: 360370 \t Train: Acc=51%, Loss=0.6883323788642883 \t\t Validation: Acc=50%, Loss=0.694826602935791\n",
            "Iteration: 360380 \t Train: Acc=50%, Loss=0.6985953450202942 \t\t Validation: Acc=52%, Loss=0.6925324201583862\n",
            "Iteration: 360390 \t Train: Acc=51%, Loss=0.6884521842002869 \t\t Validation: Acc=50%, Loss=0.696527361869812\n",
            "Iteration: 360400 \t Train: Acc=51%, Loss=0.689617395401001 \t\t Validation: Acc=52%, Loss=0.6882980465888977\n",
            "Iteration: 360410 \t Train: Acc=51%, Loss=0.6886051893234253 \t\t Validation: Acc=53%, Loss=0.6938743591308594\n",
            "Iteration: 360420 \t Train: Acc=52%, Loss=0.6861515641212463 \t\t Validation: Acc=53%, Loss=0.6929387450218201\n",
            "Iteration: 360430 \t Train: Acc=48%, Loss=0.695220947265625 \t\t Validation: Acc=51%, Loss=0.6908426284790039\n",
            "Iteration: 360440 \t Train: Acc=47%, Loss=0.6929253935813904 \t\t Validation: Acc=53%, Loss=0.6984111070632935\n",
            "Iteration: 360450 \t Train: Acc=52%, Loss=0.6865262985229492 \t\t Validation: Acc=52%, Loss=0.692480206489563\n",
            "Iteration: 360460 \t Train: Acc=53%, Loss=0.6960063576698303 \t\t Validation: Acc=47%, Loss=0.6946648359298706\n",
            "Iteration: 360470 \t Train: Acc=42%, Loss=0.7060701847076416 \t\t Validation: Acc=46%, Loss=0.6965501308441162\n",
            "Iteration: 360480 \t Train: Acc=44%, Loss=0.6966843605041504 \t\t Validation: Acc=50%, Loss=0.6929003000259399\n",
            "Iteration: 360490 \t Train: Acc=54%, Loss=0.7045889496803284 \t\t Validation: Acc=47%, Loss=0.6969847083091736\n",
            "Iteration: 360500 \t Train: Acc=53%, Loss=0.689010500907898 \t\t Validation: Acc=50%, Loss=0.6889990568161011\n",
            "Iteration: 360510 \t Train: Acc=55%, Loss=0.690958559513092 \t\t Validation: Acc=46%, Loss=0.6916897892951965\n",
            "Iteration: 360520 \t Train: Acc=47%, Loss=0.6958997249603271 \t\t Validation: Acc=50%, Loss=0.6933447122573853\n",
            "Iteration: 360530 \t Train: Acc=53%, Loss=0.6975739002227783 \t\t Validation: Acc=47%, Loss=0.7030864357948303\n",
            "Iteration: 360540 \t Train: Acc=46%, Loss=0.6947098970413208 \t\t Validation: Acc=46%, Loss=0.6980743408203125\n",
            "Iteration: 360550 \t Train: Acc=41%, Loss=0.6933180689811707 \t\t Validation: Acc=50%, Loss=0.6946869492530823\n",
            "Iteration: 360560 \t Train: Acc=46%, Loss=0.6946292519569397 \t\t Validation: Acc=50%, Loss=0.6989696025848389\n",
            "Iteration: 360570 \t Train: Acc=55%, Loss=0.6838803291320801 \t\t Validation: Acc=53%, Loss=0.6915013790130615\n",
            "Iteration: 360580 \t Train: Acc=43%, Loss=0.7092243432998657 \t\t Validation: Acc=49%, Loss=0.692665696144104\n",
            "Iteration: 360590 \t Train: Acc=50%, Loss=0.6903613805770874 \t\t Validation: Acc=52%, Loss=0.6886465549468994\n",
            "Iteration: 360600 \t Train: Acc=47%, Loss=0.6964704990386963 \t\t Validation: Acc=53%, Loss=0.6938090324401855\n",
            "Iteration: 360610 \t Train: Acc=52%, Loss=0.6850801706314087 \t\t Validation: Acc=47%, Loss=0.7001788020133972\n",
            "Iteration: 360620 \t Train: Acc=53%, Loss=0.6890186667442322 \t\t Validation: Acc=51%, Loss=0.6912765502929688\n",
            "Iteration: 360630 \t Train: Acc=52%, Loss=0.6932373642921448 \t\t Validation: Acc=50%, Loss=0.6923258900642395\n",
            "Iteration: 360640 \t Train: Acc=57%, Loss=0.688056468963623 \t\t Validation: Acc=50%, Loss=0.696258008480072\n",
            "Iteration: 360650 \t Train: Acc=51%, Loss=0.6889094114303589 \t\t Validation: Acc=49%, Loss=0.6927663087844849\n",
            "Iteration: 360660 \t Train: Acc=50%, Loss=0.6892458200454712 \t\t Validation: Acc=48%, Loss=0.6963096261024475\n",
            "Iteration: 360670 \t Train: Acc=50%, Loss=0.6901125311851501 \t\t Validation: Acc=50%, Loss=0.6940865516662598\n",
            "Iteration: 360680 \t Train: Acc=54%, Loss=0.6925044655799866 \t\t Validation: Acc=50%, Loss=0.7006840705871582\n",
            "Iteration: 360690 \t Train: Acc=42%, Loss=0.6946104764938354 \t\t Validation: Acc=50%, Loss=0.6911518573760986\n",
            "Iteration: 360700 \t Train: Acc=50%, Loss=0.6944248676300049 \t\t Validation: Acc=50%, Loss=0.6930434703826904\n",
            "Iteration: 360710 \t Train: Acc=55%, Loss=0.6845084428787231 \t\t Validation: Acc=49%, Loss=0.6868661046028137\n",
            "Iteration: 360720 \t Train: Acc=56%, Loss=0.6923424005508423 \t\t Validation: Acc=49%, Loss=0.6861472725868225\n",
            "Iteration: 360730 \t Train: Acc=53%, Loss=0.6857488751411438 \t\t Validation: Acc=52%, Loss=0.6910369396209717\n",
            "Iteration: 360740 \t Train: Acc=51%, Loss=0.6937886476516724 \t\t Validation: Acc=52%, Loss=0.689769983291626\n",
            "Iteration: 360750 \t Train: Acc=48%, Loss=0.6896520853042603 \t\t Validation: Acc=51%, Loss=0.6901574730873108\n",
            "Iteration: 360760 \t Train: Acc=49%, Loss=0.6930547952651978 \t\t Validation: Acc=50%, Loss=0.7059416770935059\n",
            "Iteration: 360770 \t Train: Acc=42%, Loss=0.6967236399650574 \t\t Validation: Acc=51%, Loss=0.6969932913780212\n",
            "Iteration: 360780 \t Train: Acc=53%, Loss=0.6892386674880981 \t\t Validation: Acc=49%, Loss=0.6913747191429138\n",
            "Iteration: 360790 \t Train: Acc=52%, Loss=0.6824924945831299 \t\t Validation: Acc=54%, Loss=0.6886698007583618\n",
            "Iteration: 360800 \t Train: Acc=50%, Loss=0.6917403340339661 \t\t Validation: Acc=49%, Loss=0.6870589852333069\n",
            "Iteration: 360810 \t Train: Acc=50%, Loss=0.6970816850662231 \t\t Validation: Acc=49%, Loss=0.6897947192192078\n",
            "Iteration: 360820 \t Train: Acc=54%, Loss=0.6833560466766357 \t\t Validation: Acc=49%, Loss=0.6885507702827454\n",
            "Iteration: 360830 \t Train: Acc=50%, Loss=0.7000821828842163 \t\t Validation: Acc=49%, Loss=0.6896878480911255\n",
            "Iteration: 360840 \t Train: Acc=51%, Loss=0.697040319442749 \t\t Validation: Acc=49%, Loss=0.6918091177940369\n",
            "Iteration: 360850 \t Train: Acc=49%, Loss=0.6922056078910828 \t\t Validation: Acc=54%, Loss=0.6939367055892944\n",
            "Iteration: 360860 \t Train: Acc=49%, Loss=0.6918110251426697 \t\t Validation: Acc=50%, Loss=0.6947635412216187\n",
            "Iteration: 360870 \t Train: Acc=46%, Loss=0.7018681764602661 \t\t Validation: Acc=49%, Loss=0.6958728432655334\n",
            "Iteration: 360880 \t Train: Acc=50%, Loss=0.689986526966095 \t\t Validation: Acc=47%, Loss=0.6890135407447815\n",
            "Iteration: 360890 \t Train: Acc=53%, Loss=0.6903676986694336 \t\t Validation: Acc=51%, Loss=0.700104296207428\n",
            "Iteration: 360900 \t Train: Acc=57%, Loss=0.6893126964569092 \t\t Validation: Acc=49%, Loss=0.6943055391311646\n",
            "Iteration: 360910 \t Train: Acc=56%, Loss=0.6905264258384705 \t\t Validation: Acc=50%, Loss=0.6883735060691833\n",
            "Iteration: 360920 \t Train: Acc=50%, Loss=0.6936766505241394 \t\t Validation: Acc=50%, Loss=0.6938135027885437\n",
            "Iteration: 360930 \t Train: Acc=46%, Loss=0.6923845410346985 \t\t Validation: Acc=51%, Loss=0.6923173666000366\n",
            "Iteration: 360940 \t Train: Acc=51%, Loss=0.6859020590782166 \t\t Validation: Acc=48%, Loss=0.7001520395278931\n",
            "Iteration: 360950 \t Train: Acc=49%, Loss=0.6938889622688293 \t\t Validation: Acc=47%, Loss=0.6929324269294739\n",
            "Iteration: 360960 \t Train: Acc=51%, Loss=0.6922696828842163 \t\t Validation: Acc=51%, Loss=0.6911935806274414\n",
            "Iteration: 360970 \t Train: Acc=53%, Loss=0.6940141916275024 \t\t Validation: Acc=48%, Loss=0.6936736106872559\n",
            "Iteration: 360980 \t Train: Acc=50%, Loss=0.6911216378211975 \t\t Validation: Acc=50%, Loss=0.693332314491272\n",
            "Iteration: 360990 \t Train: Acc=51%, Loss=0.6903176307678223 \t\t Validation: Acc=50%, Loss=0.7039203643798828\n",
            "Iteration: 361000 \t Train: Acc=57%, Loss=0.6922820210456848 \t\t Validation: Acc=46%, Loss=0.6949426531791687\n",
            "Iteration: 361010 \t Train: Acc=47%, Loss=0.6943100690841675 \t\t Validation: Acc=48%, Loss=0.6941305994987488\n",
            "Iteration: 361020 \t Train: Acc=51%, Loss=0.6896143555641174 \t\t Validation: Acc=47%, Loss=0.6954770088195801\n",
            "Iteration: 361030 \t Train: Acc=50%, Loss=0.6875311136245728 \t\t Validation: Acc=50%, Loss=0.6919304728507996\n",
            "Iteration: 361040 \t Train: Acc=49%, Loss=0.693660318851471 \t\t Validation: Acc=49%, Loss=0.6968016624450684\n",
            "Iteration: 361050 \t Train: Acc=52%, Loss=0.6853604316711426 \t\t Validation: Acc=50%, Loss=0.6908308863639832\n",
            "Iteration: 361060 \t Train: Acc=56%, Loss=0.6923297643661499 \t\t Validation: Acc=53%, Loss=0.6967976093292236\n",
            "Iteration: 361070 \t Train: Acc=52%, Loss=0.6846527457237244 \t\t Validation: Acc=47%, Loss=0.6912186741828918\n",
            "Iteration: 361080 \t Train: Acc=57%, Loss=0.6921790838241577 \t\t Validation: Acc=53%, Loss=0.69078129529953\n",
            "Iteration: 361090 \t Train: Acc=49%, Loss=0.6932532787322998 \t\t Validation: Acc=50%, Loss=0.6989648342132568\n",
            "Iteration: 361100 \t Train: Acc=48%, Loss=0.6937785744667053 \t\t Validation: Acc=48%, Loss=0.6957100629806519\n",
            "Iteration: 361110 \t Train: Acc=53%, Loss=0.6878352165222168 \t\t Validation: Acc=47%, Loss=0.6964868307113647\n",
            "Iteration: 361120 \t Train: Acc=48%, Loss=0.6928918361663818 \t\t Validation: Acc=47%, Loss=0.698683500289917\n",
            "Iteration: 361130 \t Train: Acc=50%, Loss=0.7027007341384888 \t\t Validation: Acc=50%, Loss=0.6936548948287964\n",
            "Iteration: 361140 \t Train: Acc=60%, Loss=0.6849440336227417 \t\t Validation: Acc=50%, Loss=0.6961103677749634\n",
            "Iteration: 361150 \t Train: Acc=51%, Loss=0.693324863910675 \t\t Validation: Acc=53%, Loss=0.6877896189689636\n",
            "Iteration: 361160 \t Train: Acc=51%, Loss=0.6884599924087524 \t\t Validation: Acc=52%, Loss=0.69378662109375\n",
            "Iteration: 361170 \t Train: Acc=53%, Loss=0.6894006729125977 \t\t Validation: Acc=49%, Loss=0.705078125\n",
            "Iteration: 361180 \t Train: Acc=51%, Loss=0.6925022602081299 \t\t Validation: Acc=53%, Loss=0.6877723336219788\n",
            "Iteration: 361190 \t Train: Acc=55%, Loss=0.6855078935623169 \t\t Validation: Acc=53%, Loss=0.6945592761039734\n",
            "Iteration: 361200 \t Train: Acc=49%, Loss=0.689858615398407 \t\t Validation: Acc=49%, Loss=0.6986857056617737\n",
            "Iteration: 361210 \t Train: Acc=58%, Loss=0.6862717866897583 \t\t Validation: Acc=51%, Loss=0.6882905960083008\n",
            "Iteration: 361220 \t Train: Acc=55%, Loss=0.6871517896652222 \t\t Validation: Acc=50%, Loss=0.6953962445259094\n",
            "Iteration: 361230 \t Train: Acc=55%, Loss=0.6827572584152222 \t\t Validation: Acc=50%, Loss=0.6937652826309204\n",
            "Iteration: 361240 \t Train: Acc=49%, Loss=0.6887933015823364 \t\t Validation: Acc=48%, Loss=0.6933522820472717\n",
            "Iteration: 361250 \t Train: Acc=53%, Loss=0.6904797554016113 \t\t Validation: Acc=48%, Loss=0.6955453753471375\n",
            "Iteration: 361260 \t Train: Acc=50%, Loss=0.6878039240837097 \t\t Validation: Acc=50%, Loss=0.6905531883239746\n",
            "Iteration: 361270 \t Train: Acc=53%, Loss=0.6892144083976746 \t\t Validation: Acc=50%, Loss=0.6961713433265686\n",
            "Iteration: 361280 \t Train: Acc=55%, Loss=0.6892327666282654 \t\t Validation: Acc=50%, Loss=0.6968244910240173\n",
            "Iteration: 361290 \t Train: Acc=46%, Loss=0.6931123733520508 \t\t Validation: Acc=50%, Loss=0.6948803663253784\n",
            "Iteration: 361300 \t Train: Acc=53%, Loss=0.6902795433998108 \t\t Validation: Acc=53%, Loss=0.686784565448761\n",
            "Iteration: 361310 \t Train: Acc=49%, Loss=0.6933189034461975 \t\t Validation: Acc=50%, Loss=0.6932204961776733\n",
            "Iteration: 361320 \t Train: Acc=46%, Loss=0.6874850988388062 \t\t Validation: Acc=50%, Loss=0.683468222618103\n",
            "Iteration: 361330 \t Train: Acc=53%, Loss=0.6895897388458252 \t\t Validation: Acc=52%, Loss=0.6954314708709717\n",
            "Iteration: 361340 \t Train: Acc=53%, Loss=0.6912665367126465 \t\t Validation: Acc=50%, Loss=0.6904491782188416\n",
            "Iteration: 361350 \t Train: Acc=52%, Loss=0.6910912990570068 \t\t Validation: Acc=49%, Loss=0.6977248191833496\n",
            "Iteration: 361360 \t Train: Acc=53%, Loss=0.6885443925857544 \t\t Validation: Acc=53%, Loss=0.6883320212364197\n",
            "Iteration: 361370 \t Train: Acc=46%, Loss=0.6914627552032471 \t\t Validation: Acc=49%, Loss=0.6897546052932739\n",
            "Iteration: 361380 \t Train: Acc=48%, Loss=0.6971825361251831 \t\t Validation: Acc=50%, Loss=0.696999728679657\n",
            "Iteration: 361390 \t Train: Acc=50%, Loss=0.6913593411445618 \t\t Validation: Acc=46%, Loss=0.69962078332901\n",
            "Iteration: 361400 \t Train: Acc=50%, Loss=0.6933536529541016 \t\t Validation: Acc=50%, Loss=0.6864917874336243\n",
            "Iteration: 361410 \t Train: Acc=53%, Loss=0.6868603229522705 \t\t Validation: Acc=50%, Loss=0.6809551119804382\n",
            "Iteration: 361420 \t Train: Acc=48%, Loss=0.702393651008606 \t\t Validation: Acc=51%, Loss=0.6890777349472046\n",
            "Iteration: 361430 \t Train: Acc=46%, Loss=0.6966553926467896 \t\t Validation: Acc=49%, Loss=0.7082471251487732\n",
            "Iteration: 361440 \t Train: Acc=53%, Loss=0.6891393065452576 \t\t Validation: Acc=52%, Loss=0.6903029680252075\n",
            "Iteration: 361450 \t Train: Acc=54%, Loss=0.6879078149795532 \t\t Validation: Acc=50%, Loss=0.692130446434021\n",
            "Iteration: 361460 \t Train: Acc=48%, Loss=0.6889615654945374 \t\t Validation: Acc=49%, Loss=0.7012719511985779\n",
            "Iteration: 361470 \t Train: Acc=50%, Loss=0.6903424263000488 \t\t Validation: Acc=53%, Loss=0.6923057436943054\n",
            "Iteration: 361480 \t Train: Acc=46%, Loss=0.6944337487220764 \t\t Validation: Acc=51%, Loss=0.6941820383071899\n",
            "Iteration: 361490 \t Train: Acc=48%, Loss=0.6986365914344788 \t\t Validation: Acc=52%, Loss=0.6969156861305237\n",
            "Iteration: 361500 \t Train: Acc=53%, Loss=0.6863347291946411 \t\t Validation: Acc=50%, Loss=0.6918747425079346\n",
            "Iteration: 361510 \t Train: Acc=53%, Loss=0.6881567239761353 \t\t Validation: Acc=53%, Loss=0.6879125833511353\n",
            "Iteration: 361520 \t Train: Acc=50%, Loss=0.6932555437088013 \t\t Validation: Acc=47%, Loss=0.6876099109649658\n",
            "Iteration: 361530 \t Train: Acc=55%, Loss=0.6892396807670593 \t\t Validation: Acc=46%, Loss=0.6937255263328552\n",
            "Iteration: 361540 \t Train: Acc=45%, Loss=0.6947495937347412 \t\t Validation: Acc=51%, Loss=0.6947695016860962\n",
            "Iteration: 361550 \t Train: Acc=50%, Loss=0.6928781867027283 \t\t Validation: Acc=48%, Loss=0.6923399567604065\n",
            "Iteration: 361560 \t Train: Acc=54%, Loss=0.6856136322021484 \t\t Validation: Acc=49%, Loss=0.7082007527351379\n",
            "Iteration: 361570 \t Train: Acc=49%, Loss=0.69720458984375 \t\t Validation: Acc=50%, Loss=0.6912572979927063\n",
            "Iteration: 361580 \t Train: Acc=44%, Loss=0.6941115856170654 \t\t Validation: Acc=48%, Loss=0.6944370269775391\n",
            "Iteration: 361590 \t Train: Acc=48%, Loss=0.6901886463165283 \t\t Validation: Acc=49%, Loss=0.6887407898902893\n",
            "Iteration: 361600 \t Train: Acc=53%, Loss=0.6921894550323486 \t\t Validation: Acc=49%, Loss=0.6989660263061523\n",
            "Iteration: 361610 \t Train: Acc=53%, Loss=0.6896529793739319 \t\t Validation: Acc=50%, Loss=0.6967452764511108\n",
            "Iteration: 361620 \t Train: Acc=56%, Loss=0.6902589797973633 \t\t Validation: Acc=53%, Loss=0.6857896447181702\n",
            "Iteration: 361630 \t Train: Acc=53%, Loss=0.6917636394500732 \t\t Validation: Acc=48%, Loss=0.7002885341644287\n",
            "Iteration: 361640 \t Train: Acc=49%, Loss=0.6942043304443359 \t\t Validation: Acc=50%, Loss=0.6935968995094299\n",
            "Iteration: 361650 \t Train: Acc=51%, Loss=0.6966450214385986 \t\t Validation: Acc=50%, Loss=0.7054307460784912\n",
            "Iteration: 361660 \t Train: Acc=54%, Loss=0.6849972605705261 \t\t Validation: Acc=49%, Loss=0.6988798975944519\n",
            "Iteration: 361670 \t Train: Acc=52%, Loss=0.6931065320968628 \t\t Validation: Acc=53%, Loss=0.6976410746574402\n",
            "Iteration: 361680 \t Train: Acc=52%, Loss=0.6893242001533508 \t\t Validation: Acc=53%, Loss=0.7005941867828369\n",
            "Iteration: 361690 \t Train: Acc=50%, Loss=0.6913836598396301 \t\t Validation: Acc=51%, Loss=0.6932419538497925\n",
            "Iteration: 361700 \t Train: Acc=46%, Loss=0.6964374780654907 \t\t Validation: Acc=46%, Loss=0.708235502243042\n",
            "Iteration: 361710 \t Train: Acc=51%, Loss=0.6902503371238708 \t\t Validation: Acc=51%, Loss=0.6846753358840942\n",
            "Iteration: 361720 \t Train: Acc=51%, Loss=0.6938571929931641 \t\t Validation: Acc=50%, Loss=0.6894868612289429\n",
            "Iteration: 361730 \t Train: Acc=53%, Loss=0.6833236217498779 \t\t Validation: Acc=53%, Loss=0.7053734064102173\n",
            "Iteration: 361740 \t Train: Acc=54%, Loss=0.6822078227996826 \t\t Validation: Acc=53%, Loss=0.689873218536377\n",
            "Iteration: 361750 \t Train: Acc=54%, Loss=0.693516731262207 \t\t Validation: Acc=46%, Loss=0.7001498341560364\n",
            "Iteration: 361760 \t Train: Acc=47%, Loss=0.7021838426589966 \t\t Validation: Acc=50%, Loss=0.6971735954284668\n",
            "Iteration: 361770 \t Train: Acc=46%, Loss=0.6884605288505554 \t\t Validation: Acc=49%, Loss=0.6907793283462524\n",
            "Iteration: 361780 \t Train: Acc=50%, Loss=0.6907126307487488 \t\t Validation: Acc=50%, Loss=0.6950212717056274\n",
            "Iteration: 361790 \t Train: Acc=47%, Loss=0.6927540302276611 \t\t Validation: Acc=48%, Loss=0.6939034461975098\n",
            "Iteration: 361800 \t Train: Acc=46%, Loss=0.6998456716537476 \t\t Validation: Acc=48%, Loss=0.6962807774543762\n",
            "Iteration: 361810 \t Train: Acc=49%, Loss=0.689125657081604 \t\t Validation: Acc=49%, Loss=0.693565845489502\n",
            "Iteration: 361820 \t Train: Acc=50%, Loss=0.6933538913726807 \t\t Validation: Acc=50%, Loss=0.6979427933692932\n",
            "Iteration: 361830 \t Train: Acc=55%, Loss=0.6851493716239929 \t\t Validation: Acc=54%, Loss=0.6820530891418457\n",
            "Iteration: 361840 \t Train: Acc=54%, Loss=0.6906879544258118 \t\t Validation: Acc=50%, Loss=0.6866456270217896\n",
            "Iteration: 361850 \t Train: Acc=56%, Loss=0.6911929249763489 \t\t Validation: Acc=50%, Loss=0.6967025995254517\n",
            "Iteration: 361860 \t Train: Acc=52%, Loss=0.6973701119422913 \t\t Validation: Acc=50%, Loss=0.6896110773086548\n",
            "Iteration: 361870 \t Train: Acc=55%, Loss=0.6823744773864746 \t\t Validation: Acc=49%, Loss=0.6978774666786194\n",
            "Iteration: 361880 \t Train: Acc=52%, Loss=0.6859514117240906 \t\t Validation: Acc=50%, Loss=0.6952900290489197\n",
            "Iteration: 361890 \t Train: Acc=46%, Loss=0.6933367252349854 \t\t Validation: Acc=52%, Loss=0.6924653053283691\n",
            "Iteration: 361900 \t Train: Acc=50%, Loss=0.6866754293441772 \t\t Validation: Acc=47%, Loss=0.6949682831764221\n",
            "Iteration: 361910 \t Train: Acc=50%, Loss=0.6852644085884094 \t\t Validation: Acc=48%, Loss=0.6982492804527283\n",
            "Iteration: 361920 \t Train: Acc=50%, Loss=0.6917175054550171 \t\t Validation: Acc=47%, Loss=0.6944160461425781\n",
            "Iteration: 361930 \t Train: Acc=51%, Loss=0.6936476826667786 \t\t Validation: Acc=49%, Loss=0.6994553208351135\n",
            "Iteration: 361940 \t Train: Acc=52%, Loss=0.693349301815033 \t\t Validation: Acc=54%, Loss=0.6942250728607178\n",
            "Iteration: 361950 \t Train: Acc=51%, Loss=0.6903468370437622 \t\t Validation: Acc=51%, Loss=0.6936212182044983\n",
            "Iteration: 361960 \t Train: Acc=51%, Loss=0.687687337398529 \t\t Validation: Acc=52%, Loss=0.6856514811515808\n",
            "Iteration: 361970 \t Train: Acc=46%, Loss=0.689877986907959 \t\t Validation: Acc=51%, Loss=0.6772323846817017\n",
            "Iteration: 361980 \t Train: Acc=53%, Loss=0.6899195909500122 \t\t Validation: Acc=49%, Loss=0.6907730102539062\n",
            "Iteration: 361990 \t Train: Acc=45%, Loss=0.6896677017211914 \t\t Validation: Acc=47%, Loss=0.700111448764801\n",
            "Iteration: 362000 \t Train: Acc=44%, Loss=0.6981927156448364 \t\t Validation: Acc=49%, Loss=0.6968992352485657\n",
            "Iteration: 362010 \t Train: Acc=46%, Loss=0.7030336856842041 \t\t Validation: Acc=56%, Loss=0.6859842538833618\n",
            "Iteration: 362020 \t Train: Acc=51%, Loss=0.687100350856781 \t\t Validation: Acc=51%, Loss=0.6825796961784363\n",
            "Iteration: 362030 \t Train: Acc=46%, Loss=0.6939318180084229 \t\t Validation: Acc=50%, Loss=0.6970463395118713\n",
            "Iteration: 362040 \t Train: Acc=48%, Loss=0.6878598928451538 \t\t Validation: Acc=50%, Loss=0.6888266801834106\n",
            "Iteration: 362050 \t Train: Acc=53%, Loss=0.6952459812164307 \t\t Validation: Acc=51%, Loss=0.6878089904785156\n",
            "Iteration: 362060 \t Train: Acc=46%, Loss=0.6955912113189697 \t\t Validation: Acc=53%, Loss=0.6909892559051514\n",
            "Iteration: 362070 \t Train: Acc=50%, Loss=0.6816621422767639 \t\t Validation: Acc=50%, Loss=0.7043851613998413\n",
            "Iteration: 362080 \t Train: Acc=50%, Loss=0.6856629848480225 \t\t Validation: Acc=50%, Loss=0.6890288591384888\n",
            "Iteration: 362090 \t Train: Acc=54%, Loss=0.6911154389381409 \t\t Validation: Acc=46%, Loss=0.6939336657524109\n",
            "Iteration: 362100 \t Train: Acc=50%, Loss=0.7012354135513306 \t\t Validation: Acc=50%, Loss=0.6941129565238953\n",
            "Iteration: 362110 \t Train: Acc=49%, Loss=0.6949524283409119 \t\t Validation: Acc=51%, Loss=0.6932216882705688\n",
            "Iteration: 362120 \t Train: Acc=49%, Loss=0.6956230401992798 \t\t Validation: Acc=49%, Loss=0.6864239573478699\n",
            "Iteration: 362130 \t Train: Acc=57%, Loss=0.6890121698379517 \t\t Validation: Acc=49%, Loss=0.6956863403320312\n",
            "Iteration: 362140 \t Train: Acc=46%, Loss=0.690386950969696 \t\t Validation: Acc=47%, Loss=0.6942858695983887\n",
            "Iteration: 362150 \t Train: Acc=49%, Loss=0.6842141151428223 \t\t Validation: Acc=48%, Loss=0.6991299390792847\n",
            "Iteration: 362160 \t Train: Acc=51%, Loss=0.6921066641807556 \t\t Validation: Acc=48%, Loss=0.6998566389083862\n",
            "Iteration: 362170 \t Train: Acc=50%, Loss=0.6956095099449158 \t\t Validation: Acc=53%, Loss=0.696094274520874\n",
            "Iteration: 362180 \t Train: Acc=43%, Loss=0.6954406499862671 \t\t Validation: Acc=55%, Loss=0.6867042183876038\n",
            "Iteration: 362190 \t Train: Acc=50%, Loss=0.6948758363723755 \t\t Validation: Acc=51%, Loss=0.6873710751533508\n",
            "Iteration: 362200 \t Train: Acc=49%, Loss=0.6918296217918396 \t\t Validation: Acc=49%, Loss=0.6922615766525269\n",
            "Iteration: 362210 \t Train: Acc=47%, Loss=0.6899711489677429 \t\t Validation: Acc=50%, Loss=0.6878170967102051\n",
            "Iteration: 362220 \t Train: Acc=53%, Loss=0.6881977915763855 \t\t Validation: Acc=48%, Loss=0.6949201226234436\n",
            "Iteration: 362230 \t Train: Acc=51%, Loss=0.6904686689376831 \t\t Validation: Acc=50%, Loss=0.6914632320404053\n",
            "Iteration: 362240 \t Train: Acc=53%, Loss=0.6894485354423523 \t\t Validation: Acc=50%, Loss=0.6942306756973267\n",
            "Iteration: 362250 \t Train: Acc=56%, Loss=0.6901294589042664 \t\t Validation: Acc=48%, Loss=0.6978433132171631\n",
            "Iteration: 362260 \t Train: Acc=46%, Loss=0.7000869512557983 \t\t Validation: Acc=50%, Loss=0.6988043189048767\n",
            "Iteration: 362270 \t Train: Acc=46%, Loss=0.7034937143325806 \t\t Validation: Acc=50%, Loss=0.6864539980888367\n",
            "Iteration: 362280 \t Train: Acc=48%, Loss=0.6956618428230286 \t\t Validation: Acc=50%, Loss=0.6900767087936401\n",
            "Iteration: 362290 \t Train: Acc=53%, Loss=0.6953058242797852 \t\t Validation: Acc=46%, Loss=0.6910703182220459\n",
            "Iteration: 362300 \t Train: Acc=50%, Loss=0.6900771260261536 \t\t Validation: Acc=47%, Loss=0.696174681186676\n",
            "Iteration: 362310 \t Train: Acc=49%, Loss=0.6951026916503906 \t\t Validation: Acc=48%, Loss=0.6933920383453369\n",
            "Iteration: 362320 \t Train: Acc=50%, Loss=0.6919559240341187 \t\t Validation: Acc=50%, Loss=0.6846095323562622\n",
            "Iteration: 362330 \t Train: Acc=50%, Loss=0.6907196640968323 \t\t Validation: Acc=51%, Loss=0.6917199492454529\n",
            "Iteration: 362340 \t Train: Acc=52%, Loss=0.6900214552879333 \t\t Validation: Acc=49%, Loss=0.6932188868522644\n",
            "Iteration: 362350 \t Train: Acc=56%, Loss=0.6841607689857483 \t\t Validation: Acc=51%, Loss=0.6980500221252441\n",
            "Iteration: 362360 \t Train: Acc=51%, Loss=0.6867958307266235 \t\t Validation: Acc=53%, Loss=0.6897922158241272\n",
            "Iteration: 362370 \t Train: Acc=58%, Loss=0.6887677907943726 \t\t Validation: Acc=51%, Loss=0.695210874080658\n",
            "Iteration: 362380 \t Train: Acc=47%, Loss=0.6941274404525757 \t\t Validation: Acc=50%, Loss=0.6955713033676147\n",
            "Iteration: 362390 \t Train: Acc=49%, Loss=0.6970996260643005 \t\t Validation: Acc=48%, Loss=0.703160285949707\n",
            "Iteration: 362400 \t Train: Acc=54%, Loss=0.6845376491546631 \t\t Validation: Acc=50%, Loss=0.6939311027526855\n",
            "Iteration: 362410 \t Train: Acc=53%, Loss=0.6895209550857544 \t\t Validation: Acc=47%, Loss=0.6953958868980408\n",
            "Iteration: 362420 \t Train: Acc=54%, Loss=0.6889368891716003 \t\t Validation: Acc=50%, Loss=0.6956278085708618\n",
            "Iteration: 362430 \t Train: Acc=55%, Loss=0.6838886737823486 \t\t Validation: Acc=51%, Loss=0.6895984411239624\n",
            "Iteration: 362440 \t Train: Acc=50%, Loss=0.6929402351379395 \t\t Validation: Acc=51%, Loss=0.6959706544876099\n",
            "Iteration: 362450 \t Train: Acc=52%, Loss=0.6931496262550354 \t\t Validation: Acc=50%, Loss=0.6924721598625183\n",
            "Iteration: 362460 \t Train: Acc=51%, Loss=0.6930980682373047 \t\t Validation: Acc=51%, Loss=0.6882724761962891\n",
            "Iteration: 362470 \t Train: Acc=51%, Loss=0.6885002851486206 \t\t Validation: Acc=49%, Loss=0.6868245601654053\n",
            "Iteration: 362480 \t Train: Acc=53%, Loss=0.6900034546852112 \t\t Validation: Acc=50%, Loss=0.6955912709236145\n",
            "Iteration: 362490 \t Train: Acc=53%, Loss=0.689993679523468 \t\t Validation: Acc=49%, Loss=0.6952801942825317\n",
            "Iteration: 362500 \t Train: Acc=46%, Loss=0.6959810853004456 \t\t Validation: Acc=49%, Loss=0.6959362030029297\n",
            "Iteration: 362510 \t Train: Acc=54%, Loss=0.6854441165924072 \t\t Validation: Acc=50%, Loss=0.6952764987945557\n",
            "Iteration: 362520 \t Train: Acc=53%, Loss=0.687086284160614 \t\t Validation: Acc=48%, Loss=0.6951484680175781\n",
            "Iteration: 362530 \t Train: Acc=57%, Loss=0.687689483165741 \t\t Validation: Acc=51%, Loss=0.7026401162147522\n",
            "Iteration: 362540 \t Train: Acc=51%, Loss=0.6922822594642639 \t\t Validation: Acc=46%, Loss=0.7048872709274292\n",
            "Iteration: 362550 \t Train: Acc=47%, Loss=0.696943998336792 \t\t Validation: Acc=47%, Loss=0.6952972412109375\n",
            "Iteration: 362560 \t Train: Acc=44%, Loss=0.6912956237792969 \t\t Validation: Acc=51%, Loss=0.6920225024223328\n",
            "Iteration: 362570 \t Train: Acc=46%, Loss=0.6955785155296326 \t\t Validation: Acc=51%, Loss=0.6840337514877319\n",
            "Iteration: 362580 \t Train: Acc=53%, Loss=0.6927365660667419 \t\t Validation: Acc=48%, Loss=0.6958872079849243\n",
            "Iteration: 362590 \t Train: Acc=49%, Loss=0.6906453371047974 \t\t Validation: Acc=53%, Loss=0.6877894401550293\n",
            "Iteration: 362600 \t Train: Acc=52%, Loss=0.6899251937866211 \t\t Validation: Acc=51%, Loss=0.6952390074729919\n",
            "Iteration: 362610 \t Train: Acc=51%, Loss=0.693734347820282 \t\t Validation: Acc=51%, Loss=0.6932281255722046\n",
            "Iteration: 362620 \t Train: Acc=53%, Loss=0.6893517374992371 \t\t Validation: Acc=52%, Loss=0.6955004930496216\n",
            "Iteration: 362630 \t Train: Acc=50%, Loss=0.6931467652320862 \t\t Validation: Acc=52%, Loss=0.6921194791793823\n",
            "Iteration: 362640 \t Train: Acc=46%, Loss=0.6915592551231384 \t\t Validation: Acc=51%, Loss=0.6985660791397095\n",
            "Iteration: 362650 \t Train: Acc=56%, Loss=0.6863349676132202 \t\t Validation: Acc=47%, Loss=0.6968037486076355\n",
            "Iteration: 362660 \t Train: Acc=53%, Loss=0.6925407648086548 \t\t Validation: Acc=55%, Loss=0.6856274008750916\n",
            "Iteration: 362670 \t Train: Acc=52%, Loss=0.68950355052948 \t\t Validation: Acc=49%, Loss=0.6989103555679321\n",
            "Iteration: 362680 \t Train: Acc=53%, Loss=0.6896735429763794 \t\t Validation: Acc=50%, Loss=0.6937401294708252\n",
            "Iteration: 362690 \t Train: Acc=45%, Loss=0.6934194564819336 \t\t Validation: Acc=50%, Loss=0.6975059509277344\n",
            "Iteration: 362700 \t Train: Acc=51%, Loss=0.6923854351043701 \t\t Validation: Acc=50%, Loss=0.6936962604522705\n",
            "Iteration: 362710 \t Train: Acc=53%, Loss=0.6789849996566772 \t\t Validation: Acc=50%, Loss=0.6956061720848083\n",
            "Iteration: 362720 \t Train: Acc=50%, Loss=0.6908998489379883 \t\t Validation: Acc=50%, Loss=0.6945934891700745\n",
            "Iteration: 362730 \t Train: Acc=54%, Loss=0.6879538893699646 \t\t Validation: Acc=50%, Loss=0.6833660006523132\n",
            "Iteration: 362740 \t Train: Acc=47%, Loss=0.7011556625366211 \t\t Validation: Acc=46%, Loss=0.6947500109672546\n",
            "Iteration: 362750 \t Train: Acc=53%, Loss=0.7003506422042847 \t\t Validation: Acc=49%, Loss=0.6947529911994934\n",
            "Iteration: 362760 \t Train: Acc=46%, Loss=0.6978225708007812 \t\t Validation: Acc=49%, Loss=0.6908383369445801\n",
            "Iteration: 362770 \t Train: Acc=53%, Loss=0.6879711747169495 \t\t Validation: Acc=53%, Loss=0.6910122632980347\n",
            "Iteration: 362780 \t Train: Acc=56%, Loss=0.6836367249488831 \t\t Validation: Acc=50%, Loss=0.6970536708831787\n",
            "Iteration: 362790 \t Train: Acc=48%, Loss=0.6941324472427368 \t\t Validation: Acc=51%, Loss=0.6916837692260742\n",
            "Iteration: 362800 \t Train: Acc=50%, Loss=0.6868225336074829 \t\t Validation: Acc=50%, Loss=0.6825540065765381\n",
            "Iteration: 362810 \t Train: Acc=51%, Loss=0.6876857280731201 \t\t Validation: Acc=53%, Loss=0.6958484053611755\n",
            "Iteration: 362820 \t Train: Acc=50%, Loss=0.6881394982337952 \t\t Validation: Acc=49%, Loss=0.688089907169342\n",
            "Iteration: 362830 \t Train: Acc=52%, Loss=0.6892274022102356 \t\t Validation: Acc=53%, Loss=0.6901527047157288\n",
            "Iteration: 362840 \t Train: Acc=43%, Loss=0.6873922348022461 \t\t Validation: Acc=52%, Loss=0.6918822526931763\n",
            "Iteration: 362850 \t Train: Acc=52%, Loss=0.6837480664253235 \t\t Validation: Acc=50%, Loss=0.6915320754051208\n",
            "Iteration: 362860 \t Train: Acc=49%, Loss=0.6920832991600037 \t\t Validation: Acc=49%, Loss=0.6948763728141785\n",
            "Iteration: 362870 \t Train: Acc=46%, Loss=0.6951994895935059 \t\t Validation: Acc=50%, Loss=0.6969372630119324\n",
            "Iteration: 362880 \t Train: Acc=53%, Loss=0.7005507946014404 \t\t Validation: Acc=50%, Loss=0.6935506463050842\n",
            "Iteration: 362890 \t Train: Acc=53%, Loss=0.6945240497589111 \t\t Validation: Acc=52%, Loss=0.6964908838272095\n",
            "Iteration: 362900 \t Train: Acc=57%, Loss=0.6838059425354004 \t\t Validation: Acc=49%, Loss=0.6948013305664062\n",
            "Iteration: 362910 \t Train: Acc=46%, Loss=0.6955634355545044 \t\t Validation: Acc=50%, Loss=0.6983863115310669\n",
            "Iteration: 362920 \t Train: Acc=48%, Loss=0.7019680738449097 \t\t Validation: Acc=50%, Loss=0.6888858079910278\n",
            "Iteration: 362930 \t Train: Acc=51%, Loss=0.6849632263183594 \t\t Validation: Acc=50%, Loss=0.6870789527893066\n",
            "Iteration: 362940 \t Train: Acc=46%, Loss=0.6981985569000244 \t\t Validation: Acc=46%, Loss=0.7054219245910645\n",
            "Iteration: 362950 \t Train: Acc=49%, Loss=0.6810413599014282 \t\t Validation: Acc=50%, Loss=0.6926106810569763\n",
            "Iteration: 362960 \t Train: Acc=52%, Loss=0.6980135440826416 \t\t Validation: Acc=52%, Loss=0.6854202747344971\n",
            "Iteration: 362970 \t Train: Acc=50%, Loss=0.6893761157989502 \t\t Validation: Acc=51%, Loss=0.6996772885322571\n",
            "Iteration: 362980 \t Train: Acc=55%, Loss=0.6887389421463013 \t\t Validation: Acc=50%, Loss=0.6978731751441956\n",
            "Iteration: 362990 \t Train: Acc=51%, Loss=0.6938639879226685 \t\t Validation: Acc=49%, Loss=0.6892745494842529\n",
            "Iteration: 363000 \t Train: Acc=50%, Loss=0.6947178244590759 \t\t Validation: Acc=49%, Loss=0.6895003318786621\n",
            "Iteration: 363010 \t Train: Acc=50%, Loss=0.693211555480957 \t\t Validation: Acc=53%, Loss=0.6836634874343872\n",
            "Iteration: 363020 \t Train: Acc=46%, Loss=0.6878881454467773 \t\t Validation: Acc=51%, Loss=0.6958892345428467\n",
            "Iteration: 363030 \t Train: Acc=51%, Loss=0.6944068074226379 \t\t Validation: Acc=46%, Loss=0.7000049948692322\n",
            "Iteration: 363040 \t Train: Acc=59%, Loss=0.6825661659240723 \t\t Validation: Acc=48%, Loss=0.7037357091903687\n",
            "Iteration: 363050 \t Train: Acc=50%, Loss=0.6954354047775269 \t\t Validation: Acc=52%, Loss=0.7030831575393677\n",
            "Iteration: 363060 \t Train: Acc=50%, Loss=0.7024257779121399 \t\t Validation: Acc=51%, Loss=0.6956639289855957\n",
            "Iteration: 363070 \t Train: Acc=53%, Loss=0.6874995827674866 \t\t Validation: Acc=48%, Loss=0.705436110496521\n",
            "Iteration: 363080 \t Train: Acc=53%, Loss=0.6925519704818726 \t\t Validation: Acc=48%, Loss=0.6920459270477295\n",
            "Iteration: 363090 \t Train: Acc=51%, Loss=0.6955214142799377 \t\t Validation: Acc=50%, Loss=0.6929777264595032\n",
            "Iteration: 363100 \t Train: Acc=49%, Loss=0.6923426389694214 \t\t Validation: Acc=52%, Loss=0.6988739967346191\n",
            "Iteration: 363110 \t Train: Acc=51%, Loss=0.6871336698532104 \t\t Validation: Acc=52%, Loss=0.6911443471908569\n",
            "Iteration: 363120 \t Train: Acc=49%, Loss=0.6912288069725037 \t\t Validation: Acc=50%, Loss=0.687158465385437\n",
            "Iteration: 363130 \t Train: Acc=50%, Loss=0.6921493411064148 \t\t Validation: Acc=50%, Loss=0.6880780458450317\n",
            "Iteration: 363140 \t Train: Acc=51%, Loss=0.6945552825927734 \t\t Validation: Acc=51%, Loss=0.7044504880905151\n",
            "Iteration: 363150 \t Train: Acc=46%, Loss=0.6959980130195618 \t\t Validation: Acc=51%, Loss=0.6884565353393555\n",
            "Iteration: 363160 \t Train: Acc=48%, Loss=0.6967501044273376 \t\t Validation: Acc=48%, Loss=0.6960452198982239\n",
            "Iteration: 363170 \t Train: Acc=53%, Loss=0.6995053291320801 \t\t Validation: Acc=46%, Loss=0.6906053423881531\n",
            "Iteration: 363180 \t Train: Acc=51%, Loss=0.6909096837043762 \t\t Validation: Acc=52%, Loss=0.6866718530654907\n",
            "Iteration: 363190 \t Train: Acc=50%, Loss=0.6910696029663086 \t\t Validation: Acc=51%, Loss=0.6894726753234863\n",
            "Iteration: 363200 \t Train: Acc=53%, Loss=0.6910450458526611 \t\t Validation: Acc=51%, Loss=0.6931244134902954\n",
            "Iteration: 363210 \t Train: Acc=46%, Loss=0.6897565126419067 \t\t Validation: Acc=49%, Loss=0.69422447681427\n",
            "Iteration: 363220 \t Train: Acc=53%, Loss=0.6872470378875732 \t\t Validation: Acc=54%, Loss=0.6852068901062012\n",
            "Iteration: 363230 \t Train: Acc=47%, Loss=0.6918115615844727 \t\t Validation: Acc=50%, Loss=0.6986552476882935\n",
            "Iteration: 363240 \t Train: Acc=53%, Loss=0.6888333559036255 \t\t Validation: Acc=50%, Loss=0.6923415064811707\n",
            "Iteration: 363250 \t Train: Acc=50%, Loss=0.6905506253242493 \t\t Validation: Acc=53%, Loss=0.6889466047286987\n",
            "Iteration: 363260 \t Train: Acc=49%, Loss=0.6920289397239685 \t\t Validation: Acc=50%, Loss=0.689016580581665\n",
            "Iteration: 363270 \t Train: Acc=57%, Loss=0.6873053312301636 \t\t Validation: Acc=48%, Loss=0.6958808898925781\n",
            "Iteration: 363280 \t Train: Acc=46%, Loss=0.6980171799659729 \t\t Validation: Acc=51%, Loss=0.6937735080718994\n",
            "Iteration: 363290 \t Train: Acc=49%, Loss=0.6984738111495972 \t\t Validation: Acc=49%, Loss=0.6944739818572998\n",
            "Iteration: 363300 \t Train: Acc=51%, Loss=0.6915479898452759 \t\t Validation: Acc=53%, Loss=0.6884339451789856\n",
            "Iteration: 363310 \t Train: Acc=50%, Loss=0.6913379430770874 \t\t Validation: Acc=48%, Loss=0.6989126205444336\n",
            "Iteration: 363320 \t Train: Acc=53%, Loss=0.6941156387329102 \t\t Validation: Acc=45%, Loss=0.6943565607070923\n",
            "Iteration: 363330 \t Train: Acc=53%, Loss=0.6882932782173157 \t\t Validation: Acc=53%, Loss=0.6845676898956299\n",
            "Iteration: 363340 \t Train: Acc=53%, Loss=0.6893424987792969 \t\t Validation: Acc=50%, Loss=0.6969826221466064\n",
            "Iteration: 363350 \t Train: Acc=54%, Loss=0.6840295791625977 \t\t Validation: Acc=48%, Loss=0.698225736618042\n",
            "Iteration: 363360 \t Train: Acc=47%, Loss=0.6899527907371521 \t\t Validation: Acc=52%, Loss=0.6909136772155762\n",
            "Iteration: 363370 \t Train: Acc=52%, Loss=0.6847337484359741 \t\t Validation: Acc=52%, Loss=0.6924226880073547\n",
            "Iteration: 363380 \t Train: Acc=52%, Loss=0.6826735734939575 \t\t Validation: Acc=50%, Loss=0.6890729665756226\n",
            "Iteration: 363390 \t Train: Acc=50%, Loss=0.6865893602371216 \t\t Validation: Acc=49%, Loss=0.6952462792396545\n",
            "Iteration: 363400 \t Train: Acc=53%, Loss=0.6905888915061951 \t\t Validation: Acc=49%, Loss=0.6928331255912781\n",
            "Iteration: 363410 \t Train: Acc=50%, Loss=0.6928601861000061 \t\t Validation: Acc=50%, Loss=0.6979594230651855\n",
            "Iteration: 363420 \t Train: Acc=46%, Loss=0.6948058009147644 \t\t Validation: Acc=50%, Loss=0.6981626749038696\n",
            "Iteration: 363430 \t Train: Acc=56%, Loss=0.6826311349868774 \t\t Validation: Acc=51%, Loss=0.7002642154693604\n",
            "Iteration: 363440 \t Train: Acc=54%, Loss=0.6850461363792419 \t\t Validation: Acc=50%, Loss=0.6961516737937927\n",
            "Iteration: 363450 \t Train: Acc=49%, Loss=0.691857099533081 \t\t Validation: Acc=50%, Loss=0.6951972246170044\n",
            "Iteration: 363460 \t Train: Acc=49%, Loss=0.686932384967804 \t\t Validation: Acc=52%, Loss=0.692349374294281\n",
            "Iteration: 363470 \t Train: Acc=52%, Loss=0.6968494057655334 \t\t Validation: Acc=47%, Loss=0.6869584918022156\n",
            "Iteration: 363480 \t Train: Acc=55%, Loss=0.6849886775016785 \t\t Validation: Acc=49%, Loss=0.6980980038642883\n",
            "Iteration: 363490 \t Train: Acc=48%, Loss=0.6879432201385498 \t\t Validation: Acc=50%, Loss=0.6930804252624512\n",
            "Iteration: 363500 \t Train: Acc=54%, Loss=0.6875438094139099 \t\t Validation: Acc=49%, Loss=0.6944154500961304\n",
            "Iteration: 363510 \t Train: Acc=48%, Loss=0.6957023739814758 \t\t Validation: Acc=51%, Loss=0.690989077091217\n",
            "Iteration: 363520 \t Train: Acc=51%, Loss=0.6958544850349426 \t\t Validation: Acc=50%, Loss=0.6921809911727905\n",
            "Iteration: 363530 \t Train: Acc=51%, Loss=0.6963146924972534 \t\t Validation: Acc=50%, Loss=0.6910253167152405\n",
            "Iteration: 363540 \t Train: Acc=56%, Loss=0.688378095626831 \t\t Validation: Acc=50%, Loss=0.6929095983505249\n",
            "Iteration: 363550 \t Train: Acc=52%, Loss=0.6940202116966248 \t\t Validation: Acc=52%, Loss=0.6890999674797058\n",
            "Iteration: 363560 \t Train: Acc=53%, Loss=0.6876931190490723 \t\t Validation: Acc=52%, Loss=0.6961555480957031\n",
            "Iteration: 363570 \t Train: Acc=60%, Loss=0.6895222663879395 \t\t Validation: Acc=49%, Loss=0.6933349967002869\n",
            "Iteration: 363580 \t Train: Acc=48%, Loss=0.6957228183746338 \t\t Validation: Acc=47%, Loss=0.6954512000083923\n",
            "Iteration: 363590 \t Train: Acc=47%, Loss=0.6949012875556946 \t\t Validation: Acc=50%, Loss=0.6901915073394775\n",
            "Iteration: 363600 \t Train: Acc=54%, Loss=0.6858039498329163 \t\t Validation: Acc=50%, Loss=0.6896894574165344\n",
            "Iteration: 363610 \t Train: Acc=49%, Loss=0.6935877203941345 \t\t Validation: Acc=51%, Loss=0.6912987232208252\n",
            "Iteration: 363620 \t Train: Acc=49%, Loss=0.6963127255439758 \t\t Validation: Acc=51%, Loss=0.6894445419311523\n",
            "Iteration: 363630 \t Train: Acc=46%, Loss=0.6930949687957764 \t\t Validation: Acc=49%, Loss=0.6946536898612976\n",
            "Iteration: 363640 \t Train: Acc=52%, Loss=0.6924975514411926 \t\t Validation: Acc=50%, Loss=0.696097731590271\n",
            "Iteration: 363650 \t Train: Acc=53%, Loss=0.6838155388832092 \t\t Validation: Acc=49%, Loss=0.6944212913513184\n",
            "Iteration: 363660 \t Train: Acc=46%, Loss=0.6954731941223145 \t\t Validation: Acc=53%, Loss=0.6922218799591064\n",
            "Iteration: 363670 \t Train: Acc=56%, Loss=0.6870591640472412 \t\t Validation: Acc=51%, Loss=0.6843503713607788\n",
            "Iteration: 363680 \t Train: Acc=51%, Loss=0.6897233128547668 \t\t Validation: Acc=51%, Loss=0.6904253959655762\n",
            "Iteration: 363690 \t Train: Acc=53%, Loss=0.6913492679595947 \t\t Validation: Acc=50%, Loss=0.6882475018501282\n",
            "Iteration: 363700 \t Train: Acc=53%, Loss=0.6959514021873474 \t\t Validation: Acc=47%, Loss=0.7066097259521484\n",
            "Iteration: 363710 \t Train: Acc=57%, Loss=0.6881346702575684 \t\t Validation: Acc=53%, Loss=0.6916842460632324\n",
            "Iteration: 363720 \t Train: Acc=50%, Loss=0.69163978099823 \t\t Validation: Acc=50%, Loss=0.6925929188728333\n",
            "Iteration: 363730 \t Train: Acc=51%, Loss=0.7004539370536804 \t\t Validation: Acc=50%, Loss=0.6912227869033813\n",
            "Iteration: 363740 \t Train: Acc=53%, Loss=0.6939809322357178 \t\t Validation: Acc=49%, Loss=0.699988603591919\n",
            "Iteration: 363750 \t Train: Acc=54%, Loss=0.6863762736320496 \t\t Validation: Acc=46%, Loss=0.6986584663391113\n",
            "Iteration: 363760 \t Train: Acc=49%, Loss=0.6914972066879272 \t\t Validation: Acc=49%, Loss=0.6961074471473694\n",
            "Iteration: 363770 \t Train: Acc=56%, Loss=0.6863927245140076 \t\t Validation: Acc=52%, Loss=0.6858729124069214\n",
            "Iteration: 363780 \t Train: Acc=55%, Loss=0.6837373375892639 \t\t Validation: Acc=51%, Loss=0.6884989142417908\n",
            "Iteration: 363790 \t Train: Acc=53%, Loss=0.6865277886390686 \t\t Validation: Acc=51%, Loss=0.6892513036727905\n",
            "Iteration: 363800 \t Train: Acc=46%, Loss=0.6982612609863281 \t\t Validation: Acc=50%, Loss=0.6897264719009399\n",
            "Iteration: 363810 \t Train: Acc=42%, Loss=0.7084943652153015 \t\t Validation: Acc=50%, Loss=0.6860936284065247\n",
            "Iteration: 363820 \t Train: Acc=56%, Loss=0.6886025667190552 \t\t Validation: Acc=51%, Loss=0.6812531352043152\n",
            "Iteration: 363830 \t Train: Acc=48%, Loss=0.6944040656089783 \t\t Validation: Acc=50%, Loss=0.6922578811645508\n",
            "Iteration: 363840 \t Train: Acc=50%, Loss=0.69415283203125 \t\t Validation: Acc=51%, Loss=0.6869087219238281\n",
            "Iteration: 363850 \t Train: Acc=51%, Loss=0.6849709749221802 \t\t Validation: Acc=53%, Loss=0.692521870136261\n",
            "Iteration: 363860 \t Train: Acc=55%, Loss=0.6831651926040649 \t\t Validation: Acc=49%, Loss=0.690951943397522\n",
            "Iteration: 363870 \t Train: Acc=50%, Loss=0.6918544769287109 \t\t Validation: Acc=51%, Loss=0.6934188008308411\n",
            "Iteration: 363880 \t Train: Acc=57%, Loss=0.6879048347473145 \t\t Validation: Acc=50%, Loss=0.6965492367744446\n",
            "Iteration: 363890 \t Train: Acc=46%, Loss=0.6943246126174927 \t\t Validation: Acc=50%, Loss=0.6953524351119995\n",
            "Iteration: 363900 \t Train: Acc=47%, Loss=0.6920745372772217 \t\t Validation: Acc=50%, Loss=0.6929218173027039\n",
            "Iteration: 363910 \t Train: Acc=53%, Loss=0.6911125183105469 \t\t Validation: Acc=51%, Loss=0.6950631141662598\n",
            "Iteration: 363920 \t Train: Acc=52%, Loss=0.6925487518310547 \t\t Validation: Acc=51%, Loss=0.6855553984642029\n",
            "Iteration: 363930 \t Train: Acc=49%, Loss=0.6960365772247314 \t\t Validation: Acc=48%, Loss=0.6900547742843628\n",
            "Iteration: 363940 \t Train: Acc=53%, Loss=0.6898714303970337 \t\t Validation: Acc=46%, Loss=0.6954308748245239\n",
            "Iteration: 363950 \t Train: Acc=48%, Loss=0.6868677139282227 \t\t Validation: Acc=49%, Loss=0.7031123042106628\n",
            "Iteration: 363960 \t Train: Acc=50%, Loss=0.6925514936447144 \t\t Validation: Acc=50%, Loss=0.6896725296974182\n",
            "Iteration: 363970 \t Train: Acc=52%, Loss=0.6894939541816711 \t\t Validation: Acc=50%, Loss=0.6964067816734314\n",
            "Iteration: 363980 \t Train: Acc=53%, Loss=0.6861556172370911 \t\t Validation: Acc=49%, Loss=0.6918160915374756\n",
            "Iteration: 363990 \t Train: Acc=49%, Loss=0.6941012740135193 \t\t Validation: Acc=49%, Loss=0.6943703293800354\n",
            "Iteration: 364000 \t Train: Acc=52%, Loss=0.6880297660827637 \t\t Validation: Acc=51%, Loss=0.6936850547790527\n",
            "Iteration: 364010 \t Train: Acc=51%, Loss=0.6936602592468262 \t\t Validation: Acc=50%, Loss=0.6914955973625183\n",
            "Iteration: 364020 \t Train: Acc=50%, Loss=0.6928227543830872 \t\t Validation: Acc=50%, Loss=0.6926609873771667\n",
            "Iteration: 364030 \t Train: Acc=56%, Loss=0.6915130615234375 \t\t Validation: Acc=50%, Loss=0.6962703466415405\n",
            "Iteration: 364040 \t Train: Acc=47%, Loss=0.6950016021728516 \t\t Validation: Acc=49%, Loss=0.6932573318481445\n",
            "Iteration: 364050 \t Train: Acc=48%, Loss=0.6950250864028931 \t\t Validation: Acc=50%, Loss=0.6927759647369385\n",
            "Iteration: 364060 \t Train: Acc=56%, Loss=0.6893071532249451 \t\t Validation: Acc=48%, Loss=0.6930966377258301\n",
            "Iteration: 364070 \t Train: Acc=50%, Loss=0.6932820677757263 \t\t Validation: Acc=50%, Loss=0.6893141269683838\n",
            "Iteration: 364080 \t Train: Acc=50%, Loss=0.6927367448806763 \t\t Validation: Acc=51%, Loss=0.6903256177902222\n",
            "Iteration: 364090 \t Train: Acc=48%, Loss=0.6946065425872803 \t\t Validation: Acc=54%, Loss=0.6914476752281189\n",
            "Iteration: 364100 \t Train: Acc=49%, Loss=0.6898031234741211 \t\t Validation: Acc=49%, Loss=0.6946887969970703\n",
            "Iteration: 364110 \t Train: Acc=50%, Loss=0.6923588514328003 \t\t Validation: Acc=50%, Loss=0.6911324262619019\n",
            "Iteration: 364120 \t Train: Acc=54%, Loss=0.6968411803245544 \t\t Validation: Acc=51%, Loss=0.6901693344116211\n",
            "Iteration: 364130 \t Train: Acc=50%, Loss=0.6963990330696106 \t\t Validation: Acc=48%, Loss=0.6964452266693115\n",
            "Iteration: 364140 \t Train: Acc=50%, Loss=0.690555214881897 \t\t Validation: Acc=47%, Loss=0.6947453022003174\n",
            "Iteration: 364150 \t Train: Acc=50%, Loss=0.7010879516601562 \t\t Validation: Acc=51%, Loss=0.6991925239562988\n",
            "Iteration: 364160 \t Train: Acc=45%, Loss=0.7037500739097595 \t\t Validation: Acc=48%, Loss=0.6970292329788208\n",
            "Iteration: 364170 \t Train: Acc=56%, Loss=0.6869176626205444 \t\t Validation: Acc=50%, Loss=0.6925415396690369\n",
            "Iteration: 364180 \t Train: Acc=53%, Loss=0.6863886713981628 \t\t Validation: Acc=49%, Loss=0.6879002451896667\n",
            "Iteration: 364190 \t Train: Acc=52%, Loss=0.6902740001678467 \t\t Validation: Acc=52%, Loss=0.691245973110199\n",
            "Iteration: 364200 \t Train: Acc=57%, Loss=0.6878394484519958 \t\t Validation: Acc=52%, Loss=0.6918997168540955\n",
            "Iteration: 364210 \t Train: Acc=58%, Loss=0.6856897473335266 \t\t Validation: Acc=49%, Loss=0.6898289322853088\n",
            "Iteration: 364220 \t Train: Acc=48%, Loss=0.6950845718383789 \t\t Validation: Acc=50%, Loss=0.6925311088562012\n",
            "Iteration: 364230 \t Train: Acc=52%, Loss=0.6985905170440674 \t\t Validation: Acc=50%, Loss=0.692115843296051\n",
            "Iteration: 364240 \t Train: Acc=50%, Loss=0.6948212385177612 \t\t Validation: Acc=46%, Loss=0.6901150345802307\n",
            "Iteration: 364250 \t Train: Acc=54%, Loss=0.6865483522415161 \t\t Validation: Acc=53%, Loss=0.6911333799362183\n",
            "Iteration: 364260 \t Train: Acc=48%, Loss=0.6948732137680054 \t\t Validation: Acc=53%, Loss=0.6973581910133362\n",
            "Iteration: 364270 \t Train: Acc=51%, Loss=0.6981630921363831 \t\t Validation: Acc=52%, Loss=0.6981815695762634\n",
            "Iteration: 364280 \t Train: Acc=54%, Loss=0.6835184097290039 \t\t Validation: Acc=53%, Loss=0.693243145942688\n",
            "Iteration: 364290 \t Train: Acc=52%, Loss=0.6921816468238831 \t\t Validation: Acc=50%, Loss=0.6874016523361206\n",
            "Iteration: 364300 \t Train: Acc=46%, Loss=0.6971331238746643 \t\t Validation: Acc=50%, Loss=0.6960324048995972\n",
            "Iteration: 364310 \t Train: Acc=53%, Loss=0.6870341897010803 \t\t Validation: Acc=50%, Loss=0.6938493251800537\n",
            "Iteration: 364320 \t Train: Acc=54%, Loss=0.6895124316215515 \t\t Validation: Acc=50%, Loss=0.7006337642669678\n",
            "Iteration: 364330 \t Train: Acc=58%, Loss=0.6865562796592712 \t\t Validation: Acc=51%, Loss=0.6945385336875916\n",
            "Iteration: 364340 \t Train: Acc=42%, Loss=0.7006615400314331 \t\t Validation: Acc=51%, Loss=0.6942753791809082\n",
            "Iteration: 364350 \t Train: Acc=42%, Loss=0.6991583704948425 \t\t Validation: Acc=50%, Loss=0.6906647682189941\n",
            "Iteration: 364360 \t Train: Acc=51%, Loss=0.6832854151725769 \t\t Validation: Acc=51%, Loss=0.6953708529472351\n",
            "Iteration: 364370 \t Train: Acc=53%, Loss=0.6851891279220581 \t\t Validation: Acc=48%, Loss=0.6952701807022095\n",
            "Iteration: 364380 \t Train: Acc=63%, Loss=0.6845719218254089 \t\t Validation: Acc=48%, Loss=0.6924923658370972\n",
            "Iteration: 364390 \t Train: Acc=44%, Loss=0.7046512961387634 \t\t Validation: Acc=48%, Loss=0.6930326819419861\n",
            "Iteration: 364400 \t Train: Acc=53%, Loss=0.6912808418273926 \t\t Validation: Acc=48%, Loss=0.6949431300163269\n",
            "Iteration: 364410 \t Train: Acc=53%, Loss=0.6906673908233643 \t\t Validation: Acc=49%, Loss=0.691146731376648\n",
            "Iteration: 364420 \t Train: Acc=53%, Loss=0.6910451054573059 \t\t Validation: Acc=49%, Loss=0.6976448893547058\n",
            "Iteration: 364430 \t Train: Acc=46%, Loss=0.695159912109375 \t\t Validation: Acc=50%, Loss=0.6902565360069275\n",
            "Iteration: 364440 \t Train: Acc=50%, Loss=0.6875360012054443 \t\t Validation: Acc=51%, Loss=0.6963631510734558\n",
            "Iteration: 364450 \t Train: Acc=52%, Loss=0.6925929188728333 \t\t Validation: Acc=50%, Loss=0.7002564668655396\n",
            "Iteration: 364460 \t Train: Acc=54%, Loss=0.6903524994850159 \t\t Validation: Acc=46%, Loss=0.6935276985168457\n",
            "Iteration: 364470 \t Train: Acc=48%, Loss=0.6949198246002197 \t\t Validation: Acc=55%, Loss=0.6858285069465637\n",
            "Iteration: 364480 \t Train: Acc=53%, Loss=0.6952322721481323 \t\t Validation: Acc=50%, Loss=0.6921009421348572\n",
            "Iteration: 364490 \t Train: Acc=55%, Loss=0.6901978850364685 \t\t Validation: Acc=50%, Loss=0.6951336860656738\n",
            "Iteration: 364500 \t Train: Acc=50%, Loss=0.693137526512146 \t\t Validation: Acc=50%, Loss=0.6928462386131287\n",
            "Iteration: 364510 \t Train: Acc=49%, Loss=0.6890531778335571 \t\t Validation: Acc=47%, Loss=0.7020184993743896\n",
            "Iteration: 364520 \t Train: Acc=54%, Loss=0.6931727528572083 \t\t Validation: Acc=47%, Loss=0.6912832260131836\n",
            "Iteration: 364530 \t Train: Acc=54%, Loss=0.6807712912559509 \t\t Validation: Acc=45%, Loss=0.6931841373443604\n",
            "Iteration: 364540 \t Train: Acc=53%, Loss=0.6864920854568481 \t\t Validation: Acc=52%, Loss=0.6874911785125732\n",
            "Iteration: 364550 \t Train: Acc=48%, Loss=0.6967480182647705 \t\t Validation: Acc=51%, Loss=0.6923530101776123\n",
            "Iteration: 364560 \t Train: Acc=58%, Loss=0.6863217353820801 \t\t Validation: Acc=51%, Loss=0.6918731331825256\n",
            "Iteration: 364570 \t Train: Acc=53%, Loss=0.6913183331489563 \t\t Validation: Acc=49%, Loss=0.6907894015312195\n",
            "Iteration: 364580 \t Train: Acc=43%, Loss=0.6991330981254578 \t\t Validation: Acc=50%, Loss=0.6889572739601135\n",
            "Iteration: 364590 \t Train: Acc=50%, Loss=0.690891683101654 \t\t Validation: Acc=48%, Loss=0.699657678604126\n",
            "Iteration: 364600 \t Train: Acc=53%, Loss=0.6882569789886475 \t\t Validation: Acc=49%, Loss=0.687267541885376\n",
            "Iteration: 364610 \t Train: Acc=42%, Loss=0.7009763717651367 \t\t Validation: Acc=48%, Loss=0.6955656409263611\n",
            "Iteration: 364620 \t Train: Acc=51%, Loss=0.6935351490974426 \t\t Validation: Acc=50%, Loss=0.6960088014602661\n",
            "Iteration: 364630 \t Train: Acc=46%, Loss=0.6920864582061768 \t\t Validation: Acc=50%, Loss=0.6939332485198975\n",
            "Iteration: 364640 \t Train: Acc=53%, Loss=0.6884245276451111 \t\t Validation: Acc=54%, Loss=0.6901305317878723\n",
            "Iteration: 364650 \t Train: Acc=46%, Loss=0.6968307495117188 \t\t Validation: Acc=42%, Loss=0.6916760802268982\n",
            "Iteration: 364660 \t Train: Acc=53%, Loss=0.6885591745376587 \t\t Validation: Acc=50%, Loss=0.6951820850372314\n",
            "Iteration: 364670 \t Train: Acc=53%, Loss=0.6877508163452148 \t\t Validation: Acc=50%, Loss=0.6855997443199158\n",
            "Iteration: 364680 \t Train: Acc=57%, Loss=0.6849038004875183 \t\t Validation: Acc=50%, Loss=0.6928660869598389\n",
            "Iteration: 364690 \t Train: Acc=49%, Loss=0.6895046234130859 \t\t Validation: Acc=55%, Loss=0.6889035701751709\n",
            "Iteration: 364700 \t Train: Acc=46%, Loss=0.6924788355827332 \t\t Validation: Acc=52%, Loss=0.6921626925468445\n",
            "Iteration: 364710 \t Train: Acc=52%, Loss=0.6948612928390503 \t\t Validation: Acc=51%, Loss=0.6989177465438843\n",
            "Iteration: 364720 \t Train: Acc=55%, Loss=0.6859716176986694 \t\t Validation: Acc=52%, Loss=0.688750684261322\n",
            "Iteration: 364730 \t Train: Acc=52%, Loss=0.6926134824752808 \t\t Validation: Acc=49%, Loss=0.6924586892127991\n",
            "Iteration: 364740 \t Train: Acc=55%, Loss=0.6997553706169128 \t\t Validation: Acc=49%, Loss=0.6928290128707886\n",
            "Iteration: 364750 \t Train: Acc=50%, Loss=0.6860374808311462 \t\t Validation: Acc=53%, Loss=0.6879139542579651\n",
            "Iteration: 364760 \t Train: Acc=57%, Loss=0.6905586123466492 \t\t Validation: Acc=50%, Loss=0.6941381692886353\n",
            "Iteration: 364770 \t Train: Acc=53%, Loss=0.6907867789268494 \t\t Validation: Acc=53%, Loss=0.692440390586853\n",
            "Iteration: 364780 \t Train: Acc=52%, Loss=0.6944591403007507 \t\t Validation: Acc=48%, Loss=0.6946614980697632\n",
            "Iteration: 364790 \t Train: Acc=48%, Loss=0.6880083084106445 \t\t Validation: Acc=50%, Loss=0.6914123296737671\n",
            "Iteration: 364800 \t Train: Acc=46%, Loss=0.694490909576416 \t\t Validation: Acc=52%, Loss=0.6955529451370239\n",
            "Iteration: 364810 \t Train: Acc=49%, Loss=0.6880760788917542 \t\t Validation: Acc=46%, Loss=0.6946178078651428\n",
            "Iteration: 364820 \t Train: Acc=46%, Loss=0.6960023045539856 \t\t Validation: Acc=47%, Loss=0.6943211555480957\n",
            "Iteration: 364830 \t Train: Acc=50%, Loss=0.700157642364502 \t\t Validation: Acc=49%, Loss=0.69553542137146\n",
            "Iteration: 364840 \t Train: Acc=48%, Loss=0.6924389004707336 \t\t Validation: Acc=50%, Loss=0.6932085752487183\n",
            "Iteration: 364850 \t Train: Acc=51%, Loss=0.6937573552131653 \t\t Validation: Acc=55%, Loss=0.6929272413253784\n",
            "Iteration: 364860 \t Train: Acc=50%, Loss=0.6939984560012817 \t\t Validation: Acc=52%, Loss=0.6961302161216736\n",
            "Iteration: 364870 \t Train: Acc=50%, Loss=0.6914415955543518 \t\t Validation: Acc=52%, Loss=0.6867268681526184\n",
            "Iteration: 364880 \t Train: Acc=48%, Loss=0.6903875470161438 \t\t Validation: Acc=48%, Loss=0.6928663849830627\n",
            "Iteration: 364890 \t Train: Acc=53%, Loss=0.6862466931343079 \t\t Validation: Acc=51%, Loss=0.69490647315979\n",
            "Iteration: 364900 \t Train: Acc=52%, Loss=0.6869049072265625 \t\t Validation: Acc=50%, Loss=0.695731520652771\n",
            "Iteration: 364910 \t Train: Acc=49%, Loss=0.6945125460624695 \t\t Validation: Acc=50%, Loss=0.6921697854995728\n",
            "Iteration: 364920 \t Train: Acc=49%, Loss=0.6925755739212036 \t\t Validation: Acc=51%, Loss=0.6917661428451538\n",
            "Iteration: 364930 \t Train: Acc=51%, Loss=0.6953088045120239 \t\t Validation: Acc=48%, Loss=0.6952089071273804\n",
            "Iteration: 364940 \t Train: Acc=50%, Loss=0.6927493214607239 \t\t Validation: Acc=51%, Loss=0.689344584941864\n",
            "Iteration: 364950 \t Train: Acc=50%, Loss=0.692594587802887 \t\t Validation: Acc=52%, Loss=0.6947541236877441\n",
            "Iteration: 364960 \t Train: Acc=46%, Loss=0.6935846209526062 \t\t Validation: Acc=48%, Loss=0.6924671530723572\n",
            "Iteration: 364970 \t Train: Acc=50%, Loss=0.6894974708557129 \t\t Validation: Acc=51%, Loss=0.690184473991394\n",
            "Iteration: 364980 \t Train: Acc=50%, Loss=0.6935669183731079 \t\t Validation: Acc=51%, Loss=0.6882411241531372\n",
            "Iteration: 364990 \t Train: Acc=53%, Loss=0.687498152256012 \t\t Validation: Acc=50%, Loss=0.6984858512878418\n",
            "Iteration: 365000 \t Train: Acc=46%, Loss=0.6964157223701477 \t\t Validation: Acc=52%, Loss=0.6866179704666138\n",
            "Iteration: 365010 \t Train: Acc=46%, Loss=0.6917137503623962 \t\t Validation: Acc=51%, Loss=0.6925222277641296\n",
            "Iteration: 365020 \t Train: Acc=55%, Loss=0.6887041926383972 \t\t Validation: Acc=51%, Loss=0.6864199638366699\n",
            "Iteration: 365030 \t Train: Acc=50%, Loss=0.6926043033599854 \t\t Validation: Acc=48%, Loss=0.6940885186195374\n",
            "Iteration: 365040 \t Train: Acc=50%, Loss=0.6920388340950012 \t\t Validation: Acc=50%, Loss=0.6928436756134033\n",
            "Iteration: 365050 \t Train: Acc=47%, Loss=0.691891074180603 \t\t Validation: Acc=52%, Loss=0.6932483911514282\n",
            "Iteration: 365060 \t Train: Acc=52%, Loss=0.6889756917953491 \t\t Validation: Acc=46%, Loss=0.6915706992149353\n",
            "Iteration: 365070 \t Train: Acc=48%, Loss=0.6916335225105286 \t\t Validation: Acc=49%, Loss=0.6978468298912048\n",
            "Iteration: 365080 \t Train: Acc=53%, Loss=0.6885693669319153 \t\t Validation: Acc=46%, Loss=0.7016717195510864\n",
            "Iteration: 365090 \t Train: Acc=46%, Loss=0.695675253868103 \t\t Validation: Acc=46%, Loss=0.6933797001838684\n",
            "Iteration: 365100 \t Train: Acc=53%, Loss=0.6862510442733765 \t\t Validation: Acc=52%, Loss=0.6999326944351196\n",
            "Iteration: 365110 \t Train: Acc=53%, Loss=0.6921266317367554 \t\t Validation: Acc=50%, Loss=0.690161406993866\n",
            "Iteration: 365120 \t Train: Acc=54%, Loss=0.6918577551841736 \t\t Validation: Acc=50%, Loss=0.6900733709335327\n",
            "Iteration: 365130 \t Train: Acc=57%, Loss=0.6870629191398621 \t\t Validation: Acc=47%, Loss=0.697830855846405\n",
            "Iteration: 365140 \t Train: Acc=53%, Loss=0.6911801695823669 \t\t Validation: Acc=53%, Loss=0.692132294178009\n",
            "Iteration: 365150 \t Train: Acc=53%, Loss=0.6942421197891235 \t\t Validation: Acc=50%, Loss=0.6935487389564514\n",
            "Iteration: 365160 \t Train: Acc=50%, Loss=0.6991313099861145 \t\t Validation: Acc=50%, Loss=0.696395218372345\n",
            "Iteration: 365170 \t Train: Acc=46%, Loss=0.6953094005584717 \t\t Validation: Acc=48%, Loss=0.6928882002830505\n",
            "Iteration: 365180 \t Train: Acc=55%, Loss=0.6864819526672363 \t\t Validation: Acc=44%, Loss=0.6909952759742737\n",
            "Iteration: 365190 \t Train: Acc=53%, Loss=0.6870676875114441 \t\t Validation: Acc=56%, Loss=0.6870205998420715\n",
            "Iteration: 365200 \t Train: Acc=53%, Loss=0.6886805295944214 \t\t Validation: Acc=50%, Loss=0.6875188946723938\n",
            "Iteration: 365210 \t Train: Acc=49%, Loss=0.6895488500595093 \t\t Validation: Acc=48%, Loss=0.6969201564788818\n",
            "Iteration: 365220 \t Train: Acc=48%, Loss=0.6930053234100342 \t\t Validation: Acc=52%, Loss=0.6847893595695496\n",
            "Iteration: 365230 \t Train: Acc=57%, Loss=0.685669481754303 \t\t Validation: Acc=53%, Loss=0.6925148963928223\n",
            "Iteration: 365240 \t Train: Acc=52%, Loss=0.6895855665206909 \t\t Validation: Acc=46%, Loss=0.6957045793533325\n",
            "Iteration: 365250 \t Train: Acc=50%, Loss=0.6938375234603882 \t\t Validation: Acc=50%, Loss=0.6956406831741333\n",
            "Iteration: 365260 \t Train: Acc=48%, Loss=0.6956711411476135 \t\t Validation: Acc=49%, Loss=0.6979408860206604\n",
            "Iteration: 365270 \t Train: Acc=60%, Loss=0.6837216019630432 \t\t Validation: Acc=48%, Loss=0.6899337768554688\n",
            "Iteration: 365280 \t Train: Acc=47%, Loss=0.691767692565918 \t\t Validation: Acc=52%, Loss=0.6907069087028503\n",
            "Iteration: 365290 \t Train: Acc=55%, Loss=0.6755021810531616 \t\t Validation: Acc=53%, Loss=0.6875118613243103\n",
            "Iteration: 365300 \t Train: Acc=46%, Loss=0.6983829140663147 \t\t Validation: Acc=52%, Loss=0.6905764937400818\n",
            "Iteration: 365310 \t Train: Acc=50%, Loss=0.6897061467170715 \t\t Validation: Acc=52%, Loss=0.6896576881408691\n",
            "Iteration: 365320 \t Train: Acc=47%, Loss=0.690428614616394 \t\t Validation: Acc=50%, Loss=0.6945348381996155\n",
            "Iteration: 365330 \t Train: Acc=50%, Loss=0.6867541670799255 \t\t Validation: Acc=53%, Loss=0.6965251564979553\n",
            "Iteration: 365340 \t Train: Acc=49%, Loss=0.6871175765991211 \t\t Validation: Acc=51%, Loss=0.6885982155799866\n",
            "Iteration: 365350 \t Train: Acc=49%, Loss=0.69087815284729 \t\t Validation: Acc=49%, Loss=0.6979909539222717\n",
            "Iteration: 365360 \t Train: Acc=48%, Loss=0.6940831542015076 \t\t Validation: Acc=50%, Loss=0.6959993839263916\n",
            "Iteration: 365370 \t Train: Acc=49%, Loss=0.6973251104354858 \t\t Validation: Acc=46%, Loss=0.694668710231781\n",
            "Iteration: 365380 \t Train: Acc=48%, Loss=0.699882984161377 \t\t Validation: Acc=56%, Loss=0.694983720779419\n",
            "Iteration: 365390 \t Train: Acc=50%, Loss=0.6912071704864502 \t\t Validation: Acc=50%, Loss=0.7008020281791687\n",
            "Iteration: 365400 \t Train: Acc=53%, Loss=0.6912218332290649 \t\t Validation: Acc=49%, Loss=0.6888515949249268\n",
            "Iteration: 365410 \t Train: Acc=48%, Loss=0.695715069770813 \t\t Validation: Acc=45%, Loss=0.7006052732467651\n",
            "Iteration: 365420 \t Train: Acc=45%, Loss=0.6984259486198425 \t\t Validation: Acc=49%, Loss=0.6948493719100952\n",
            "Iteration: 365430 \t Train: Acc=51%, Loss=0.690911591053009 \t\t Validation: Acc=50%, Loss=0.6967218518257141\n",
            "Iteration: 365440 \t Train: Acc=51%, Loss=0.6894239187240601 \t\t Validation: Acc=50%, Loss=0.6893337368965149\n",
            "Iteration: 365450 \t Train: Acc=50%, Loss=0.6880797743797302 \t\t Validation: Acc=53%, Loss=0.6889038681983948\n",
            "Iteration: 365460 \t Train: Acc=49%, Loss=0.6938121318817139 \t\t Validation: Acc=50%, Loss=0.6978233456611633\n",
            "Iteration: 365470 \t Train: Acc=55%, Loss=0.6863141655921936 \t\t Validation: Acc=53%, Loss=0.6917942762374878\n",
            "Iteration: 365480 \t Train: Acc=53%, Loss=0.6951683759689331 \t\t Validation: Acc=53%, Loss=0.6897193789482117\n",
            "Iteration: 365490 \t Train: Acc=48%, Loss=0.6924373507499695 \t\t Validation: Acc=52%, Loss=0.6902868747711182\n",
            "Iteration: 365500 \t Train: Acc=49%, Loss=0.6951170563697815 \t\t Validation: Acc=50%, Loss=0.6918637156486511\n",
            "Iteration: 365510 \t Train: Acc=50%, Loss=0.6851760745048523 \t\t Validation: Acc=46%, Loss=0.6852865219116211\n",
            "Iteration: 365520 \t Train: Acc=46%, Loss=0.6890819072723389 \t\t Validation: Acc=52%, Loss=0.6974305510520935\n",
            "Iteration: 365530 \t Train: Acc=50%, Loss=0.6906567811965942 \t\t Validation: Acc=50%, Loss=0.6890251040458679\n",
            "Iteration: 365540 \t Train: Acc=59%, Loss=0.6865917444229126 \t\t Validation: Acc=50%, Loss=0.6929464340209961\n",
            "Iteration: 365550 \t Train: Acc=54%, Loss=0.6956696510314941 \t\t Validation: Acc=53%, Loss=0.6893221735954285\n",
            "Iteration: 365560 \t Train: Acc=50%, Loss=0.6936224699020386 \t\t Validation: Acc=50%, Loss=0.6910187005996704\n",
            "Iteration: 365570 \t Train: Acc=56%, Loss=0.6845564246177673 \t\t Validation: Acc=52%, Loss=0.692181408405304\n",
            "Iteration: 365580 \t Train: Acc=47%, Loss=0.6930638551712036 \t\t Validation: Acc=52%, Loss=0.6934994459152222\n",
            "Iteration: 365590 \t Train: Acc=45%, Loss=0.6969049572944641 \t\t Validation: Acc=53%, Loss=0.6969850063323975\n",
            "Iteration: 365600 \t Train: Acc=50%, Loss=0.6958216428756714 \t\t Validation: Acc=50%, Loss=0.6940253376960754\n",
            "Iteration: 365610 \t Train: Acc=59%, Loss=0.680725634098053 \t\t Validation: Acc=51%, Loss=0.6911759376525879\n",
            "Iteration: 365620 \t Train: Acc=53%, Loss=0.6861937046051025 \t\t Validation: Acc=49%, Loss=0.6904942989349365\n",
            "Iteration: 365630 \t Train: Acc=54%, Loss=0.6875657439231873 \t\t Validation: Acc=50%, Loss=0.6937965750694275\n",
            "Iteration: 365640 \t Train: Acc=50%, Loss=0.6913066506385803 \t\t Validation: Acc=50%, Loss=0.7002487182617188\n",
            "Iteration: 365650 \t Train: Acc=57%, Loss=0.6831808686256409 \t\t Validation: Acc=51%, Loss=0.6941515207290649\n",
            "Iteration: 365660 \t Train: Acc=47%, Loss=0.6888425946235657 \t\t Validation: Acc=48%, Loss=0.6870818734169006\n",
            "Iteration: 365670 \t Train: Acc=50%, Loss=0.6964865922927856 \t\t Validation: Acc=50%, Loss=0.6844373941421509\n",
            "Iteration: 365680 \t Train: Acc=42%, Loss=0.7038964629173279 \t\t Validation: Acc=51%, Loss=0.6865600347518921\n",
            "Iteration: 365690 \t Train: Acc=46%, Loss=0.6892592906951904 \t\t Validation: Acc=47%, Loss=0.6959148645401001\n",
            "Iteration: 365700 \t Train: Acc=47%, Loss=0.6982909440994263 \t\t Validation: Acc=50%, Loss=0.6915118098258972\n",
            "Iteration: 365710 \t Train: Acc=52%, Loss=0.6974334716796875 \t\t Validation: Acc=50%, Loss=0.6877087354660034\n",
            "Iteration: 365720 \t Train: Acc=52%, Loss=0.688660204410553 \t\t Validation: Acc=50%, Loss=0.6909849643707275\n",
            "Iteration: 365730 \t Train: Acc=50%, Loss=0.6920796632766724 \t\t Validation: Acc=47%, Loss=0.6950289607048035\n",
            "Iteration: 365740 \t Train: Acc=46%, Loss=0.6954630613327026 \t\t Validation: Acc=49%, Loss=0.6930168867111206\n",
            "Iteration: 365750 \t Train: Acc=46%, Loss=0.6970099806785583 \t\t Validation: Acc=53%, Loss=0.6864854097366333\n",
            "Iteration: 365760 \t Train: Acc=56%, Loss=0.6850114464759827 \t\t Validation: Acc=53%, Loss=0.6898960471153259\n",
            "Iteration: 365770 \t Train: Acc=49%, Loss=0.6968620419502258 \t\t Validation: Acc=49%, Loss=0.6946244835853577\n",
            "Iteration: 365780 \t Train: Acc=55%, Loss=0.6916075348854065 \t\t Validation: Acc=53%, Loss=0.6900819540023804\n",
            "Iteration: 365790 \t Train: Acc=50%, Loss=0.6891093254089355 \t\t Validation: Acc=57%, Loss=0.6898223757743835\n",
            "Iteration: 365800 \t Train: Acc=54%, Loss=0.6810141801834106 \t\t Validation: Acc=50%, Loss=0.7024013996124268\n",
            "Iteration: 365810 \t Train: Acc=53%, Loss=0.6862088441848755 \t\t Validation: Acc=49%, Loss=0.691972017288208\n",
            "Iteration: 365820 \t Train: Acc=52%, Loss=0.6889223456382751 \t\t Validation: Acc=48%, Loss=0.6959916353225708\n",
            "Iteration: 365830 \t Train: Acc=45%, Loss=0.6964244842529297 \t\t Validation: Acc=47%, Loss=0.6953026056289673\n",
            "Iteration: 365840 \t Train: Acc=49%, Loss=0.6975823640823364 \t\t Validation: Acc=49%, Loss=0.6958706378936768\n",
            "Iteration: 365850 \t Train: Acc=50%, Loss=0.6963649392127991 \t\t Validation: Acc=54%, Loss=0.6915585398674011\n",
            "Iteration: 365860 \t Train: Acc=50%, Loss=0.6911934614181519 \t\t Validation: Acc=51%, Loss=0.6907498836517334\n",
            "Iteration: 365870 \t Train: Acc=51%, Loss=0.6902902722358704 \t\t Validation: Acc=48%, Loss=0.6971073150634766\n",
            "Iteration: 365880 \t Train: Acc=52%, Loss=0.6912499070167542 \t\t Validation: Acc=49%, Loss=0.6865216493606567\n",
            "Iteration: 365890 \t Train: Acc=49%, Loss=0.7003446221351624 \t\t Validation: Acc=52%, Loss=0.6911150217056274\n",
            "Iteration: 365900 \t Train: Acc=56%, Loss=0.6864870190620422 \t\t Validation: Acc=46%, Loss=0.7012267112731934\n",
            "Iteration: 365910 \t Train: Acc=52%, Loss=0.6870346665382385 \t\t Validation: Acc=49%, Loss=0.6897539496421814\n",
            "Iteration: 365920 \t Train: Acc=50%, Loss=0.6924049854278564 \t\t Validation: Acc=48%, Loss=0.6979296803474426\n",
            "Iteration: 365930 \t Train: Acc=54%, Loss=0.6875147819519043 \t\t Validation: Acc=50%, Loss=0.6952437162399292\n",
            "Iteration: 365940 \t Train: Acc=54%, Loss=0.6902210712432861 \t\t Validation: Acc=53%, Loss=0.6993671655654907\n",
            "Iteration: 365950 \t Train: Acc=54%, Loss=0.687340259552002 \t\t Validation: Acc=53%, Loss=0.6881621479988098\n",
            "Iteration: 365960 \t Train: Acc=49%, Loss=0.6918374300003052 \t\t Validation: Acc=50%, Loss=0.6874666213989258\n",
            "Iteration: 365970 \t Train: Acc=57%, Loss=0.6909908056259155 \t\t Validation: Acc=50%, Loss=0.6963728666305542\n",
            "Iteration: 365980 \t Train: Acc=50%, Loss=0.6900057792663574 \t\t Validation: Acc=51%, Loss=0.6813791990280151\n",
            "Iteration: 365990 \t Train: Acc=49%, Loss=0.6987569332122803 \t\t Validation: Acc=49%, Loss=0.6915252804756165\n",
            "Iteration: 366000 \t Train: Acc=51%, Loss=0.6881036162376404 \t\t Validation: Acc=50%, Loss=0.6890062689781189\n",
            "Iteration: 366010 \t Train: Acc=47%, Loss=0.6898592710494995 \t\t Validation: Acc=51%, Loss=0.6947663426399231\n",
            "Iteration: 366020 \t Train: Acc=49%, Loss=0.6932750940322876 \t\t Validation: Acc=49%, Loss=0.6964144706726074\n",
            "Iteration: 366030 \t Train: Acc=50%, Loss=0.6953017711639404 \t\t Validation: Acc=48%, Loss=0.6972153186798096\n",
            "Iteration: 366040 \t Train: Acc=55%, Loss=0.6897746324539185 \t\t Validation: Acc=50%, Loss=0.6905480623245239\n",
            "Iteration: 366050 \t Train: Acc=52%, Loss=0.6905220150947571 \t\t Validation: Acc=50%, Loss=0.691441535949707\n",
            "Iteration: 366060 \t Train: Acc=51%, Loss=0.691717803478241 \t\t Validation: Acc=50%, Loss=0.6896268129348755\n",
            "Iteration: 366070 \t Train: Acc=50%, Loss=0.6923296451568604 \t\t Validation: Acc=50%, Loss=0.6913284063339233\n",
            "Iteration: 366080 \t Train: Acc=52%, Loss=0.6877400875091553 \t\t Validation: Acc=49%, Loss=0.6959850788116455\n",
            "Iteration: 366090 \t Train: Acc=59%, Loss=0.6912244558334351 \t\t Validation: Acc=51%, Loss=0.692307710647583\n",
            "Iteration: 366100 \t Train: Acc=56%, Loss=0.6874626874923706 \t\t Validation: Acc=46%, Loss=0.7040132880210876\n",
            "Iteration: 366110 \t Train: Acc=53%, Loss=0.6879895329475403 \t\t Validation: Acc=52%, Loss=0.6845442652702332\n",
            "Iteration: 366120 \t Train: Acc=53%, Loss=0.686611533164978 \t\t Validation: Acc=51%, Loss=0.6935145854949951\n",
            "Iteration: 366130 \t Train: Acc=49%, Loss=0.6920110583305359 \t\t Validation: Acc=53%, Loss=0.6927562952041626\n",
            "Iteration: 366140 \t Train: Acc=53%, Loss=0.692198634147644 \t\t Validation: Acc=50%, Loss=0.6938169002532959\n",
            "Iteration: 366150 \t Train: Acc=46%, Loss=0.6954557299613953 \t\t Validation: Acc=47%, Loss=0.6876339912414551\n",
            "Iteration: 366160 \t Train: Acc=53%, Loss=0.6920026540756226 \t\t Validation: Acc=48%, Loss=0.6898932456970215\n",
            "Iteration: 366170 \t Train: Acc=54%, Loss=0.6849026679992676 \t\t Validation: Acc=47%, Loss=0.702340841293335\n",
            "Iteration: 366180 \t Train: Acc=51%, Loss=0.695946216583252 \t\t Validation: Acc=48%, Loss=0.6937218308448792\n",
            "Iteration: 366190 \t Train: Acc=48%, Loss=0.6906418800354004 \t\t Validation: Acc=52%, Loss=0.6819871664047241\n",
            "Iteration: 366200 \t Train: Acc=53%, Loss=0.6894599199295044 \t\t Validation: Acc=47%, Loss=0.6924821138381958\n",
            "Iteration: 366210 \t Train: Acc=47%, Loss=0.6899445056915283 \t\t Validation: Acc=50%, Loss=0.6916411519050598\n",
            "Iteration: 366220 \t Train: Acc=50%, Loss=0.6889287829399109 \t\t Validation: Acc=51%, Loss=0.692913293838501\n",
            "Iteration: 366230 \t Train: Acc=54%, Loss=0.6942468881607056 \t\t Validation: Acc=47%, Loss=0.6937283277511597\n",
            "Iteration: 366240 \t Train: Acc=50%, Loss=0.69391930103302 \t\t Validation: Acc=51%, Loss=0.687483012676239\n",
            "Iteration: 366250 \t Train: Acc=50%, Loss=0.6898281574249268 \t\t Validation: Acc=52%, Loss=0.6892074942588806\n",
            "Iteration: 366260 \t Train: Acc=45%, Loss=0.6965108513832092 \t\t Validation: Acc=49%, Loss=0.6956453323364258\n",
            "Iteration: 366270 \t Train: Acc=52%, Loss=0.6901661157608032 \t\t Validation: Acc=47%, Loss=0.6972750425338745\n",
            "Iteration: 366280 \t Train: Acc=50%, Loss=0.6849873661994934 \t\t Validation: Acc=45%, Loss=0.6943968534469604\n",
            "Iteration: 366290 \t Train: Acc=50%, Loss=0.6900016069412231 \t\t Validation: Acc=48%, Loss=0.6955636739730835\n",
            "Iteration: 366300 \t Train: Acc=56%, Loss=0.6865595579147339 \t\t Validation: Acc=53%, Loss=0.6912632584571838\n",
            "Iteration: 366310 \t Train: Acc=52%, Loss=0.6941065192222595 \t\t Validation: Acc=53%, Loss=0.6881906390190125\n",
            "Iteration: 366320 \t Train: Acc=54%, Loss=0.6905936002731323 \t\t Validation: Acc=48%, Loss=0.6825621724128723\n",
            "Iteration: 366330 \t Train: Acc=50%, Loss=0.6930079460144043 \t\t Validation: Acc=50%, Loss=0.694690465927124\n",
            "Iteration: 366340 \t Train: Acc=49%, Loss=0.6961857080459595 \t\t Validation: Acc=46%, Loss=0.6947201490402222\n",
            "Iteration: 366350 \t Train: Acc=53%, Loss=0.6893410682678223 \t\t Validation: Acc=51%, Loss=0.6925894618034363\n",
            "Iteration: 366360 \t Train: Acc=48%, Loss=0.7028758525848389 \t\t Validation: Acc=52%, Loss=0.6906944513320923\n",
            "Iteration: 366370 \t Train: Acc=49%, Loss=0.6910396814346313 \t\t Validation: Acc=50%, Loss=0.6986504197120667\n",
            "Iteration: 366380 \t Train: Acc=55%, Loss=0.6890571713447571 \t\t Validation: Acc=49%, Loss=0.6898275017738342\n",
            "Iteration: 366390 \t Train: Acc=50%, Loss=0.6963412761688232 \t\t Validation: Acc=48%, Loss=0.692935585975647\n",
            "Iteration: 366400 \t Train: Acc=42%, Loss=0.6925327777862549 \t\t Validation: Acc=52%, Loss=0.687716007232666\n",
            "Iteration: 366410 \t Train: Acc=49%, Loss=0.6932767629623413 \t\t Validation: Acc=47%, Loss=0.6866164207458496\n",
            "Iteration: 366420 \t Train: Acc=57%, Loss=0.6908361315727234 \t\t Validation: Acc=50%, Loss=0.6985310912132263\n",
            "Iteration: 366430 \t Train: Acc=46%, Loss=0.691601037979126 \t\t Validation: Acc=51%, Loss=0.6933349370956421\n",
            "Iteration: 366440 \t Train: Acc=53%, Loss=0.6832711696624756 \t\t Validation: Acc=53%, Loss=0.6852928400039673\n",
            "Iteration: 366450 \t Train: Acc=49%, Loss=0.6908406019210815 \t\t Validation: Acc=48%, Loss=0.6962742209434509\n",
            "Iteration: 366460 \t Train: Acc=50%, Loss=0.690091073513031 \t\t Validation: Acc=50%, Loss=0.6936885118484497\n",
            "Iteration: 366470 \t Train: Acc=50%, Loss=0.7023440599441528 \t\t Validation: Acc=47%, Loss=0.6920496821403503\n",
            "Iteration: 366480 \t Train: Acc=47%, Loss=0.6908885836601257 \t\t Validation: Acc=54%, Loss=0.6914342045783997\n",
            "Iteration: 366490 \t Train: Acc=54%, Loss=0.6883662939071655 \t\t Validation: Acc=51%, Loss=0.6939243674278259\n",
            "Iteration: 366500 \t Train: Acc=53%, Loss=0.696070671081543 \t\t Validation: Acc=48%, Loss=0.6915759444236755\n",
            "Iteration: 366510 \t Train: Acc=45%, Loss=0.6899763941764832 \t\t Validation: Acc=52%, Loss=0.6927856802940369\n",
            "Iteration: 366520 \t Train: Acc=50%, Loss=0.6951768398284912 \t\t Validation: Acc=48%, Loss=0.6921254396438599\n",
            "Iteration: 366530 \t Train: Acc=50%, Loss=0.6845981478691101 \t\t Validation: Acc=52%, Loss=0.6926345229148865\n",
            "Iteration: 366540 \t Train: Acc=52%, Loss=0.6874935626983643 \t\t Validation: Acc=50%, Loss=0.6896634697914124\n",
            "Iteration: 366550 \t Train: Acc=46%, Loss=0.6961352825164795 \t\t Validation: Acc=49%, Loss=0.6976066827774048\n",
            "Iteration: 366560 \t Train: Acc=56%, Loss=0.6897932291030884 \t\t Validation: Acc=49%, Loss=0.6853780746459961\n",
            "Iteration: 366570 \t Train: Acc=46%, Loss=0.6874895691871643 \t\t Validation: Acc=51%, Loss=0.6979084014892578\n",
            "Iteration: 366580 \t Train: Acc=49%, Loss=0.6941007375717163 \t\t Validation: Acc=52%, Loss=0.6968528032302856\n",
            "Iteration: 366590 \t Train: Acc=53%, Loss=0.6994214057922363 \t\t Validation: Acc=49%, Loss=0.6954326629638672\n",
            "Iteration: 366600 \t Train: Acc=50%, Loss=0.6951230764389038 \t\t Validation: Acc=46%, Loss=0.6966102123260498\n",
            "Iteration: 366610 \t Train: Acc=56%, Loss=0.6914528608322144 \t\t Validation: Acc=47%, Loss=0.6975864171981812\n",
            "Iteration: 366620 \t Train: Acc=55%, Loss=0.6804519891738892 \t\t Validation: Acc=50%, Loss=0.6920150518417358\n",
            "Iteration: 366630 \t Train: Acc=50%, Loss=0.6954085230827332 \t\t Validation: Acc=53%, Loss=0.691519021987915\n",
            "Iteration: 366640 \t Train: Acc=42%, Loss=0.6976176500320435 \t\t Validation: Acc=51%, Loss=0.6964219212532043\n",
            "Iteration: 366650 \t Train: Acc=49%, Loss=0.690471887588501 \t\t Validation: Acc=52%, Loss=0.6913660764694214\n",
            "Iteration: 366660 \t Train: Acc=52%, Loss=0.6889406442642212 \t\t Validation: Acc=47%, Loss=0.6959340572357178\n",
            "Iteration: 366670 \t Train: Acc=54%, Loss=0.6831662654876709 \t\t Validation: Acc=50%, Loss=0.6929241418838501\n",
            "Iteration: 366680 \t Train: Acc=55%, Loss=0.6903831958770752 \t\t Validation: Acc=54%, Loss=0.6833663582801819\n",
            "Iteration: 366690 \t Train: Acc=51%, Loss=0.6984865665435791 \t\t Validation: Acc=50%, Loss=0.6973092555999756\n",
            "Iteration: 366700 \t Train: Acc=59%, Loss=0.681107759475708 \t\t Validation: Acc=50%, Loss=0.6948724389076233\n",
            "Iteration: 366710 \t Train: Acc=50%, Loss=0.6855663061141968 \t\t Validation: Acc=49%, Loss=0.6960743069648743\n",
            "Iteration: 366720 \t Train: Acc=53%, Loss=0.6851774454116821 \t\t Validation: Acc=50%, Loss=0.6931819915771484\n",
            "Iteration: 366730 \t Train: Acc=50%, Loss=0.6896829605102539 \t\t Validation: Acc=50%, Loss=0.6985467672348022\n",
            "Iteration: 366740 \t Train: Acc=51%, Loss=0.6889474987983704 \t\t Validation: Acc=50%, Loss=0.6984841227531433\n",
            "Iteration: 366750 \t Train: Acc=50%, Loss=0.6885837316513062 \t\t Validation: Acc=48%, Loss=0.6953151822090149\n",
            "Iteration: 366760 \t Train: Acc=50%, Loss=0.6965997219085693 \t\t Validation: Acc=50%, Loss=0.6924445033073425\n",
            "Iteration: 366770 \t Train: Acc=50%, Loss=0.6945059299468994 \t\t Validation: Acc=47%, Loss=0.697021484375\n",
            "Iteration: 366780 \t Train: Acc=48%, Loss=0.6983171701431274 \t\t Validation: Acc=46%, Loss=0.6931273341178894\n",
            "Iteration: 366790 \t Train: Acc=53%, Loss=0.6849720478057861 \t\t Validation: Acc=50%, Loss=0.6931712627410889\n",
            "Iteration: 366800 \t Train: Acc=56%, Loss=0.6863245368003845 \t\t Validation: Acc=50%, Loss=0.6914640069007874\n",
            "Iteration: 366810 \t Train: Acc=50%, Loss=0.6953999996185303 \t\t Validation: Acc=50%, Loss=0.6907681822776794\n",
            "Iteration: 366820 \t Train: Acc=52%, Loss=0.6891463994979858 \t\t Validation: Acc=50%, Loss=0.7015836834907532\n",
            "Iteration: 366830 \t Train: Acc=49%, Loss=0.6930649280548096 \t\t Validation: Acc=50%, Loss=0.6907694935798645\n",
            "Iteration: 366840 \t Train: Acc=56%, Loss=0.6885753870010376 \t\t Validation: Acc=52%, Loss=0.6936119794845581\n",
            "Iteration: 366850 \t Train: Acc=52%, Loss=0.6880835890769958 \t\t Validation: Acc=50%, Loss=0.6981576085090637\n",
            "Iteration: 366860 \t Train: Acc=50%, Loss=0.6895760297775269 \t\t Validation: Acc=50%, Loss=0.6975341439247131\n",
            "Iteration: 366870 \t Train: Acc=51%, Loss=0.6885555982589722 \t\t Validation: Acc=48%, Loss=0.6918686628341675\n",
            "Iteration: 366880 \t Train: Acc=53%, Loss=0.6972302198410034 \t\t Validation: Acc=50%, Loss=0.6870100498199463\n",
            "Iteration: 366890 \t Train: Acc=47%, Loss=0.6941156983375549 \t\t Validation: Acc=51%, Loss=0.7005661129951477\n",
            "Iteration: 366900 \t Train: Acc=46%, Loss=0.698014497756958 \t\t Validation: Acc=51%, Loss=0.6920673251152039\n",
            "Iteration: 366910 \t Train: Acc=51%, Loss=0.6982465982437134 \t\t Validation: Acc=47%, Loss=0.695461094379425\n",
            "Iteration: 366920 \t Train: Acc=51%, Loss=0.6989649534225464 \t\t Validation: Acc=48%, Loss=0.6959978342056274\n",
            "Iteration: 366930 \t Train: Acc=53%, Loss=0.693020224571228 \t\t Validation: Acc=46%, Loss=0.6955432295799255\n",
            "Iteration: 366940 \t Train: Acc=48%, Loss=0.6967952251434326 \t\t Validation: Acc=55%, Loss=0.6878596544265747\n",
            "Iteration: 366950 \t Train: Acc=52%, Loss=0.6947550773620605 \t\t Validation: Acc=50%, Loss=0.6939493417739868\n",
            "Iteration: 366960 \t Train: Acc=49%, Loss=0.6947872638702393 \t\t Validation: Acc=53%, Loss=0.6907925605773926\n",
            "Iteration: 366970 \t Train: Acc=56%, Loss=0.6868822574615479 \t\t Validation: Acc=51%, Loss=0.6968680620193481\n",
            "Iteration: 366980 \t Train: Acc=57%, Loss=0.6834168434143066 \t\t Validation: Acc=49%, Loss=0.6881580948829651\n",
            "Iteration: 366990 \t Train: Acc=52%, Loss=0.6878789067268372 \t\t Validation: Acc=49%, Loss=0.7017112970352173\n",
            "Iteration: 367000 \t Train: Acc=53%, Loss=0.6931600570678711 \t\t Validation: Acc=46%, Loss=0.6962448358535767\n",
            "Iteration: 367010 \t Train: Acc=54%, Loss=0.6937680840492249 \t\t Validation: Acc=50%, Loss=0.6933228373527527\n",
            "Iteration: 367020 \t Train: Acc=50%, Loss=0.6934864521026611 \t\t Validation: Acc=52%, Loss=0.6906423568725586\n",
            "Iteration: 367030 \t Train: Acc=47%, Loss=0.6977552771568298 \t\t Validation: Acc=52%, Loss=0.6978390216827393\n",
            "Iteration: 367040 \t Train: Acc=51%, Loss=0.6987687945365906 \t\t Validation: Acc=50%, Loss=0.6892275810241699\n",
            "Iteration: 367050 \t Train: Acc=49%, Loss=0.696532666683197 \t\t Validation: Acc=52%, Loss=0.6920264363288879\n",
            "Iteration: 367060 \t Train: Acc=52%, Loss=0.6921294331550598 \t\t Validation: Acc=53%, Loss=0.6946672797203064\n",
            "Iteration: 367070 \t Train: Acc=52%, Loss=0.6914806365966797 \t\t Validation: Acc=50%, Loss=0.6954419612884521\n",
            "Iteration: 367080 \t Train: Acc=52%, Loss=0.6929738521575928 \t\t Validation: Acc=49%, Loss=0.6916176080703735\n",
            "Iteration: 367090 \t Train: Acc=53%, Loss=0.6882827281951904 \t\t Validation: Acc=50%, Loss=0.6908115148544312\n",
            "Iteration: 367100 \t Train: Acc=46%, Loss=0.6952592730522156 \t\t Validation: Acc=49%, Loss=0.6933220028877258\n",
            "Iteration: 367110 \t Train: Acc=53%, Loss=0.6904138326644897 \t\t Validation: Acc=50%, Loss=0.6895902752876282\n",
            "Iteration: 367120 \t Train: Acc=52%, Loss=0.6945005059242249 \t\t Validation: Acc=53%, Loss=0.6921376585960388\n",
            "Iteration: 367130 \t Train: Acc=53%, Loss=0.6911542415618896 \t\t Validation: Acc=49%, Loss=0.6912784576416016\n",
            "Iteration: 367140 \t Train: Acc=56%, Loss=0.6839230060577393 \t\t Validation: Acc=48%, Loss=0.6968398690223694\n",
            "Iteration: 367150 \t Train: Acc=50%, Loss=0.6938413381576538 \t\t Validation: Acc=51%, Loss=0.6922019124031067\n",
            "Iteration: 367160 \t Train: Acc=47%, Loss=0.6999830603599548 \t\t Validation: Acc=53%, Loss=0.691807746887207\n",
            "Iteration: 367170 \t Train: Acc=49%, Loss=0.6942167282104492 \t\t Validation: Acc=51%, Loss=0.6907156109809875\n",
            "Iteration: 367180 \t Train: Acc=49%, Loss=0.6907814145088196 \t\t Validation: Acc=52%, Loss=0.6927179098129272\n",
            "Iteration: 367190 \t Train: Acc=52%, Loss=0.6957236528396606 \t\t Validation: Acc=49%, Loss=0.6919604539871216\n",
            "Iteration: 367200 \t Train: Acc=51%, Loss=0.6943851709365845 \t\t Validation: Acc=49%, Loss=0.6860839128494263\n",
            "Iteration: 367210 \t Train: Acc=48%, Loss=0.6933271884918213 \t\t Validation: Acc=49%, Loss=0.6905800700187683\n",
            "Iteration: 367220 \t Train: Acc=55%, Loss=0.6933926343917847 \t\t Validation: Acc=49%, Loss=0.697965145111084\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-1e213dc4f7e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-1e213dc4f7e8>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-1e213dc4f7e8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}