{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "https://github.com/suhayb-h/Acute-Lymphoblastic-Leukemia-Classifier/blob/main/OmniGlot_CNMC.ipynb",
      "authorship_tag": "ABX9TyOyloJUwjtTrbLB+Gq3hgTn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suhayb-h/Acute-Lymphoblastic-Leukemia-Classifier/blob/main/OmniGlot_CNMC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Download\n",
        "import os\n",
        "import urllib.request\n",
        "import numpy as np\n",
        "import zipfile\n",
        "from imageio import imread #changed from scipy.ndimage -> imageio\n",
        "import matplotlib.image as img\n",
        "import glob as glob\n",
        "from numpy import asarray\n",
        "from numpy import save\n",
        "\n",
        "#Batcher\n",
        "from numpy.random import choice\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from PIL import Image\n",
        "\n",
        "#Model\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "#Train\n",
        "import argparse\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "#vizualize\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl"
      ],
      "metadata": {
        "id": "pziLE6sG5yxK"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ = []\n",
        "\n",
        "for i in glob.glob(\n",
        "    '/content/drive/Othercomputers/My MacBook Air/C-NMC_Leukemia/training_data/hem/*.bmp'):\n",
        "    im=img.imread(i)\n",
        "    train_.append(im)\n",
        "\n",
        "for i in glob.glob(\n",
        "    '/content/drive/Othercomputers/My MacBook Air/C-NMC_Leukemia/training_data/all/*.bmp'):\n",
        "    im=img.imread(i)\n",
        "    train_.append(im)\n",
        "\n",
        "train_array = np.array(train_)\n",
        "\n",
        "# save to npy file\n",
        "save('/content/drive/Othercomputers/My MacBook Air/C-NMC_Leukemia/training_data/train.npy', train_array)"
      ],
      "metadata": {
        "id": "pQwI1LtoW2jj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Batcher: Original Source -> https://github.com/pranv/ARC\n",
        "use_cuda = False\n",
        "\n",
        "class Omniglot(object):\n",
        "    def __init__(self, path=os.path.join('/content/drive/Othercomputers/My MacBook Air/C-NMC_Leukemia/training_data/', 'train.npy'), batch_size=128, image_size=224):\n",
        "        \"\"\"\n",
        "        batch_size: the output is (2 * batch size, 1, image_size, image_size)\n",
        "                    X[i] & X[i + batch_size] are the pair\n",
        "        image_size: size of the image\n",
        "        data_split: in number of alphabets, e.g. [30, 10] means out of 50 Omniglot characters,\n",
        "                    30 is for training, 10 for validation and the remaining(10) for testing\n",
        "        within_alphabet: for verfication task, when 2 characters are sampled to form a pair,\n",
        "                        this flag specifies if should they be from the same alphabet/language\n",
        "        ---------------------\n",
        "        Data Augmentation Parameters:\n",
        "            flip: here flipping both the images in a pair\n",
        "            scale: x would scale image by + or - x%\n",
        "            rotation_deg\n",
        "            shear_deg\n",
        "            translation_px: in both x and y directions\n",
        "        \"\"\"\n",
        "        chars = np.load(path)\n",
        "\n",
        "        # resize the images\n",
        "        resized_chars = np.zeros((10661, 20, image_size, image_size), dtype='uint8')\n",
        "        for i in range(10661):\n",
        "            for j in range(20):\n",
        "                resized_chars[i, j] = np.resize(chars[i, j], (image_size, image_size)) #np added for compatability\n",
        "        chars = resized_chars\n",
        "\n",
        "        self.mean_pixel = chars.mean() / 255.0  # used later for mean subtraction\n",
        "\n",
        "        # starting index of each alphabet in a list of chars\n",
        "        a_start = [0, 3389]\n",
        "\n",
        "        # size of each alphabet (num of chars)\n",
        "        a_size = [3389, 7272]\n",
        "\n",
        "        # each alphabet/language has different number of characters.\n",
        "        # in order to uniformly sample all characters, we need weigh the probability\n",
        "        # of sampling a alphabet by its size. p is that probability\n",
        "        def size2p(size):\n",
        "            s = np.array(size).astype('float64')\n",
        "            return s / s.sum()\n",
        "\n",
        "        self.size2p = size2p\n",
        "        self.data = chars\n",
        "        self.a_start = a_start\n",
        "        self.a_size = a_size\n",
        "        self.image_size = image_size\n",
        "        self.batch_size = batch_size\n",
        "        flip = True\n",
        "        scale = 0.2\n",
        "        rotation_deg = 20\n",
        "        shear_deg = 10\n",
        "        translation_px = 5\n",
        "        #self.augmentor = ImageAugmenter(image_size, image_size,\n",
        "        #                                hflip=flip, vflip=flip,\n",
        "        #                                scale_to_percent=1.0 + scale, rotation_deg=rotation_deg, shear_deg=shear_deg,\n",
        "        #                                translation_x_px=translation_px, translation_y_px=translation_px)\n",
        "\n",
        "    def fetch_batch(self, part):\n",
        "        \"\"\"\n",
        "            This outputs batch_size number of pairs\n",
        "            Thus the actual number of images outputted is 2 * batch_size\n",
        "            Say A & B form the half of a pair\n",
        "            The Batch is divided into 4 parts:\n",
        "                Dissimilar A \t\tDissimilar B\n",
        "                Similar A \t\t\tSimilar B\n",
        "\n",
        "            Corresponding images in Similar A and Similar B form the similar pair\n",
        "            similarly, Dissimilar A and Dissimilar B form the dissimilar pair\n",
        "\n",
        "            When flattened, the batch has 4 parts with indices:\n",
        "                Dissimilar A \t\t0 - batch_size / 2\n",
        "                Similar A    \t\tbatch_size / 2  - batch_size\n",
        "                Dissimilar B \t\tbatch_size  - 3 * batch_size / 2\n",
        "                Similar B \t\t\t3 * batch_size / 2 - batch_size\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "class Batcher(Omniglot):\n",
        "    def __init__(self, path=os.path.join('/content/drive/Othercomputers/My MacBook Air/C-NMC_Leukemia/training_data', 'train.npy'), batch_size=128, image_size=32):\n",
        "        Omniglot.__init__(self, path, batch_size, image_size)\n",
        "\n",
        "        a_start = self.a_start\n",
        "        a_size = self.a_size\n",
        "\n",
        "        # slicing indices for splitting a_start & a_size\n",
        "        i = 20\n",
        "        j = 30\n",
        "        starts = {}\n",
        "        starts['train'], starts['val'], starts['test'] = a_start[:i], a_start[i:j], a_start[j:]\n",
        "        sizes = {}\n",
        "        sizes['train'], sizes['val'], sizes['test'] = a_size[:i], a_size[i:j], a_size[j:]\n",
        "        size2p = self.size2p\n",
        "        p = {}\n",
        "        p['train'], p['val'], p['test'] = size2p(sizes['train']), size2p(sizes['val']), size2p(sizes['test'])\n",
        "        self.starts = starts\n",
        "        self.sizes = sizes\n",
        "        self.p = p\n",
        "\n",
        "    def fetch_batch(self, part, batch_size: int = None):\n",
        "\n",
        "        if batch_size is None:\n",
        "            batch_size = self.batch_size\n",
        "\n",
        "        X, Y = self._fetch_batch(part, batch_size)\n",
        "        X = Variable(torch.from_numpy(X)).view(2*batch_size, self.image_size, self.image_size)\n",
        "        X1 = X[:batch_size]  # (B, h, w)\n",
        "        X2 = X[batch_size:]  # (B, h, w)\n",
        "        X = torch.stack([X1, X2], dim=1)  # (B, 2, h, w)\n",
        "        Y = Variable(torch.from_numpy(Y))\n",
        "\n",
        "        if use_cuda:\n",
        "            X, Y = X.cuda(), Y.cuda()\n",
        "\n",
        "        return X, Y\n",
        "\n",
        "    def _fetch_batch(self, part, batch_size: int = None):\n",
        "        if batch_size is None:\n",
        "            batch_size = self.batch_size\n",
        "\n",
        "        data = self.data\n",
        "        starts = self.starts[part]\n",
        "        sizes = self.sizes[part]\n",
        "        p = self.p[part]\n",
        "        image_size = self.image_size\n",
        "        num_alphbts = len(starts)\n",
        "        X = np.zeros((2 * batch_size, image_size, image_size), dtype='uint8')\n",
        "        #for i in range(batch_size // 2):\n",
        "            # choose similar chars\n",
        "#            same_idx = choice(range(starts[0], starts[-1] + sizes[-1])) #og code: choice(range(starts[0], starts[-1] + sizes[-1]))\n",
        "\n",
        "            # choose dissimilar chars within alphabet\n",
        "#            alphbt_idx = choice(num_alphbts, p=p)\n",
        "# #           char_offset = choice(sizes[alphbt_idx], 2, replace=False)\n",
        "#  #          diff_idx = starts[alphbt_idx] + char_offset\n",
        "#   #         X[i], X[i + batch_size] = data[diff_idx, choice(20, 2)]\n",
        "#    #        X[i + batch_size // 2], X[i + 3 * batch_size // 2] = data[same_idx, choice(20, 2, replace=False)]\n",
        "\n",
        "        y = np.zeros((batch_size, 1), dtype='int32')\n",
        "        y[:batch_size // 2] = 0\n",
        "        y[batch_size // 2:] = 1\n",
        "\n",
        "        if part == 'train':\n",
        "            #X = self.augmentor.augment_batch(X) -> These two lines need to be removed for compatibility to work without ImageAugmenter\n",
        "        #else:\n",
        "            X = X / 255.0\n",
        "\n",
        "        X = X - self.mean_pixel\n",
        "        X = X[:, np.newaxis]\n",
        "        X = X.astype(\"float32\")\n",
        "\n",
        "        return X, y\n"
      ],
      "metadata": {
        "id": "y4Rtd_2wEMzV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mylist = [\"apple\", \"banana\", \"cherry\"]\n",
        "\n",
        "print(choice(range(mylist)))"
      ],
      "metadata": {
        "id": "Wj_gFXSoSeyx",
        "outputId": "2c182d25-5611-446a-f037-7678e473d167",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-dd0dff384287>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmylist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"apple\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"banana\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cherry\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmylist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'list' object cannot be interpreted as an integer"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Model\n",
        "use_cuda = False\n",
        "\n",
        "class GlimpseWindow:\n",
        "    \"\"\"\n",
        "    Generates glimpses from images using Cauchy kernels.\n",
        "    Args:\n",
        "        glimpse_h (int): The height of the glimpses to be generated.\n",
        "        glimpse_w (int): The width of the glimpses to be generated.\n",
        "    \"\"\"\n",
        "    def __init__(self, glimpse_h: int, glimpse_w: int):\n",
        "        self.glimpse_h = glimpse_h\n",
        "        self.glimpse_w = glimpse_w\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_filterbanks(delta_caps: Variable, center_caps: Variable, image_size: int, glimpse_size: int) -> Variable:\n",
        "        \"\"\"\n",
        "        Generates Cauchy Filter Banks along a dimension.\n",
        "        Args:\n",
        "            delta_caps (B,):  A batch of deltas [-1, 1]\n",
        "            center_caps (B,): A batch of [-1, 1] reals that dictate the location of center of cauchy kernel glimpse.\n",
        "            image_size (int): size of images along that dimension\n",
        "            glimpse_size (int): size of glimpses to be generated along that dimension\n",
        "        Returns:\n",
        "            (B, image_size, glimpse_size): A batch of filter banks\n",
        "        \"\"\"\n",
        "        # convert dimension sizes to float. lots of math ahead.\n",
        "        image_size = float(image_size)\n",
        "        glimpse_size = float(glimpse_size)\n",
        "\n",
        "        # scale the centers and the deltas to map to the actual size of given image.\n",
        "        centers = (image_size - 1) * (center_caps + 1) / 2.0  # (B)\n",
        "        deltas = (float(image_size) / glimpse_size) * (1.0 - torch.abs(delta_caps))\n",
        "\n",
        "        # calculate gamma for cauchy kernel\n",
        "        gammas = torch.exp(1.0 - 2 * torch.abs(delta_caps))  # (B)\n",
        "\n",
        "        # coordinate of pixels on the glimpse\n",
        "        glimpse_pixels = Variable(torch.arange(0, glimpse_size) - (glimpse_size - 1.0) / 2.0)  # (glimpse_size)\n",
        "        if use_cuda:\n",
        "            glimpse_pixels = glimpse_pixels.cuda()\n",
        "\n",
        "        # space out with delta\n",
        "        glimpse_pixels = deltas[:, None] * glimpse_pixels[None, :]  # (B, glimpse_size)\n",
        "        # center around the centers\n",
        "        glimpse_pixels = centers[:, None] + glimpse_pixels  # (B, glimpse_size)\n",
        "\n",
        "        # coordinates of pixels on the image\n",
        "        image_pixels = Variable(torch.arange(0, image_size))  # (image_size)\n",
        "        if use_cuda:\n",
        "            image_pixels = image_pixels.cuda()\n",
        "\n",
        "        fx = image_pixels - glimpse_pixels[:, :, None]  # (B, glimpse_size, image_size)\n",
        "        fx = fx / gammas[:, None, None]\n",
        "        fx = fx ** 2.0\n",
        "        fx = 1.0 + fx\n",
        "        fx = math.pi * gammas[:, None, None] * fx\n",
        "        fx = 1.0 / fx\n",
        "        fx = fx / (torch.sum(fx, dim=2) + 1e-4)[:, :, None]  # we add a small constant in the denominator division by 0.\n",
        "\n",
        "        return fx.transpose(1, 2)\n",
        "\n",
        "    def get_attention_mask(self, glimpse_params: Variable, mask_h: int, mask_w: int) -> Variable:\n",
        "        \"\"\"\n",
        "        For visualization, generate a heat map (or mask) of which pixels got the most \"attention\".\n",
        "        Args:\n",
        "            glimpse_params (B, hx):  A batch of glimpse parameters.\n",
        "            mask_h (int): The height of the image for which the mask is being generated.\n",
        "            mask_w (int): The width of the image for which the mask is being generated.\n",
        "        Returns:\n",
        "            (B, mask_h, mask_w): A batch of masks with attended pixels weighted more.\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size, _ = glimpse_params.size()\n",
        "\n",
        "        # (B, image_h, glimpse_h)\n",
        "        F_h = self._get_filterbanks(delta_caps=glimpse_params[:, 2], center_caps=glimpse_params[:, 0],\n",
        "                                    image_size=mask_h, glimpse_size=self.glimpse_h)\n",
        "\n",
        "        # (B, image_w, glimpse_w)\n",
        "        F_w = self._get_filterbanks(delta_caps=glimpse_params[:, 2], center_caps=glimpse_params[:, 1],\n",
        "                                    image_size=mask_w, glimpse_size=self.glimpse_w)\n",
        "\n",
        "        # (B, glimpse_h, glimpse_w)\n",
        "        glimpse_proxy = Variable(torch.ones(batch_size, self.glimpse_h, self.glimpse_w))\n",
        "\n",
        "        # find the attention mask that lead to the glimpse.\n",
        "        mask = glimpse_proxy\n",
        "        mask = torch.bmm(F_h, mask)\n",
        "        mask = torch.bmm(mask, F_w.transpose(1, 2))\n",
        "\n",
        "        # scale to between 0 and 1.0\n",
        "        mask = mask - mask.min()\n",
        "        mask = mask / mask.max()\n",
        "        mask = mask.float()\n",
        "\n",
        "        return mask\n",
        "\n",
        "    def get_glimpse(self, images: Variable, glimpse_params: Variable) -> Variable:\n",
        "        \"\"\"\n",
        "        Generate glimpses given images and glimpse parameters. This is the main method of this class.\n",
        "        The glimpse parameters are (h_center, w_center, delta). (h_center, w_center)\n",
        "        represents the relative position of the center of the glimpse on the image. delta determines\n",
        "        the zoom factor of the glimpse.\n",
        "        Args:\n",
        "            images (B, h, w):  A batch of images\n",
        "            glimpse_params (B, 3):  A batch of glimpse parameters (h_center, w_center, delta)\n",
        "        Returns:\n",
        "            (B, glimpse_h, glimpse_w): A batch of glimpses.\n",
        "        \"\"\"\n",
        "        batch_size, image_h, image_w = images.size()\n",
        "\n",
        "        # (B, image_h, glimpse_h)\n",
        "        F_h = self._get_filterbanks(delta_caps=glimpse_params[:, 2], center_caps=glimpse_params[:, 0],\n",
        "                                    image_size=image_h, glimpse_size=self.glimpse_h)\n",
        "\n",
        "        # (B, image_w, glimpse_w)\n",
        "        F_w = self._get_filterbanks(delta_caps=glimpse_params[:, 2], center_caps=glimpse_params[:, 1],\n",
        "                                    image_size=image_w, glimpse_size=self.glimpse_w)\n",
        "\n",
        "        # F_h.T * images * F_w\n",
        "        glimpses = images\n",
        "        glimpses = torch.bmm(F_h.transpose(1, 2), glimpses)\n",
        "        glimpses = torch.bmm(glimpses, F_w)\n",
        "\n",
        "        return glimpses  # (B, glimpse_h, glimpse_w)\n",
        "\n",
        "class ARC(nn.Module):\n",
        "    \"\"\"\n",
        "    This class implements the Attentive Recurrent Comparators. This module has two main parts.\n",
        "    1.) controller: The RNN module that takes as input glimpses from a pair of images and emits a hidden state.\n",
        "    2.) glimpser: A Linear layer that takes the hidden state emitted by the controller and generates the glimpse\n",
        "                    parameters. These glimpse parameters are (h_center, w_center, delta). (h_center, w_center)\n",
        "                    represents the relative position of the center of the glimpse on the image. delta determines\n",
        "                    the zoom factor of the glimpse.\n",
        "    Args:\n",
        "        num_glimpses (int): How many glimpses must the ARC \"see\" before emitting the final hidden state.\n",
        "        glimpse_h (int): The height of the glimpse in pixels.\n",
        "        glimpse_w (int): The width of the glimpse in pixels.\n",
        "        controller_out (int): The size of the hidden state emitted by the controller.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_glimpses: int=8, glimpse_h: int=8, glimpse_w: int=8, controller_out: int=128) -> None:\n",
        "        super().__init__()\n",
        "        self.num_glimpses = num_glimpses\n",
        "        self.glimpse_h = glimpse_h\n",
        "        self.glimpse_w = glimpse_w\n",
        "        self.controller_out = controller_out\n",
        "\n",
        "        # main modules of ARC\n",
        "        self.controller = nn.LSTMCell(input_size=(glimpse_h * glimpse_w), hidden_size=self.controller_out)\n",
        "        self.glimpser = nn.Linear(in_features=self.controller_out, out_features=3)\n",
        "\n",
        "        # this will actually generate glimpses from images using the glimpse parameters.\n",
        "        self.glimpse_window = GlimpseWindow(glimpse_h=self.glimpse_h, glimpse_w=self.glimpse_w)\n",
        "\n",
        "    def forward(self, image_pairs: Variable) -> Variable:\n",
        "        \"\"\"\n",
        "        The method calls the internal _forward() method which returns hidden states for all time steps. This i\n",
        "        Args:\n",
        "            image_pairs (B, 2, h, w):  A batch of pairs of images\n",
        "        Returns:\n",
        "            (B, controller_out): A batch of final hidden states after each pair of image has been shown for num_glimpses\n",
        "            glimpses.\n",
        "        \"\"\"\n",
        "        # return only the last hidden state\n",
        "        all_hidden = self._forward(image_pairs)  # (2*num_glimpses, B, controller_out)\n",
        "        last_hidden = all_hidden[-1, :, :]  # (B, controller_out)\n",
        "\n",
        "        return last_hidden\n",
        "\n",
        "    def _forward(self, image_pairs: Variable) -> Variable:\n",
        "        \"\"\"\n",
        "        The main forward method of ARC. But it returns hidden state from all time steps (all glimpses) as opposed to\n",
        "        just the last one. See the exposed forward() method.\n",
        "        Args:\n",
        "            image_pairs: (B, 2, h, w) A batch of pairs of images\n",
        "        Returns:\n",
        "            (2*num_glimpses, B, controller_out) Hidden states from ALL time steps.\n",
        "        \"\"\"\n",
        "        # convert to images to float.\n",
        "        image_pairs = image_pairs.float()\n",
        "\n",
        "        # calculate the batch size\n",
        "        batch_size = image_pairs.size()[0]\n",
        "\n",
        "        # an array for collecting hidden states from each time step.\n",
        "        all_hidden = []\n",
        "\n",
        "        # initial hidden state of the LSTM.\n",
        "        Hx = Variable(torch.zeros(batch_size, self.controller_out))  # (B, controller_out)\n",
        "        Cx = Variable(torch.zeros(batch_size, self.controller_out))  # (B, controller_out)\n",
        "\n",
        "        if use_cuda:\n",
        "            Hx, Cx = Hx.cuda(), Cx.cuda()\n",
        "\n",
        "        # take `num_glimpses` glimpses for both images, alternatingly.\n",
        "        for turn in range(2*self.num_glimpses):\n",
        "            # select image to show, alternate between the first and second image in the pair\n",
        "            images_to_observe = image_pairs[:,  turn % 2]  # (B, h, w)\n",
        "\n",
        "            # choose a portion from image to glimpse using attention\n",
        "            glimpse_params = torch.tanh(self.glimpser(Hx))  # (B, 3)  a batch of glimpse params (x, y, delta)\n",
        "            glimpses = self.glimpse_window.get_glimpse(images_to_observe, glimpse_params)  # (B, glimpse_h, glimpse_w)\n",
        "            flattened_glimpses = glimpses.view(batch_size, -1)  # (B, glimpse_h * glimpse_w), one time-step\n",
        "\n",
        "            # feed the glimpses and the previous hidden state to the LSTM.\n",
        "            Hx, Cx = self.controller(flattened_glimpses, (Hx, Cx))  # (B, controller_out), (B, controller_out)\n",
        "\n",
        "            # append this hidden state to all states\n",
        "            all_hidden.append(Hx)\n",
        "\n",
        "        all_hidden = torch.stack(all_hidden)  # (2*num_glimpses, B, controller_out)\n",
        "\n",
        "        # return a batch of all hidden states.\n",
        "        return all_hidden\n",
        "\n",
        "class ArcBinaryClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    A binary classifier that uses ARC.\n",
        "    Given a pair of images, feeds them the ARC and uses the final hidden state of ARC to\n",
        "    classify the images as belonging to the same class or not.\n",
        "    Args:\n",
        "        num_glimpses (int): How many glimpses must the ARC \"see\" before emitting the final hidden state.\n",
        "        glimpse_h (int): The height of the glimpse in pixels.\n",
        "        glimpse_w (int): The width of the glimpse in pixels.\n",
        "        controller_out (int): The size of the hidden state emitted by the controller.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_glimpses: int=8, glimpse_h: int=8, glimpse_w: int=8, controller_out: int = 128):\n",
        "        super().__init__()\n",
        "        self.arc = ARC(\n",
        "            num_glimpses=num_glimpses,\n",
        "            glimpse_h=glimpse_h,\n",
        "            glimpse_w=glimpse_w,\n",
        "            controller_out=controller_out)\n",
        "\n",
        "        # two dense layers, which take the hidden state from the controller of ARC and\n",
        "        # classify the images as belonging to the same class or not.\n",
        "        self.dense1 = nn.Linear(controller_out, 64)\n",
        "        self.dense2 = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, image_pairs: Variable) -> Variable:\n",
        "        arc_out = self.arc(image_pairs)\n",
        "\n",
        "        d1 = F.elu(self.dense1(arc_out))\n",
        "        decision = torch.sigmoid(self.dense2(d1))\n",
        "\n",
        "        return decision\n",
        "\n",
        "    def save_to_file(self, file_path: str) -> None:\n",
        "        torch.save(self.state_dict(), file_path)"
      ],
      "metadata": {
        "id": "ESAiij2DD-ra"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('-f') #neccessary null argument for colab compatibility\n",
        "parser.add_argument('--batchSize', type=int, default=128, help='input batch size')\n",
        "parser.add_argument('--imageSize', type=int, default=32, help='the height / width of the input image to ARC')\n",
        "parser.add_argument('--glimpseSize', type=int, default=8, help='the height / width of glimpse seen by ARC')\n",
        "parser.add_argument('--numStates', type=int, default=128, help='number of hidden states in ARC controller')\n",
        "parser.add_argument('--numGlimpses', type=int, default=6, help='the number glimpses of each image in pair seen by ARC')\n",
        "parser.add_argument('--lr', type=float, default=0.0002, help='learning rate, default=0.0002')\n",
        "parser.add_argument('--cuda', action='store_true', help='enables cuda')\n",
        "parser.add_argument('--name', default=None, help='Custom name for this configuration. Needed for saving'\n",
        "                                                 ' model checkpoints in a separate folder.')\n",
        "parser.add_argument('--load', default=None, help='the model to load from. Start fresh if not specified.')\n",
        "\n",
        "def get_pct_accuracy(pred: Variable, target) -> int:\n",
        "    hard_pred = (pred > 0.5).int()\n",
        "    correct = (hard_pred == target).sum().data#[0]\n",
        "    accuracy = float(correct) / target.size()[0]\n",
        "    accuracy = int(accuracy * 100)\n",
        "    return accuracy\n",
        "\n",
        "def train():\n",
        "    opt = parser.parse_args()\n",
        "\n",
        "    if opt.cuda:\n",
        "        batcher.use_cuda = True\n",
        "        models.use_cuda = True\n",
        "\n",
        "    if opt.name is None:\n",
        "        # if no name is given, we generate a name from the parameters.\n",
        "        # only those parameters are taken, which if changed break torch.load compatibility.\n",
        "        opt.name = \"{}_{}_{}_{}\".format(opt.numGlimpses, opt.glimpseSize, opt.numStates,\n",
        "                                        \"cuda\" if opt.cuda else \"cpu\")\n",
        "        \n",
        "    # make directory for storing models.\n",
        "    models_path = os.path.join(\"saved_models\", opt.name)\n",
        "    os.makedirs(models_path, exist_ok=True)\n",
        "\n",
        "    # initialise the model\n",
        "    discriminator = ArcBinaryClassifier(num_glimpses=opt.numGlimpses,\n",
        "                                        glimpse_h=opt.glimpseSize,\n",
        "                                        glimpse_w=opt.glimpseSize,\n",
        "                                        controller_out=opt.numStates)\n",
        "\n",
        "    if opt.cuda:\n",
        "        discriminator.cuda()\n",
        "\n",
        "    # load from a previous checkpoint, if specified.\n",
        "    if opt.load is not None:\n",
        "        discriminator.load_state_dict(torch.load(os.path.join(models_path, opt.load)))\n",
        "\n",
        "    # set up the optimizer.\n",
        "    bce = torch.nn.BCELoss()\n",
        "    if opt.cuda:\n",
        "        bce = bce.cuda()\n",
        "\n",
        "    optimizer = torch.optim.Adam(params=discriminator.parameters(), lr=opt.lr)\n",
        "\n",
        "    # load the dataset in memory.\n",
        "    loader = Batcher(batch_size=opt.batchSize, image_size=opt.imageSize)\n",
        "\n",
        "    # ready to train ...\n",
        "    best_validation_loss = None\n",
        "    saving_threshold = 1.02\n",
        "    last_saved = datetime.utcnow()\n",
        "    save_every = timedelta(minutes=10)\n",
        "\n",
        "    i = -1\n",
        "    while True:\n",
        "        i += 1\n",
        "        X, Y = loader.fetch_batch(\"train\")\n",
        "        pred = discriminator(X)\n",
        "        loss = bce(pred, Y.float())\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            # validate your model\n",
        "            X_val, Y_val = loader.fetch_batch(\"val\")\n",
        "            pred_val = discriminator(X_val)\n",
        "            loss_val = bce(pred_val, Y_val.float())\n",
        "\n",
        "            training_loss = loss.data#[0]\n",
        "            validation_loss = loss_val.data#[0]\n",
        "\n",
        "            print(\"Iteration: {} \\t Train: Acc={}%, Loss={} \\t\\t Validation: Acc={}%, Loss={}\".format(\n",
        "                i, get_pct_accuracy(pred, Y), training_loss, get_pct_accuracy(pred_val, Y_val), validation_loss\n",
        "            ))\n",
        "\n",
        "            if best_validation_loss is None:\n",
        "                best_validation_loss = validation_loss\n",
        "\n",
        "            if best_validation_loss > (saving_threshold * validation_loss):\n",
        "                print(\"Significantly improved validation loss from {} --> {}. Saving...\".format(\n",
        "                    best_validation_loss, validation_loss\n",
        "                ))\n",
        "                discriminator.save_to_file(os.path.join(models_path, str(validation_loss)))\n",
        "                best_validation_loss = validation_loss\n",
        "                last_saved = datetime.utcnow()\n",
        "\n",
        "            if last_saved + save_every < datetime.utcnow():\n",
        "                print(\"It's been too long since we last saved the model. Saving...\")\n",
        "                discriminator.save_to_file(os.path.join(models_path, str(validation_loss)))\n",
        "                last_saved = datetime.utcnow()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "def main() -> None:\n",
        "    train()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KPNmyUcoDodI",
        "outputId": "8e86c7da-239b-4cd3-f9f2-1a4c6cf32fb7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 0 \t Train: Acc=50%, Loss=0.6935591101646423 \t\t Validation: Acc=50%, Loss=0.6935591101646423\n",
            "Iteration: 10 \t Train: Acc=50%, Loss=0.6931472420692444 \t\t Validation: Acc=50%, Loss=0.6931472420692444\n",
            "Iteration: 20 \t Train: Acc=50%, Loss=0.6931724548339844 \t\t Validation: Acc=50%, Loss=0.6931724548339844\n",
            "Iteration: 30 \t Train: Acc=50%, Loss=0.6931485533714294 \t\t Validation: Acc=50%, Loss=0.6931485533714294\n",
            "Iteration: 40 \t Train: Acc=50%, Loss=0.6931498050689697 \t\t Validation: Acc=50%, Loss=0.6931498050689697\n",
            "Iteration: 50 \t Train: Acc=50%, Loss=0.6931477785110474 \t\t Validation: Acc=50%, Loss=0.6931477785110474\n",
            "Iteration: 60 \t Train: Acc=50%, Loss=0.6931472420692444 \t\t Validation: Acc=50%, Loss=0.6931472420692444\n",
            "Iteration: 70 \t Train: Acc=50%, Loss=0.6931473016738892 \t\t Validation: Acc=50%, Loss=0.6931473016738892\n",
            "Iteration: 80 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 90 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 100 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 110 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 120 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 130 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 140 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 150 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 160 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 170 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 180 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 190 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 200 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 210 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 220 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 230 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 240 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 250 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 260 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 270 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 280 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 290 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 300 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 310 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 320 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 330 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 340 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 350 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 360 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 370 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 380 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 390 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 400 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 410 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 420 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 430 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 440 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 450 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 460 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 470 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 480 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 490 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 500 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 510 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 520 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 530 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 540 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 550 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 560 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 570 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 580 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 590 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 600 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 610 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 620 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 630 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 640 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 650 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 660 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 670 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 680 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 690 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 700 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 710 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 720 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 730 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 740 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 750 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 760 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 770 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 780 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 790 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 800 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 810 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 820 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 830 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 840 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 850 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 860 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 870 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 880 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 890 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 900 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 910 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 920 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 930 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 940 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 950 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 960 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 970 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 980 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 990 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 1000 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 1010 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 1020 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 1030 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 1040 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 1050 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 1060 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 1070 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 1080 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 1090 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 1100 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 1110 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 1120 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 1130 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 1140 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 1150 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 1160 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 1170 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 1180 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 1190 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 1200 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 1210 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 1220 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 1230 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 1240 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 1250 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n",
            "Iteration: 1260 \t Train: Acc=50%, Loss=0.6931471824645996 \t\t Validation: Acc=50%, Loss=0.6931471824645996\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-71f6e90c9214>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-71f6e90c9214>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-71f6e90c9214>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-843ab2436cbb>\u001b[0m in \u001b[0;36mfetch_batch\u001b[0;34m(self, part, batch_size)\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mX1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# (B, h, w)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-843ab2436cbb>\u001b[0m in \u001b[0;36m_fetch_batch\u001b[0;34m(self, part, batch_size)\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"float32\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}