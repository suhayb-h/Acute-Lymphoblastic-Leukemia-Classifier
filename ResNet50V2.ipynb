{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ResNet50V2.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOb5d25aBoixiK4ib0R7XxR"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Introduction\n",
        "\n",
        "The utilization of neural networks in medicine continues to evolve every year, much like medical research in general. This central ethos is a reflection of the overall purpose of medicine which is the continual striving to improve patient care. Within the context of using data science as a tool in medical diagnosis, testing different models and tweaking the hyperparameters allows for the improvement in accuracy and precision, amongst other statistical outcomes. Ultimately, even a 1% percent improvement in any metric can result in diagnosing ALL earlier. \n",
        "\n",
        "In the previous paper, a CNN was used to showcase how neural networks could be constructed. In reality, there are many more neural networks that are much more complex than a CNN, and such networks might result in improved performance. The CNN constructed in the previous paper was relatively simple, containing a total of 6 layers. In the paper \"Pathology Image Analysis Using Segmentation Deep Learning Algorithms\" by Wang et. al, the performance of 9 different neural networks were compared to one another with regards to diagnosing pathological slides. One of these models was the ResNetXt101 model. To specifically assess the ResNet family of neural networks, a ResNet50V2 model was trained and analyzed in this report. This model contains over 50 layers and allows for additional layers to be added."
      ],
      "metadata": {
        "id": "A3g6GGxRPiYU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Creating the Model\n",
        "\n",
        "Constructing neural networks is relatively similar amongst models. Much of the process of establishing the ResNetV2 was identical to the process for the CNN model in the second report. The initial steps were to import relevant libraries, import the dataset, scale the data, and finally split the data into a training set and a validation set. This split was done directly within the image data generator, just as it was in the sequential CNN model construction."
      ],
      "metadata": {
        "id": "AuKHryJtPqL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import ResNet50V2\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras import regularizers\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "metadata": {
        "id": "1whXO6QHv21g"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUSl78LhqnW1",
        "outputId": "f366aa37-9aff-4e1b-daa4-0c99145d1043"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just as the sequential CNN model, the images in this dataset are passed through an image generator where they are rescaled and permitted to be randomly rotated and flipped horizontally or vertically. Since the test dataset had 1867 images, the validation dataset was set to 17.5% of the training dataset, equating to 1865 images. Within the generators themselves, all images were forced to retain their color and were subsequently shuffled."
      ],
      "metadata": {
        "id": "eqdRgrten7-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training image data generator.\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, \n",
        "                                   rotation_range = 180, \n",
        "                                   horizontal_flip = True, \n",
        "                                   vertical_flip = True, \n",
        "                                   validation_split = 0.175)\n",
        "\n",
        "# Finaly we specify where the images should be loaded from as well as some additional attributes:\n",
        "train_generator = train_datagen.flow_from_directory('/content/drive/Othercomputers/My MacBook Air/BrainStation/Capstone/capstone_data/training_data', \n",
        "                                                    target_size = (256, 256), \n",
        "                                                    batch_size = 256,\n",
        "                                                    color_mode = 'rgb', \n",
        "                                                    class_mode = 'categorical',\n",
        "                                                    shuffle = True, \n",
        "                                                    subset = 'training')\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory('/content/drive/Othercomputers/My MacBook Air/BrainStation/Capstone/capstone_data/training_data', \n",
        "                                                         target_size = (256, 256), \n",
        "                                                         batch_size = 256,\n",
        "                                                         color_mode = 'rgb', \n",
        "                                                         class_mode = 'categorical',\n",
        "                                                         shuffle = True, \n",
        "                                                         subset = 'validation')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zM8Lae2oqmb",
        "outputId": "b1f76cae-3156-4394-979c-3a742249310f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 8796 images belonging to 2 classes.\n",
            "Found 1865 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Instantiating the ResNet50V2 Model\n",
        "\n",
        "As previously mentioned, the ResNetV2 model is substantially more complex than the CNN model. In contrast to the sequential CNN, layers are added to the model by multiplying each subsequent layer with its previous layer. By extension, the model has several hyperparamaters that can be adjusted to affect the overall performance of the model. There were several distinct decisions that were made in the creation of the ResNetV2 model for this report:\n",
        "\n",
        "*   Unlike the CNN in the second report, this model is capable of utilizing pre-established weights. Most data science projects choose to set the weight parameter to the default \"Imagnet\". However, since this image dataset deals with cellular images, such broadly established weights might prove detrimental in the classification of cancer cells. For this reason, the pre-established weights parameter was set to \"None\"\n",
        "*   The ResNetV2 model allows for a fully connected layer that lies on top of the model. A fully connected layer is dense layer that is more deeply connected with the layer below it, when compared to a traditional dense layer. This layer was turned off simply because it goes against the purpose of this report. This ResNetv2 model was constructed to compare it's baseline performance to that of the CNN in the second report. To establish a baseline, the model should be simplified as much as possible, with the exclusion of as many extra layers as possible. \n",
        "*   The layers in the default ResNetV2 model were frozen, resulting in no training being possible on these layers. This decision was made to, again, serve the purpose of comparing the baseline performance of a ResNetV2 model to the performance of the CNN model created in the second report. The CNN model had 6 layers and thus the ResNetV2 model was constructed to emulate 6 layers. By disallowing the default 50+ layers to be trained, these layers effectively serve as one untrainable layer, and 5 new layers were added at the end of the model. These subsequent 5 layers are capable of training, which means any improvement in performance of this model will be a direct result of these 5 additional layers, and not the default ResNetV2 layers.\n",
        "*   The classification by which loss was measured in this model was identical to the CNN model, being a \"Binary Cross-Entropy\". Since this model is a binary classifier, setting the loss classifier to binary is the most appropriate decision, just as it was for the CNN model.\n",
        "\n"
      ],
      "metadata": {
        "id": "lRVfwn4QPxCD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Lpg08XKEobhm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "397443cb-d041-43dc-e102-6320e66fabb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94674944/94668760 [==============================] - 1s 0us/step\n",
            "94683136/94668760 [==============================] - 1s 0us/step\n"
          ]
        }
      ],
      "source": [
        "resnet_model = ResNet50V2(weights = 'imagenet', # <-- Imagenet weights are used for this model\n",
        "                   include_top = False, # <-- Turned off the fully connected layer\n",
        "                   input_shape = (256, 256, 3))\n",
        "#resnet_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in resnet_model.layers:\n",
        "    layer.trainable = False # <-- Default ResNet50V2 Layers were frozen, thereby allowing for no training on these layers"
      ],
      "metadata": {
        "id": "4-IHgwrVolHo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5 additional layers were added to this model:\n",
        "\n",
        "\n",
        "*   A flattened layer was added to flatten the tensor array in order to make it compatible with the subsequent layer below it.\n",
        "*   Three dense layers were added and were specifically designated to contian regularizer parameters. The primary purpose of the kernel and activity regularizers was to reduce overfitting. Bias regularizers were also an option, but were ultimately decided against, since doing so can result in underfitting. Two regularizers were available, L1 and L2, with the option to utilize both regularizers in conjuntion for each parameter. Ultimately, L2 was chosen as the sole regularizer since it specifically serves the function of reducing overfitting.\n",
        "*   The final output layer of this model was set to a sigmoid activating dense layer with two outputs. This final output layer serves to provide a binary classification system between two final results, those being cancerous and non-cancerous. \n",
        "\n"
      ],
      "metadata": {
        "id": "pCgaqWuKo8qc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add a flatten layer\n",
        "x0 = Flatten()(resnet_model.output)"
      ],
      "metadata": {
        "id": "7ruwWYvronVw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add dense layers\n",
        "x1 = Dense(128, activation=\"relu\", kernel_regularizer = regularizers.L2(0.01), activity_regularizer = regularizers.L2(0.01))(x0)\n",
        "x2 = Dense(64, activation=\"relu\", kernel_regularizer = regularizers.L2(0.01), activity_regularizer = regularizers.L2(0.01))(x1)\n",
        "x3 = Dense(32, activation=\"relu\", kernel_regularizer = regularizers.L2(0.01), activity_regularizer = regularizers.L2(0.01))(x2)\n",
        "output = Dense(2, activation=\"sigmoid\")(x1) # <-- The final output layer of the model is set to a sigmoidal activator with 2 nodes, signifying 2 binary outcomes"
      ],
      "metadata": {
        "id": "NZcvtMRmptdO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(inputs = resnet_model.input, \n",
        "              outputs = output)"
      ],
      "metadata": {
        "id": "ljX6gtBapttq"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = 'Adam',\n",
        "              loss = 'binary_crossentropy',\n",
        "              metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "QQbLnTK_uy9V"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use an early stopping callback to stop training\n",
        "# once we no longer have improvements in our validation loss\n",
        "early_stop = EarlyStopping(monitor = 'val_loss', \n",
        "                           patience = 5, \n",
        "                           mode = 'min', \n",
        "                           verbose = 1)\n",
        "\n",
        "hist = model.fit(train_generator,\n",
        "          epochs = 30,\n",
        "          validation_data = validation_generator,\n",
        "          callbacks = [early_stop])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EO2A9aV9puEF",
        "outputId": "36bd6fc4-f03f-4a7b-fe94-de3bf533e500"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "32/35 [==========================>...] - ETA: 3:51 - loss: 7.8305 - accuracy: 0.6834"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Analysis of Model Performance\n",
        "\n",
        "For the purposes of this preliminary report, an early stop was set in place for this model based on insufficient improvements in lowering validation loss. This was the same strategy used for the sequential CNN model in the second report. Based on the preliminary analysis of 16 epochs, this model seems to have a lot of room for parameter optimization. There is definite overfitting of the training data, as dilineated by the two graphs below. In typical workflows, the model would continuously be optimized in an iterative process. For the purposes of this preliminary report, analysis was continued as if this model had been completely optimized. Testing data analysis resulted in incredibly low accuracy values despite the model having trained well. To get a better understanding of how the model was being trained, the validation set will be used for final analysis in place of the test dataset. The test data will still be imported as it would in normal workflows. "
      ],
      "metadata": {
        "id": "Q-YjJNXFqH-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure()\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(hist.history['loss'], \n",
        "         color = 'teal', \n",
        "         label = 'loss')\n",
        "plt.plot(hist.history['val_loss'], \n",
        "         color = 'orange', \n",
        "         label = 'val_loss')\n",
        "fig.suptitle('Loss', fontsize=20)\n",
        "plt.legend(loc = \"upper left\")\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epochs')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(hist.history['accuracy'], \n",
        "         color = 'teal', \n",
        "         label = 'accuracy')\n",
        "plt.plot(hist.history['val_accuracy'], \n",
        "         color = 'orange', \n",
        "         label = 'val_accuracy')\n",
        "fig.suptitle('Accuracy', fontsize=20)\n",
        "plt.legend(loc = \"upper left\")\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hGNkH6L87jvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The final phase of analysis was to assess the models performance on the testing data. Just as before, the testing data was imported via a dataframe image generator."
      ],
      "metadata": {
        "id": "BscIXjo6sQyx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = pd.read_csv(\"/content/drive/Othercomputers/My MacBook Air/BrainStation/Capstone/capstone_data/test_data/val_data.csv\", \n",
        "                      dtype = str)\n",
        "df_test"
      ],
      "metadata": {
        "id": "wdcnUyu4va6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the same rescaling factor as our other generators\n",
        "test_datagen = ImageDataGenerator(rescale=1./255, \n",
        "                                  rotation_range = 180, \n",
        "                                  horizontal_flip = True, \n",
        "                                  vertical_flip = True)\n",
        "test_generator = test_datagen.flow_from_dataframe(dataframe = df_test,\n",
        "                                                  directory = '/content/drive/Othercomputers/My MacBook Air/BrainStation/Capstone/capstone_data/test_data',\n",
        "                                                  target_size = (256, 256),\n",
        "                                                  batch_size = 256,\n",
        "                                                  x_col = 'new_names',\n",
        "                                                  y_col = 'labels',\n",
        "                                                  color_mode = 'rgb',\n",
        "                                                  class_mode = 'categorical')\n",
        "\n",
        "X_test, y_test = validation_generator.next() #<-- Validation data was used for all further analysis as opposed to the test data"
      ],
      "metadata": {
        "id": "SXUdlUDFqH8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Final analysis\n",
        "\n",
        "This model showcased a test accuracy of 73.44%. While this model did not result in an accuracy score higher than the sequential model, there are a few reasons why this model can  still be considered a success. Despite the overall accuracy being lower, fewer normal lymphoblasts were misdiagnosed as ALL cells. Additionally, more normal lymphoblasts were correctly labelled when compared to the sequential CNN model.\n",
        "\n",
        "However, this model suffered from the same issue as the Sequential CNN. Both models were extremely unlikely to predict that any cell was a normal lymphoblast. Considering the distribution of the data, where only 31.79% of all images were healthy lymphoblasts in the training dataset, this isn't surprising. However, the two models should have still predicted normal lymphoblasts 31% of the time, accounting for about 79 predictions. Instead, based on the confusion matrix, the model only predicted 38 healthy lymphoblasts. "
      ],
      "metadata": {
        "id": "H3d20agpsmnu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model's performance on the test data\n",
        "score = model.evaluate(X_test, \n",
        "                       y_test)\n",
        "\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "metadata": {
        "id": "xMsqHEinqIOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the predictions\n",
        "predict_probas = model.predict(X_test)\n",
        "\n",
        "# Convert probabilities to label encoding\n",
        "y_predict = np.argmax(predict_probas, axis=1)"
      ],
      "metadata": {
        "id": "g7Y1-vuoqIi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = validation_generator.class_indices\n",
        "\n",
        "y_test_labels = np.argmax(y_test, axis=1)\n",
        "conf_mat = confusion_matrix(y_test_labels, y_predict)\n",
        "\n",
        "plt.figure(figsize = (8,6))\n",
        "sns.heatmap(conf_mat,\n",
        "            annot=True,\n",
        "            cbar=False,\n",
        "            xticklabels = class_names,\n",
        "            yticklabels = class_names,\n",
        "            cmap = \"rocket_r\",\n",
        "            linewidths = 1)\n",
        "plt.title('Confusion Matrix ResNetV50V2', \n",
        "          size = 25, \n",
        "          y = 1.01)\n",
        "plt.xlabel(\"Predicted Label\", size = 20)\n",
        "plt.ylabel(\"True Label\", size = 20)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SwTOTRTpqI-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Conclusion and Future Advancements\n",
        "\n",
        "This capstone project meant to serve as a case study to assess the efficacy of utilizing neural networks and machine learning to diagnose Acute Lymphoblastic Leukemia. Despite not establishing a useful model, it is worth reflecting on the limitations of the data that was available. \n",
        "\n",
        "The initial phase of this capstone project was an experience in understanding the limitations of machine learning. Despite having a small sample size, computational resources were consistently an issue when dealing with image data. Any classic modelling was impossible since it required dealing with images as numpy arrays. Such attempts caused even Google's CoLab GPUs to crash. Dealing with images as tensors proved to utilize GPU power much more efficiently and some classic machine learning models have started to gain development on tensorflow and pytorch, specifically logistic regression and decision forests. These models are still in their preliminary development phases with very little documentation available on hyperparameter optimization, but implementing these models might provide new useful insights. \n",
        "\n",
        "This was a relatively small dataset, with only 12528 images available to train and test on. Furthermore, there is a class imbalance of roughly 69% cancer cells to 31% healthy lymphoblasts in the training dataset. Both of these factors can negatively impact the establishment of an effective neural network model. Reducing the dataset to create a more even split might affect performance positively or negatively. Having less data can result in substantially worse performance, so for the purposes of this preliminary report all images were utilized.\n",
        "\n",
        "Neural network optimization is a mundane and iterative process. For the sequential CNN, more layers could be added, layers can be removed and parameters can be continued to be tweaked, but until the model is trained there is no means of knowing what combination works best for the model. The initial exploratory data analysis in report 1 showcased a clear differentiation of cells based on size and eosin colour distribution, so adjusting the mathematical formulas of individual layers to better select for these two variables might result in a better performing model.\n",
        "\n",
        "The ResNetV2 model has the most potential for improved performance. Independent testing in turning on the fully connected top dense layer and allowing the default layers to be trainable resulted in increased performance but significant overfitting. Adding more layers or tuning parameters within these layers might have allowed for these two features to be active while reducing overfitting. Furthermore, this specific model is called the ResNet50V2 because it has 50 default layers and is the second iteration of the model. A V3 does exist and might result in improved performance, while other ResNet models exist with thousands of layers. Above all else, one feature within this model holds the most potential for improved performance, that being the allowance for weight data. ImageNet was specifically decided against since these weights were established from the differentiation of everyday objects instead of minutia between extremely similar cells. A medical ImageNet does exist but is over 1000 terabytes in size. Being able to train any medical image classifier on this specific weight library might result in the highest possible performance. "
      ],
      "metadata": {
        "id": "_gjrBZjlwDcv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Cl9SQ1Mw0QnW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}