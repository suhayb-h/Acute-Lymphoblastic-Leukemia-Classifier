{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1UYS5yBWIeNbijJYf5Kz5bSBbYKGKl4au",
      "authorship_tag": "ABX9TyOpUAEgigW/0T3WO/NwnFbR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suhayb-h/Acute-Lymphoblastic-Leukemia-Classifier/blob/main/NOV24_ARC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "#Image Augmenter\n",
        "from skimage import transform as tf\n",
        "import random\n",
        "#Batcher\n",
        "from numpy.random import choice\n",
        "from torch.autograd import Variable\n",
        "#from scipy.misc import imresize as resize\n",
        "#Models\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "#Train\n",
        "import argparse\n",
        "from datetime import datetime, timedelta"
      ],
      "metadata": {
        "id": "aXzeiqZt6kqY"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Image Augmenter\n",
        "\n",
        "def is_minmax_tuple(param):\n",
        "    \"\"\"Returns whether the parameter is a tuple containing two values.\n",
        "\n",
        "    Used in create_aug_matrices() and probably useless everywhere else.\n",
        "\n",
        "    Args:\n",
        "        param: The parameter to check (whether it is a tuple of length 2).\n",
        "\n",
        "    Returns:\n",
        "        Boolean\n",
        "    \"\"\"\n",
        "    return type(param) is tuple and len(param) == 2\n",
        "\n",
        "def create_aug_matrices(nb_matrices, img_width_px, img_height_px,\n",
        "                        scale_to_percent=1.0, scale_axis_equally=False,\n",
        "                        rotation_deg=0, shear_deg=0,\n",
        "                        translation_x_px=0, translation_y_px=0,\n",
        "                        seed=None):\n",
        "    \"\"\"Creates the augmentation matrices that may later be used to transform\n",
        "    images.\n",
        "\n",
        "    This is a wrapper around scikit-image's transform.AffineTransform class.\n",
        "    You can apply those matrices to images using the apply_aug_matrices()\n",
        "    function.\n",
        "\n",
        "    Args:\n",
        "        nb_matrices: How many matrices to return, e.g. 100 returns 100 different\n",
        "            random-generated matrices (= 100 different transformations).\n",
        "        img_width_px: Width of the images that will be transformed later\n",
        "            on (same as the width of each of the matrices).\n",
        "        img_height_px: Height of the images that will be transformed later\n",
        "            on (same as the height of each of the matrices).\n",
        "        scale_to_percent: Same as in ImageAugmenter.__init__().\n",
        "            Up to which percentage the images may be\n",
        "            scaled/zoomed. The negative scaling is automatically derived\n",
        "            from this value. A value of 1.1 allows scaling by any value\n",
        "            between -10% and +10%. You may set min and max values yourself\n",
        "            by using a tuple instead, like (1.1, 1.2) to scale between\n",
        "            +10% and +20%. Default is 1.0 (no scaling).\n",
        "        scale_axis_equally: Same as in ImageAugmenter.__init__().\n",
        "            Whether to always scale both axis (x and y)\n",
        "            in the same way. If set to False, then e.g. the Augmenter\n",
        "            might scale the x-axis by 20% and the y-axis by -5%.\n",
        "            Default is False.\n",
        "        rotation_deg: Same as in ImageAugmenter.__init__().\n",
        "            By how much the image may be rotated around its\n",
        "            center (in degrees). The negative rotation will automatically\n",
        "            be derived from this value. E.g. a value of 20 allows any\n",
        "            rotation between -20 degrees and +20 degrees. You may set min\n",
        "            and max values yourself by using a tuple instead, e.g. (5, 20)\n",
        "            to rotate between +5 und +20 degrees. Default is 0 (no\n",
        "            rotation).\n",
        "        shear_deg: Same as in ImageAugmenter.__init__().\n",
        "            By how much the image may be sheared (in degrees). The\n",
        "            negative value will automatically be derived from this value.\n",
        "            E.g. a value of 20 allows any shear between -20 degrees and\n",
        "            +20 degrees. You may set min and max values yourself by using a\n",
        "            tuple instead, e.g. (5, 20) to shear between +5 und +20\n",
        "            degrees. Default is 0 (no shear).\n",
        "        translation_x_px: Same as in ImageAugmenter.__init__().\n",
        "            By up to how many pixels the image may be\n",
        "            translated (moved) on the x-axis. The negative value will\n",
        "            automatically be derived from this value. E.g. a value of +7\n",
        "            allows any translation between -7 and +7 pixels on the x-axis.\n",
        "            You may set min and max values yourself by using a tuple\n",
        "            instead, e.g. (5, 20) to translate between +5 und +20 pixels.\n",
        "            Default is 0 (no translation on the x-axis).\n",
        "        translation_y_px: Same as in ImageAugmenter.__init__().\n",
        "            See translation_x_px, just for the y-axis.\n",
        "        seed: Seed to use for python's and numpy's random functions.\n",
        "\n",
        "    Returns:\n",
        "        List of augmentation matrices.\n",
        "    \"\"\"\n",
        "    assert nb_matrices > 0\n",
        "    assert img_width_px > 0\n",
        "    assert img_height_px > 0\n",
        "    assert is_minmax_tuple(scale_to_percent) or scale_to_percent >= 1.0\n",
        "    assert is_minmax_tuple(rotation_deg) or rotation_deg >= 0\n",
        "    assert is_minmax_tuple(shear_deg) or shear_deg >= 0\n",
        "    assert is_minmax_tuple(translation_x_px) or translation_x_px >= 0\n",
        "    assert is_minmax_tuple(translation_y_px) or translation_y_px >= 0\n",
        "\n",
        "    if seed is not None:\n",
        "        random.seed(seed)\n",
        "        np.random.seed(seed)\n",
        "\n",
        "    result = []\n",
        "\n",
        "    shift_x = int(img_width_px / 2.0)\n",
        "    shift_y = int(img_height_px / 2.0)\n",
        "\n",
        "    # prepare min and max values for\n",
        "    # scaling/zooming (min/max values)\n",
        "    if is_minmax_tuple(scale_to_percent):\n",
        "        scale_x_min = scale_to_percent[0]\n",
        "        scale_x_max = scale_to_percent[1]\n",
        "    else:\n",
        "        scale_x_min = scale_to_percent\n",
        "        scale_x_max = 1.0 - (scale_to_percent - 1.0)\n",
        "    assert scale_x_min > 0.0\n",
        "    #if scale_x_max >= 2.0:\n",
        "    #     warnings.warn(\"Scaling by more than 100 percent (%.2f).\" % (scale_x_max,))\n",
        "    scale_y_min = scale_x_min # scale_axis_equally affects the random value generation\n",
        "    scale_y_max = scale_x_max\n",
        "\n",
        "    # rotation (min/max values)\n",
        "    if is_minmax_tuple(rotation_deg):\n",
        "        rotation_deg_min = rotation_deg[0]\n",
        "        rotation_deg_max = rotation_deg[1]\n",
        "    else:\n",
        "        rotation_deg_min = (-1) * int(rotation_deg)\n",
        "        rotation_deg_max = int(rotation_deg)\n",
        "\n",
        "    # shear (min/max values)\n",
        "    if is_minmax_tuple(shear_deg):\n",
        "        shear_deg_min = shear_deg[0]\n",
        "        shear_deg_max = shear_deg[1]\n",
        "    else:\n",
        "        shear_deg_min = (-1) * int(shear_deg)\n",
        "        shear_deg_max = int(shear_deg)\n",
        "\n",
        "    # translation x-axis (min/max values)\n",
        "    if is_minmax_tuple(translation_x_px):\n",
        "        translation_x_px_min = translation_x_px[0]\n",
        "        translation_x_px_max = translation_x_px[1]\n",
        "    else:\n",
        "        translation_x_px_min = (-1) * translation_x_px\n",
        "        translation_x_px_max = translation_x_px\n",
        "\n",
        "    # translation y-axis (min/max values)\n",
        "    if is_minmax_tuple(translation_y_px):\n",
        "        translation_y_px_min = translation_y_px[0]\n",
        "        translation_y_px_max = translation_y_px[1]\n",
        "    else:\n",
        "        translation_y_px_min = (-1) * translation_y_px\n",
        "        translation_y_px_max = translation_y_px\n",
        "\n",
        "    # create nb_matrices randomized affine transformation matrices\n",
        "    for _ in range(nb_matrices):\n",
        "        # generate random values for scaling, rotation, shear, translation\n",
        "        scale_x = random.uniform(scale_x_min, scale_x_max)\n",
        "        scale_y = random.uniform(scale_y_min, scale_y_max)\n",
        "        if not scale_axis_equally:\n",
        "            scale_y = random.uniform(scale_y_min, scale_y_max)\n",
        "        else:\n",
        "            scale_y = scale_x\n",
        "        rotation = np.deg2rad(random.randint(rotation_deg_min, rotation_deg_max))\n",
        "        shear = np.deg2rad(random.randint(shear_deg_min, shear_deg_max))\n",
        "        translation_x = random.randint(translation_x_px_min, translation_x_px_max)\n",
        "        translation_y = random.randint(translation_y_px_min, translation_y_px_max)\n",
        "\n",
        "        # create three affine transformation matrices\n",
        "        # 1st one moves the image to the top left, 2nd one transforms it, 3rd one\n",
        "        # moves it back to the center.\n",
        "        # The movement is neccessary, because rotation is applied to the top left\n",
        "        # and not to the image's center (same for scaling and shear).\n",
        "        matrix_to_topleft = tf.SimilarityTransform(translation=[-shift_x, -shift_y])\n",
        "        matrix_transforms = tf.AffineTransform(scale=(scale_x, scale_y),\n",
        "                                               rotation=rotation, shear=shear,\n",
        "                                               translation=(translation_x,\n",
        "                                                            translation_y))\n",
        "        matrix_to_center = tf.SimilarityTransform(translation=[shift_x, shift_y])\n",
        "\n",
        "        # Combine the three matrices to one affine transformation (one matrix)\n",
        "        matrix = matrix_to_topleft + matrix_transforms + matrix_to_center\n",
        "\n",
        "        # one matrix is ready, add it to the result\n",
        "        result.append(matrix.inverse)\n",
        "\n",
        "    return result\n",
        "\n",
        "def apply_aug_matrices(images, matrices, transform_channels_equally=True,\n",
        "                       channel_is_first_axis=False, random_order=True,\n",
        "                       mode=\"constant\", cval=0.0, interpolation_order=1,\n",
        "                       seed=None):\n",
        "    \"\"\"Augment the given images using the given augmentation matrices.\n",
        "\n",
        "    This function is a wrapper around scikit-image's transform.warp().\n",
        "    It is expected to be called by ImageAugmenter.augment_batch().\n",
        "    The matrices may be generated by create_aug_matrices().\n",
        "\n",
        "    Args:\n",
        "        images: Same as in ImageAugmenter.augment_batch().\n",
        "            Numpy array (dtype: uint8, i.e. values 0-255) with the images.\n",
        "            Expected shape is either (image-index, height, width) for\n",
        "            grayscale images or (image-index, channel, height, width) for\n",
        "            images with channels (e.g. RGB) where the channel has the first\n",
        "            index or (image-index, height, width, channel) for images with\n",
        "            channels, where the channel is the last index.\n",
        "            If your shape is (image-index, channel, width, height) then\n",
        "            you must also set channel_is_first_axis=True in the constructor.\n",
        "        matrices: A list of augmentation matrices as produced by\n",
        "            create_aug_matrices().\n",
        "        transform_channels_equally: Same as in ImageAugmenter.__init__().\n",
        "            Whether to apply the exactly same\n",
        "            transformations to each channel of an image (True). Setting\n",
        "            it to False allows different transformations per channel,\n",
        "            e.g. the red-channel might be rotated by +20 degrees, while\n",
        "            the blue channel (of the same image) might be rotated\n",
        "            by -5 degrees. If you don't have any channels (2D grayscale),\n",
        "            you can simply ignore this setting.\n",
        "            Default is True (transform all equally).\n",
        "        channel_is_first_axis: Same as in ImageAugmenter.__init__().\n",
        "            Whether the channel (e.g. RGB) is the first\n",
        "            axis of each image (True) or the last axis (False).\n",
        "            False matches the scipy and PIL implementation and is the\n",
        "            default. If your images are 2D-grayscale then you can ignore\n",
        "            this setting (as the augmenter will ignore it too).\n",
        "        random_order: Whether to apply the augmentation matrices in a random\n",
        "            order (True, e.g. the 2nd matrix might be applied to the\n",
        "            5th image) or in the given order (False, e.g. the 2nd matrix might\n",
        "            be applied to the 2nd image).\n",
        "            Notice that for multi-channel images (e.g. RGB) this function\n",
        "            will use a different matrix for each channel, unless\n",
        "            transform_channels_equally is set to True.\n",
        "        mode: Parameter used for the transform.warp-function of scikit-image.\n",
        "            Can usually be ignored.\n",
        "        cval: Parameter used for the transform.warp-function of scikit-image.\n",
        "            Defines the fill color for \"new\" pixels, e.g. for empty areas\n",
        "            after rotations. (0.0 is black, 1.0 is white.)\n",
        "        interpolation_order: Parameter used for the transform.warp-function of\n",
        "            scikit-image. Defines the order of all interpolations used to\n",
        "            generate the new/augmented image. See their documentation for\n",
        "            further details.\n",
        "        seed: Seed to use for python's and numpy's random functions.\n",
        "    \"\"\"\n",
        "    # images must be numpy array\n",
        "    assert type(images).__module__ == np.__name__, \"Expected numpy array for \" \\\n",
        "                                                   \"parameter 'images'.\"\n",
        "\n",
        "    # images must have uint8 as dtype (0-255)\n",
        "    assert images.dtype.name == \"uint8\", \"Expected numpy.uint8 as image dtype.\"\n",
        "\n",
        "    # 3 axis total (2 per image) for grayscale,\n",
        "    # 4 axis total (3 per image) for RGB (usually)\n",
        "    assert len(images.shape) in [3, 4], \"\"\"Expected 'images' parameter to have\n",
        "        either shape (image index, y, x) for greyscale\n",
        "        or (image index, channel, y, x) / (image index, y, x, channel)\n",
        "        for multi-channel (usually color) images.\"\"\"\n",
        "\n",
        "    if seed:\n",
        "        np.random.seed(seed)\n",
        "\n",
        "    nb_images = images.shape[0]\n",
        "\n",
        "    # estimate number of channels, set to 1 if there is no axis channel,\n",
        "    # otherwise it will usually be 3\n",
        "    has_channels = False\n",
        "    nb_channels = 1\n",
        "    if len(images.shape) == 4:\n",
        "        has_channels = True\n",
        "        if channel_is_first_axis:\n",
        "            nb_channels = images.shape[1] # first axis within each image\n",
        "        else:\n",
        "            nb_channels = images.shape[3] # last axis within each image\n",
        "\n",
        "    # whether to apply the transformations directly to the whole image\n",
        "    # array (True) or for each channel individually (False)\n",
        "    apply_directly = not has_channels or (transform_channels_equally\n",
        "                                          and not channel_is_first_axis)\n",
        "\n",
        "    # We generate here the order in which the matrices may be applied.\n",
        "    # At the end, order_indices will contain the index of the matrix to use\n",
        "    # for each image, e.g. [15, 2] would mean, that the 15th matrix will be\n",
        "    # applied to the 0th image, the 2nd matrix to the 1st image.\n",
        "    # If the images gave multiple channels (e.g. RGB) and\n",
        "    # transform_channels_equally has been set to False, we will need one\n",
        "    # matrix per channel instead of per image.\n",
        "\n",
        "    # 0 to nb_images, but restart at 0 if index is beyond number of matrices\n",
        "    len_indices = nb_images if apply_directly else nb_images * nb_channels\n",
        "    if random_order:\n",
        "        # Notice: This way to choose random matrices is concise, but can create\n",
        "        # problems if there is a low amount of images and matrices.\n",
        "        # E.g. suppose that 2 images are ought to be transformed by either\n",
        "        # 0px translation on the x-axis or 1px translation. So 50% of all\n",
        "        # matrices translate by 0px and 50% by 1px. The following method\n",
        "        # will randomly choose a combination of the two matrices for the\n",
        "        # two images (matrix 0 for image 0 and matrix 0 for image 1,\n",
        "        # matrix 0 for image 0 and matrix 1 for image 1, ...).\n",
        "        # In 50% of these cases, a different matrix will be chosen for image 0\n",
        "        # and image 1 (matrices 0, 1 or matrices 1, 0). But 50% of these\n",
        "        # \"different\" matrices (different index) will be the same, as 50%\n",
        "        # translate by 1px and 50% by 0px. As a result, 75% of all augmentations\n",
        "        # will transform both images in the same way.\n",
        "        # The effect decreases if more matrices or images are chosen.\n",
        "        order_indices = np.random.random_integers(0, len(matrices) - 1, len_indices)\n",
        "    else:\n",
        "        # monotonously growing indexes (each by +1), but none of them may be\n",
        "        # higher than or equal to the number of matrices\n",
        "        order_indices = np.arange(0, len_indices) % len(matrices)\n",
        "\n",
        "    result = np.zeros(images.shape, dtype=np.float32)\n",
        "    matrix_number = 0\n",
        "\n",
        "    # iterate over every image, find out which matrix to apply and then use\n",
        "    # that matrix to augment the image\n",
        "    for img_idx, image in enumerate(images):\n",
        "        if apply_directly:\n",
        "            # we can apply the matrix to the whole numpy array of the image\n",
        "            # at the same time, so we do that to save time (instead of eg. three\n",
        "            # steps for three channels as in the else-part)\n",
        "            matrix = matrices[order_indices[matrix_number]]\n",
        "            result[img_idx, ...] = tf.warp(image, matrix, mode=mode, cval=cval,\n",
        "                                           order=interpolation_order)\n",
        "            matrix_number += 1\n",
        "        else:\n",
        "            # we cant apply the matrix to the whole image in one step, instead\n",
        "            # we have to apply it to each channel individually. that happens\n",
        "            # if the channel is the first axis of each image (incompatible with\n",
        "            # tf.warp()) or if it was explicitly requested via\n",
        "            # transform_channels_equally=False.\n",
        "            for channel_idx in range(nb_channels):\n",
        "                matrix = matrices[order_indices[matrix_number]]\n",
        "                if channel_is_first_axis:\n",
        "                    warped = tf.warp(image[channel_idx], matrix, mode=mode,\n",
        "                                     cval=cval, order=interpolation_order)\n",
        "                    result[img_idx, channel_idx, ...] = warped\n",
        "                else:\n",
        "                    warped = tf.warp(image[..., channel_idx], matrix, mode=mode,\n",
        "                                     cval=cval, order=interpolation_order)\n",
        "                    result[img_idx, ..., channel_idx] = warped\n",
        "\n",
        "                if not transform_channels_equally:\n",
        "                    matrix_number += 1\n",
        "            if transform_channels_equally:\n",
        "                matrix_number += 1\n",
        "\n",
        "    return result\n",
        "\n",
        "class ImageAugmenter(object):\n",
        "    \"\"\"Helper class to randomly augment images, usually for neural networks.\n",
        "\n",
        "    Example usage:\n",
        "        img_width = 32 # width of the images\n",
        "        img_height = 32 # height of the images\n",
        "        images = ... # e.g. load via scipy.misc.imload(filename)\n",
        "\n",
        "        # For each image: randomly flip it horizontally (50% chance),\n",
        "        # randomly rotate it between -20 and +20 degrees, randomly translate\n",
        "        # it on the x-axis between -5 and +5 pixel.\n",
        "        ia = ImageAugmenter(img_width, img_height, hlip=True, rotation_deg=20,\n",
        "                            translation_x_px=5)\n",
        "        augmented_images = ia.augment_batch(images)\n",
        "    \"\"\"\n",
        "    def __init__(self, img_width_px, img_height_px, channel_is_first_axis=False,\n",
        "                 hflip=False, vflip=False,\n",
        "                 scale_to_percent=1.0, scale_axis_equally=False,\n",
        "                 rotation_deg=0, shear_deg=0,\n",
        "                 translation_x_px=0, translation_y_px=0,\n",
        "                 transform_channels_equally=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img_width_px: The intended width of each image in pixels.\n",
        "            img_height_px: The intended height of each image in pixels.\n",
        "            channel_is_first_axis: Whether the channel (e.g. RGB) is the first\n",
        "                axis of each image (True) or the last axis (False).\n",
        "                False matches the scipy and PIL implementation and is the\n",
        "                default. If your images are 2D-grayscale then you can ignore\n",
        "                this setting (as the augmenter will ignore it too).\n",
        "            hflip: Whether to randomly flip images horizontally (on the y-axis).\n",
        "                You may choose either False (no horizontal flipping),\n",
        "                True (flip with probability 0.5) or use a float\n",
        "                value (probability) between 0.0 and 1.0. Default is False.\n",
        "            vflip: Whether to randomly flip images vertically (on the x-axis).\n",
        "                You may choose either False (no vertical flipping),\n",
        "                True (flip with probability 0.5) or use a float\n",
        "                value (probability) between 0.0 and 1.0. Default is False.\n",
        "            scale_to_percent: Up to which percentage the images may be\n",
        "                scaled/zoomed. The negative scaling is automatically derived\n",
        "                from this value. A value of 1.1 allows scaling by any value\n",
        "                between -10% and +10%. You may set min and max values yourself\n",
        "                by using a tuple instead, like (1.1, 1.2) to scale between\n",
        "                +10% and +20%. Default is 1.0 (no scaling).\n",
        "            scale_axis_equally: Whether to always scale both axis (x and y)\n",
        "                in the same way. If set to False, then e.g. the Augmenter\n",
        "                might scale the x-axis by 20% and the y-axis by -5%.\n",
        "                Default is False.\n",
        "            rotation_deg: By how much the image may be rotated around its\n",
        "                center (in degrees). The negative rotation will automatically\n",
        "                be derived from this value. E.g. a value of 20 allows any\n",
        "                rotation between -20 degrees and +20 degrees. You may set min\n",
        "                and max values yourself by using a tuple instead, e.g. (5, 20)\n",
        "                to rotate between +5 und +20 degrees. Default is 0 (no\n",
        "                rotation).\n",
        "            shear_deg: By how much the image may be sheared (in degrees). The\n",
        "                negative value will automatically be derived from this value.\n",
        "                E.g. a value of 20 allows any shear between -20 degrees and\n",
        "                +20 degrees. You may set min and max values yourself by using a\n",
        "                tuple instead, e.g. (5, 20) to shear between +5 und +20\n",
        "                degrees. Default is 0 (no shear).\n",
        "            translation_x_px: By up to how many pixels the image may be\n",
        "                translated (moved) on the x-axis. The negative value will\n",
        "                automatically be derived from this value. E.g. a value of +7\n",
        "                allows any translation between -7 and +7 pixels on the x-axis.\n",
        "                You may set min and max values yourself by using a tuple\n",
        "                instead, e.g. (5, 20) to translate between +5 und +20 pixels.\n",
        "                Default is 0 (no translation on the x-axis).\n",
        "            translation_y_px: See translation_x_px, just for the y-axis.\n",
        "            transform_channels_equally: Whether to apply the exactly same\n",
        "                transformations to each channel of an image (True). Setting\n",
        "                it to False allows different transformations per channel,\n",
        "                e.g. the red-channel might be rotated by +20 degrees, while\n",
        "                the blue channel (of the same image) might be rotated\n",
        "                by -5 degrees. If you don't have any channels (2D grayscale),\n",
        "                you can simply ignore this setting.\n",
        "                Default is True (transform all equally).\n",
        "        \"\"\"\n",
        "        self.img_width_px = img_width_px\n",
        "        self.img_height_px = img_height_px\n",
        "        self.channel_is_first_axis = channel_is_first_axis\n",
        "\n",
        "        self.hflip_prob = 0.0\n",
        "        # note: we have to check first for floats, otherwise \"hflip == True\"\n",
        "        # will evaluate to true if hflip is 1.0. So chosing 1.0 (100%) would\n",
        "        # result in hflip_prob to be set to 0.5 (50%).\n",
        "        if isinstance(hflip, float):\n",
        "            assert hflip >= 0.0 and hflip <= 1.0\n",
        "            self.hflip_prob = hflip\n",
        "        elif hflip == True:\n",
        "            self.hflip_prob = 0.5\n",
        "        elif hflip == False:\n",
        "            self.hflip_prob = 0.0\n",
        "        else:\n",
        "            raise Exception(\"Unexpected value for parameter 'hflip'.\")\n",
        "\n",
        "        self.vflip_prob = 0.0\n",
        "        if isinstance(vflip, float):\n",
        "            assert vflip >= 0.0 and vflip <= 1.0\n",
        "            self.vflip_prob = vflip\n",
        "        elif vflip == True:\n",
        "            self.vflip_prob = 0.5\n",
        "        elif vflip == False:\n",
        "            self.vflip_prob = 0.0\n",
        "        else:\n",
        "            raise Exception(\"Unexpected value for parameter 'vflip'.\")\n",
        "\n",
        "        self.scale_to_percent = scale_to_percent\n",
        "        self.scale_axis_equally = scale_axis_equally\n",
        "        self.rotation_deg = rotation_deg\n",
        "        self.shear_deg = shear_deg\n",
        "        self.translation_x_px = translation_x_px\n",
        "        self.translation_y_px = translation_y_px\n",
        "        self.transform_channels_equally = transform_channels_equally\n",
        "        self.cval = 0.0\n",
        "        self.interpolation_order = 1\n",
        "        self.pregenerated_matrices = None\n",
        "\n",
        "    def pregenerate_matrices(self, nb_matrices, seed=None):\n",
        "        \"\"\"Pregenerate/cache augmentation matrices.\n",
        "\n",
        "        If matrices are pregenerated, augment_batch() will reuse them on\n",
        "        each call. The augmentations will not always be the same,\n",
        "        as the order of the matrices will be randomized (when\n",
        "        they are applied to the images). The requirement for that is though\n",
        "        that you pregenerate enough of them (e.g. a couple thousand).\n",
        "\n",
        "        Note that generating the augmentation matrices is usually fast\n",
        "        and only starts to make sense if you process millions of small images\n",
        "        or many tens of thousands of big images.\n",
        "\n",
        "        Each call to this method results in pregenerating a new set of matrices,\n",
        "        e.g. to replace a list of matrices that has been used often enough.\n",
        "\n",
        "        Calling this method with nb_matrices set to 0 will remove the\n",
        "        pregenerated matrices and augment_batch() returns to its default\n",
        "        behaviour of generating new matrices on each call.\n",
        "\n",
        "        Args:\n",
        "            nb_matrices: The number of matrices to pregenerate. E.g. a few\n",
        "                thousand. If set to 0, the matrices will be generated again on\n",
        "                each call of augment_batch().\n",
        "            seed: A random seed to use.\n",
        "        \"\"\"\n",
        "        assert nb_matrices >= 0\n",
        "        if nb_matrices == 0:\n",
        "            self.pregenerated_matrices = None\n",
        "        else:\n",
        "            matrices = create_aug_matrices(nb_matrices,\n",
        "                                           self.img_width_px,\n",
        "                                           self.img_height_px,\n",
        "                                           scale_to_percent=self.scale_to_percent,\n",
        "                                           scale_axis_equally=self.scale_axis_equally,\n",
        "                                           rotation_deg=self.rotation_deg,\n",
        "                                           shear_deg=self.shear_deg,\n",
        "                                           translation_x_px=self.translation_x_px,\n",
        "                                           translation_y_px=self.translation_y_px,\n",
        "                                           seed=seed)\n",
        "            self.pregenerated_matrices = matrices\n",
        "\n",
        "    def augment_batch(self, images, seed=None):\n",
        "        \"\"\"Augments a batch of images.\n",
        "\n",
        "        Applies all settings (rotation, shear, translation, ...) that\n",
        "        have been chosen in the constructor.\n",
        "\n",
        "        Args:\n",
        "            images: Numpy array (dtype: uint8, i.e. values 0-255) with the images.\n",
        "                Expected shape is either (image-index, height, width) for\n",
        "                grayscale images or (image-index, channel, height, width) for\n",
        "                images with channels (e.g. RGB) where the channel has the first\n",
        "                index or (image-index, height, width, channel) for images with\n",
        "                channels, where the channel is the last index.\n",
        "                If your shape is (image-index, channel, width, height) then\n",
        "                you must also set channel_is_first_axis=True in the constructor.\n",
        "            seed: Seed to use for python's and numpy's random functions.\n",
        "                Default is None (dont use a seed).\n",
        "\n",
        "        Returns:\n",
        "            Augmented images as numpy array of dtype float32 (i.e. values\n",
        "            are between 0.0 and 1.0).\n",
        "        \"\"\"\n",
        "        shape = images.shape\n",
        "        nb_channels = 0\n",
        "        if len(shape) == 3:\n",
        "            # shape like (image_index, y-axis, x-axis)\n",
        "            assert shape[1] == self.img_height_px\n",
        "            assert shape[2] == self.img_width_px\n",
        "            nb_channels = 1\n",
        "        elif len(shape) == 4:\n",
        "            if not self.channel_is_first_axis:\n",
        "                # shape like (image-index, y-axis, x-axis, channel-index)\n",
        "                assert shape[1] == self.img_height_px\n",
        "                assert shape[2] == self.img_width_px\n",
        "                nb_channels = shape[3]\n",
        "            else:\n",
        "                # shape like (image-index, channel-index, y-axis, x-axis)\n",
        "                assert shape[2] == self.img_height_px\n",
        "                assert shape[3] == self.img_width_px\n",
        "                nb_channels = shape[1]\n",
        "        else:\n",
        "            msg = \"Mismatch between images shape %s and \" \\\n",
        "                  \"predefined image width/height (%d/%d).\"\n",
        "            raise Exception(msg % (str(shape), self.img_width_px, self.img_height_px))\n",
        "\n",
        "        if seed:\n",
        "            random.seed(seed)\n",
        "            np.random.seed(seed)\n",
        "\n",
        "        # --------------------------------\n",
        "        # horizontal and vertical flipping/mirroring\n",
        "        # --------------------------------\n",
        "        if self.hflip_prob > 0 or self.vflip_prob > 0:\n",
        "            # TODO this currently ignores the setting in\n",
        "            # transform_channels_equally and will instead always flip all\n",
        "            # channels equally\n",
        "\n",
        "            # if this is simply a view, then the input array gets flipped too\n",
        "            # for some reason\n",
        "            images_flipped = np.copy(images)\n",
        "            #images_flipped = images.view()\n",
        "\n",
        "            if len(shape) == 4 and self.channel_is_first_axis:\n",
        "                # roll channel to the last axis\n",
        "                # swapaxes doesnt work here, because\n",
        "                #  (image index, channel, y, x)\n",
        "                # would be turned into\n",
        "                #  (image index, x, y, channel)\n",
        "                # and y needs to come before x\n",
        "                images_flipped = np.rollaxis(images_flipped, 1, 4)\n",
        "\n",
        "            y_p = self.hflip_prob\n",
        "            x_p = self.vflip_prob\n",
        "            batch_size = images.shape[0] // 2\n",
        "            for i in range(batch_size):\n",
        "                if y_p > 0 and random.random() < y_p:\n",
        "                    images_flipped[i] = np.fliplr(images_flipped[i])\n",
        "                    images_flipped[i+batch_size] = np.fliplr(images_flipped[i+batch_size])\n",
        "                if x_p > 0 and random.random() < x_p:\n",
        "                    images_flipped[i] = np.flipud(images_flipped[i])\n",
        "                    images_flipped[i+batch_size] = np.flipud(images_flipped[i+batch_size])\n",
        "\n",
        "            if len(shape) == 4 and self.channel_is_first_axis:\n",
        "                # roll channel back to the second axis (index 1)\n",
        "                images_flipped = np.rollaxis(images_flipped, 3, 1)\n",
        "            images = images_flipped\n",
        "\n",
        "        # --------------------------------\n",
        "        # if no augmentation has been chosen, stop early\n",
        "        # for improved performance (evade applying matrices)\n",
        "        # --------------------------------\n",
        "        if self.pregenerated_matrices is None \\\n",
        "           and self.scale_to_percent == 1.0 and self.rotation_deg == 0 \\\n",
        "           and self.shear_deg == 0 \\\n",
        "           and self.translation_x_px == 0 and self.translation_y_px == 0:\n",
        "            return np.array(images, dtype=np.float32) / 255\n",
        "\n",
        "        # --------------------------------\n",
        "        # generate transformation matrices\n",
        "        # --------------------------------\n",
        "        if self.pregenerated_matrices is not None:\n",
        "            matrices = self.pregenerated_matrices\n",
        "        else:\n",
        "            # estimate the number of matrices required\n",
        "            if self.transform_channels_equally:\n",
        "                nb_matrices = shape[0]\n",
        "            else:\n",
        "                nb_matrices = shape[0] * nb_channels\n",
        "\n",
        "            # generate matrices\n",
        "            matrices = create_aug_matrices(nb_matrices,\n",
        "                                           self.img_width_px,\n",
        "                                           self.img_height_px,\n",
        "                                           scale_to_percent=self.scale_to_percent,\n",
        "                                           scale_axis_equally=self.scale_axis_equally,\n",
        "                                           rotation_deg=self.rotation_deg,\n",
        "                                           shear_deg=self.shear_deg,\n",
        "                                           translation_x_px=self.translation_x_px,\n",
        "                                           translation_y_px=self.translation_y_px,\n",
        "                                           seed=seed)\n",
        "\n",
        "        # --------------------------------\n",
        "        # apply transformation matrices (i.e. augment images)\n",
        "        # --------------------------------\n",
        "        return apply_aug_matrices(images, matrices,\n",
        "                                  transform_channels_equally=self.transform_channels_equally,\n",
        "                                  channel_is_first_axis=self.channel_is_first_axis,\n",
        "                                  cval=self.cval, interpolation_order=self.interpolation_order,\n",
        "                                  seed=seed)\n",
        "\n",
        "    def plot_image(self, image, nb_repeat=40, show_plot=True):\n",
        "        \"\"\"Plot augmented variations of an image.\n",
        "\n",
        "        This method takes an image and plots it by default in 40 differently\n",
        "        augmented versions.\n",
        "\n",
        "        This method is intended to visualize the strength of your chosen\n",
        "        augmentations (so for debugging).\n",
        "\n",
        "        Args:\n",
        "            image: The image to plot.\n",
        "            nb_repeat: How often to plot the image. Each time it is plotted,\n",
        "                the chosen augmentation will be different. (Default: 40).\n",
        "            show_plot: Whether to show the plot. False makes sense if you\n",
        "                don't have a graphical user interface on the machine.\n",
        "                (Default: True)\n",
        "\n",
        "        Returns:\n",
        "            The figure of the plot.\n",
        "            Use figure.savefig() to save the image.\n",
        "        \"\"\"\n",
        "        if len(image.shape) == 2:\n",
        "            images = np.resize(image, (nb_repeat, image.shape[0], image.shape[1]))\n",
        "        else:\n",
        "            images = np.resize(image, (nb_repeat, image.shape[0], image.shape[1],\n",
        "                               image.shape[2]))\n",
        "        return self.plot_images(images, True, show_plot=show_plot)\n",
        "\n",
        "    def plot_images(self, images, augment, show_plot=True, figure=None):\n",
        "        \"\"\"Plot augmented variations of images.\n",
        "\n",
        "        The images will all be shown in the same window.\n",
        "        It is recommended to not plot too many of them (i.e. stay below 100).\n",
        "\n",
        "        This method is intended to visualize the strength of your chosen\n",
        "        augmentations (so for debugging).\n",
        "\n",
        "        Args:\n",
        "            images: A numpy array of images. See augment_batch().\n",
        "            augment: Whether to augment the images (True) or just display\n",
        "                them in the way they are (False).\n",
        "            show_plot: Whether to show the plot. False makes sense if you\n",
        "                don't have a graphical user interface on the machine.\n",
        "                (Default: True)\n",
        "            figure: The figure of the plot in which to draw the images.\n",
        "                Provide the return value of this function (from a prior call)\n",
        "                to draw in the same plot window again. Chosing 'None' will\n",
        "                create a new figure. (Default is None.)\n",
        "\n",
        "        Returns:\n",
        "            The figure of the plot.\n",
        "            Use figure.savefig() to save the image.\n",
        "        \"\"\"\n",
        "        import matplotlib.pyplot as plt\n",
        "        import matplotlib.cm as cm\n",
        "\n",
        "        if augment:\n",
        "            images = self.augment_batch(images)\n",
        "\n",
        "        # (Lists of) Grayscale images have the shape (image index, y, x)\n",
        "        # Multi-Channel images therefore must have 4 or more axes here\n",
        "        if len(images.shape) >= 4:\n",
        "            # The color-channel is expected to be the last axis by matplotlib\n",
        "            # therefore exchange the axes, if its the first one here\n",
        "            if self.channel_is_first_axis:\n",
        "                images = np.rollaxis(images, 1, 4)\n",
        "\n",
        "        nb_cols = 10\n",
        "        nb_rows = 1 + int(images.shape[0] / nb_cols)\n",
        "        if figure is not None:\n",
        "            fig = figure\n",
        "            plt.figure(fig.number)\n",
        "            fig.clear()\n",
        "        else:\n",
        "            fig = plt.figure(figsize=(10, 10))\n",
        "\n",
        "        for i, image in enumerate(images):\n",
        "            image = images[i]\n",
        "\n",
        "            plot_number = i + 1\n",
        "            ax = fig.add_subplot(nb_rows, nb_cols, plot_number, xticklabels=[],\n",
        "                                 yticklabels=[])\n",
        "            ax.set_axis_off()\n",
        "            # \"cmap\" should restrict the color map to grayscale, but strangely\n",
        "            # also works well with color images\n",
        "            imgplot = plt.imshow(image, cmap=cm.Greys_r, aspect=\"equal\")\n",
        "\n",
        "        # not showing the plot might be useful e.g. on clusters\n",
        "        if show_plot:\n",
        "            plt.show()\n",
        "\n",
        "        return fig"
      ],
      "metadata": {
        "id": "wSkCUJ-i6HCw"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Batcher\n",
        "\"\"\"\n",
        "taken and modified from https://github.com/pranv/ARC\n",
        "\"\"\"\n",
        "\n",
        "use_cuda = False\n",
        "\n",
        "\n",
        "class Omniglot(object):\n",
        "    def __init__(self, path=os.path.join('/content/drive/MyDrive/Omniglot_ARC', 'omniglot.npy'), batch_size=128, image_size=32):\n",
        "        \"\"\"\n",
        "        batch_size: the output is (2 * batch size, 1, image_size, image_size)\n",
        "                    X[i] & X[i + batch_size] are the pair\n",
        "        image_size: size of the image\n",
        "        data_split: in number of alphabets, e.g. [30, 10] means out of 50 Omniglot characters,\n",
        "                    30 is for training, 10 for validation and the remaining(10) for testing\n",
        "        within_alphabet: for verfication task, when 2 characters are sampled to form a pair,\n",
        "                        this flag specifies if should they be from the same alphabet/language\n",
        "        ---------------------\n",
        "        Data Augmentation Parameters:\n",
        "            flip: here flipping both the images in a pair\n",
        "            scale: x would scale image by + or - x%\n",
        "            rotation_deg\n",
        "            shear_deg\n",
        "            translation_px: in both x and y directions\n",
        "        \"\"\"\n",
        "        chars = np.load(path)\n",
        "\n",
        "        # resize the images\n",
        "        resized_chars = np.zeros((1623, 20, image_size, image_size), dtype='uint8')\n",
        "        for i in range(1623):\n",
        "            for j in range(20):\n",
        "                resized_chars[i, j] = resize(chars[i, j], (image_size, image_size))\n",
        "        chars = resized_chars\n",
        "\n",
        "        self.mean_pixel = chars.mean() / 255.0  # used later for mean subtraction\n",
        "\n",
        "        # starting index of each alphabet in a list of chars\n",
        "        a_start = [0, 20, 49, 75, 116, 156, 180, 226, 240, 266, 300, 333, 355, 381,\n",
        "                   424, 448, 496, 518, 534, 586, 633, 673, 699, 739, 780, 813,\n",
        "                   827, 869, 892, 909, 964, 984, 1010, 1036, 1062, 1088, 1114,\n",
        "                   1159, 1204, 1245, 1271, 1318, 1358, 1388, 1433, 1479, 1507,\n",
        "                   1530, 1555, 1597]\n",
        "\n",
        "        # size of each alphabet (num of chars)\n",
        "        a_size = [20, 29, 26, 41, 40, 24, 46, 14, 26, 34, 33, 22, 26, 43, 24, 48, 22,\n",
        "                  16, 52, 47, 40, 26, 40, 41, 33, 14, 42, 23, 17, 55, 20, 26, 26, 26,\n",
        "                  26, 26, 45, 45, 41, 26, 47, 40, 30, 45, 46, 28, 23, 25, 42, 26]\n",
        "\n",
        "        # each alphabet/language has different number of characters.\n",
        "        # in order to uniformly sample all characters, we need weigh the probability\n",
        "        # of sampling a alphabet by its size. p is that probability\n",
        "        def size2p(size):\n",
        "            s = np.array(size).astype('float64')\n",
        "            return s / s.sum()\n",
        "\n",
        "        self.size2p = size2p\n",
        "\n",
        "        self.data = chars\n",
        "        self.a_start = a_start\n",
        "        self.a_size = a_size\n",
        "        self.image_size = image_size\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        flip = True\n",
        "        scale = 0.2\n",
        "        rotation_deg = 20\n",
        "        shear_deg = 10\n",
        "        translation_px = 5\n",
        "        self.augmentor = ImageAugmenter(image_size, image_size,\n",
        "                                        hflip=flip, vflip=flip,\n",
        "                                        scale_to_percent=1.0 + scale, rotation_deg=rotation_deg, shear_deg=shear_deg,\n",
        "                                        translation_x_px=translation_px, translation_y_px=translation_px)\n",
        "\n",
        "    def fetch_batch(self, part):\n",
        "        \"\"\"\n",
        "            This outputs batch_size number of pairs\n",
        "            Thus the actual number of images outputted is 2 * batch_size\n",
        "            Say A & B form the half of a pair\n",
        "            The Batch is divided into 4 parts:\n",
        "                Dissimilar A \t\tDissimilar B\n",
        "                Similar A \t\t\tSimilar B\n",
        "\n",
        "            Corresponding images in Similar A and Similar B form the similar pair\n",
        "            similarly, Dissimilar A and Dissimilar B form the dissimilar pair\n",
        "\n",
        "            When flattened, the batch has 4 parts with indices:\n",
        "                Dissimilar A \t\t0 - batch_size / 2\n",
        "                Similar A    \t\tbatch_size / 2  - batch_size\n",
        "                Dissimilar B \t\tbatch_size  - 3 * batch_size / 2\n",
        "                Similar B \t\t\t3 * batch_size / 2 - batch_size\n",
        "\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "\n",
        "class Batcher(Omniglot):\n",
        "    def __init__(self, path=os.path.join('/content/drive/MyDrive/Omniglot_ARC', 'omniglot.npy'), batch_size=128, image_size=32):\n",
        "        Omniglot.__init__(self, path, batch_size, image_size)\n",
        "\n",
        "        a_start = self.a_start\n",
        "        a_size = self.a_size\n",
        "\n",
        "        # slicing indices for splitting a_start & a_size\n",
        "        i = 20\n",
        "        j = 30\n",
        "        starts = {}\n",
        "        starts['train'], starts['val'], starts['test'] = a_start[:i], a_start[i:j], a_start[j:]\n",
        "        sizes = {}\n",
        "        sizes['train'], sizes['val'], sizes['test'] = a_size[:i], a_size[i:j], a_size[j:]\n",
        "\n",
        "        size2p = self.size2p\n",
        "\n",
        "        p = {}\n",
        "        p['train'], p['val'], p['test'] = size2p(sizes['train']), size2p(sizes['val']), size2p(sizes['test'])\n",
        "\n",
        "        self.starts = starts\n",
        "        self.sizes = sizes\n",
        "        self.p = p\n",
        "\n",
        "    def fetch_batch(self, part, batch_size: int = None):\n",
        "\n",
        "        if batch_size is None:\n",
        "            batch_size = self.batch_size\n",
        "\n",
        "        X, Y = self._fetch_batch(part, batch_size)\n",
        "\n",
        "        X = Variable(torch.from_numpy(X)).view(2*batch_size, self.image_size, self.image_size)\n",
        "\n",
        "        X1 = X[:batch_size]  # (B, h, w)\n",
        "        X2 = X[batch_size:]  # (B, h, w)\n",
        "\n",
        "        X = torch.stack([X1, X2], dim=1)  # (B, 2, h, w)\n",
        "\n",
        "        Y = Variable(torch.from_numpy(Y))\n",
        "\n",
        "        if use_cuda:\n",
        "            X, Y = X.cuda(), Y.cuda()\n",
        "\n",
        "        return X, Y\n",
        "\n",
        "    def _fetch_batch(self, part, batch_size: int = None):\n",
        "        if batch_size is None:\n",
        "            batch_size = self.batch_size\n",
        "\n",
        "        data = self.data\n",
        "        starts = self.starts[part]\n",
        "        sizes = self.sizes[part]\n",
        "        p = self.p[part]\n",
        "        image_size = self.image_size\n",
        "\n",
        "        num_alphbts = len(starts)\n",
        "\n",
        "        X = np.zeros((2 * batch_size, image_size, image_size), dtype='uint8')\n",
        "        for i in range(batch_size // 2):\n",
        "            # choose similar chars\n",
        "            same_idx = choice(range(starts[0], starts[-1] + sizes[-1]))\n",
        "\n",
        "            # choose dissimilar chars within alphabet\n",
        "            alphbt_idx = choice(num_alphbts, p=p)\n",
        "            char_offset = choice(sizes[alphbt_idx], 2, replace=False)\n",
        "            diff_idx = starts[alphbt_idx] + char_offset\n",
        "\n",
        "            X[i], X[i + batch_size] = data[diff_idx, choice(20, 2)]\n",
        "            X[i + batch_size // 2], X[i + 3 * batch_size // 2] = data[same_idx, choice(20, 2, replace=False)]\n",
        "\n",
        "        y = np.zeros((batch_size, 1), dtype='int32')\n",
        "        y[:batch_size // 2] = 0\n",
        "        y[batch_size // 2:] = 1\n",
        "\n",
        "        if part == 'train':\n",
        "            X = self.augmentor.augment_batch(X)\n",
        "        else:\n",
        "            X = X / 255.0\n",
        "\n",
        "        X = X - self.mean_pixel\n",
        "        X = X[:, np.newaxis]\n",
        "        X = X.astype(\"float32\")\n",
        "\n",
        "        return X, y\n"
      ],
      "metadata": {
        "id": "Aa60Q1Pi5yyi"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model\n",
        "use_cuda = False\n",
        "\n",
        "class GlimpseWindow:\n",
        "    \"\"\"\n",
        "    Generates glimpses from images using Cauchy kernels.\n",
        "\n",
        "    Args:\n",
        "        glimpse_h (int): The height of the glimpses to be generated.\n",
        "        glimpse_w (int): The width of the glimpses to be generated.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, glimpse_h: int, glimpse_w: int):\n",
        "        self.glimpse_h = glimpse_h\n",
        "        self.glimpse_w = glimpse_w\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_filterbanks(delta_caps: Variable, center_caps: Variable, image_size: int, glimpse_size: int) -> Variable:\n",
        "        \"\"\"\n",
        "        Generates Cauchy Filter Banks along a dimension.\n",
        "\n",
        "        Args:\n",
        "            delta_caps (B,):  A batch of deltas [-1, 1]\n",
        "            center_caps (B,): A batch of [-1, 1] reals that dictate the location of center of cauchy kernel glimpse.\n",
        "            image_size (int): size of images along that dimension\n",
        "            glimpse_size (int): size of glimpses to be generated along that dimension\n",
        "\n",
        "        Returns:\n",
        "            (B, image_size, glimpse_size): A batch of filter banks\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # convert dimension sizes to float. lots of math ahead.\n",
        "        image_size = float(image_size)\n",
        "        glimpse_size = float(glimpse_size)\n",
        "\n",
        "        # scale the centers and the deltas to map to the actual size of given image.\n",
        "        centers = (image_size - 1) * (center_caps + 1) / 2.0  # (B)\n",
        "        deltas = (float(image_size) / glimpse_size) * (1.0 - torch.abs(delta_caps))\n",
        "\n",
        "        # calculate gamma for cauchy kernel\n",
        "        gammas = torch.exp(1.0 - 2 * torch.abs(delta_caps))  # (B)\n",
        "\n",
        "        # coordinate of pixels on the glimpse\n",
        "        glimpse_pixels = Variable(torch.arange(0, glimpse_size) - (glimpse_size - 1.0) / 2.0)  # (glimpse_size)\n",
        "        if use_cuda:\n",
        "            glimpse_pixels = glimpse_pixels.cuda()\n",
        "\n",
        "        # space out with delta\n",
        "        glimpse_pixels = deltas[:, None] * glimpse_pixels[None, :]  # (B, glimpse_size)\n",
        "        # center around the centers\n",
        "        glimpse_pixels = centers[:, None] + glimpse_pixels  # (B, glimpse_size)\n",
        "\n",
        "        # coordinates of pixels on the image\n",
        "        image_pixels = Variable(torch.arange(0, image_size))  # (image_size)\n",
        "        if use_cuda:\n",
        "            image_pixels = image_pixels.cuda()\n",
        "\n",
        "        fx = image_pixels - glimpse_pixels[:, :, None]  # (B, glimpse_size, image_size)\n",
        "        fx = fx / gammas[:, None, None]\n",
        "        fx = fx ** 2.0\n",
        "        fx = 1.0 + fx\n",
        "        fx = math.pi * gammas[:, None, None] * fx\n",
        "        fx = 1.0 / fx\n",
        "        fx = fx / (torch.sum(fx, dim=2) + 1e-4)[:, :, None]  # we add a small constant in the denominator division by 0.\n",
        "\n",
        "        return fx.transpose(1, 2)\n",
        "\n",
        "    def get_attention_mask(self, glimpse_params: Variable, mask_h: int, mask_w: int) -> Variable:\n",
        "        \"\"\"\n",
        "        For visualization, generate a heat map (or mask) of which pixels got the most \"attention\".\n",
        "\n",
        "        Args:\n",
        "            glimpse_params (B, hx):  A batch of glimpse parameters.\n",
        "            mask_h (int): The height of the image for which the mask is being generated.\n",
        "            mask_w (int): The width of the image for which the mask is being generated.\n",
        "\n",
        "        Returns:\n",
        "            (B, mask_h, mask_w): A batch of masks with attended pixels weighted more.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size, _ = glimpse_params.size()\n",
        "\n",
        "        # (B, image_h, glimpse_h)\n",
        "        F_h = self._get_filterbanks(delta_caps=glimpse_params[:, 2], center_caps=glimpse_params[:, 0],\n",
        "                                    image_size=mask_h, glimpse_size=self.glimpse_h)\n",
        "\n",
        "        # (B, image_w, glimpse_w)\n",
        "        F_w = self._get_filterbanks(delta_caps=glimpse_params[:, 2], center_caps=glimpse_params[:, 1],\n",
        "                                    image_size=mask_w, glimpse_size=self.glimpse_w)\n",
        "\n",
        "        # (B, glimpse_h, glimpse_w)\n",
        "        glimpse_proxy = Variable(torch.ones(batch_size, self.glimpse_h, self.glimpse_w))\n",
        "\n",
        "        # find the attention mask that lead to the glimpse.\n",
        "        mask = glimpse_proxy\n",
        "        mask = torch.bmm(F_h, mask)\n",
        "        mask = torch.bmm(mask, F_w.transpose(1, 2))\n",
        "\n",
        "        # scale to between 0 and 1.0\n",
        "        mask = mask - mask.min()\n",
        "        mask = mask / mask.max()\n",
        "        mask = mask.float()\n",
        "\n",
        "        return mask\n",
        "\n",
        "    def get_glimpse(self, images: Variable, glimpse_params: Variable) -> Variable:\n",
        "        \"\"\"\n",
        "        Generate glimpses given images and glimpse parameters. This is the main method of this class.\n",
        "\n",
        "        The glimpse parameters are (h_center, w_center, delta). (h_center, w_center)\n",
        "        represents the relative position of the center of the glimpse on the image. delta determines\n",
        "        the zoom factor of the glimpse.\n",
        "\n",
        "        Args:\n",
        "            images (B, h, w):  A batch of images\n",
        "            glimpse_params (B, 3):  A batch of glimpse parameters (h_center, w_center, delta)\n",
        "\n",
        "        Returns:\n",
        "            (B, glimpse_h, glimpse_w): A batch of glimpses.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size, image_h, image_w = images.size()\n",
        "\n",
        "        # (B, image_h, glimpse_h)\n",
        "        F_h = self._get_filterbanks(delta_caps=glimpse_params[:, 2], center_caps=glimpse_params[:, 0],\n",
        "                                    image_size=image_h, glimpse_size=self.glimpse_h)\n",
        "\n",
        "        # (B, image_w, glimpse_w)\n",
        "        F_w = self._get_filterbanks(delta_caps=glimpse_params[:, 2], center_caps=glimpse_params[:, 1],\n",
        "                                    image_size=image_w, glimpse_size=self.glimpse_w)\n",
        "\n",
        "        # F_h.T * images * F_w\n",
        "        glimpses = images\n",
        "        glimpses = torch.bmm(F_h.transpose(1, 2), glimpses)\n",
        "        glimpses = torch.bmm(glimpses, F_w)\n",
        "\n",
        "        return glimpses  # (B, glimpse_h, glimpse_w)\n",
        "\n",
        "\n",
        "class ARC(nn.Module):\n",
        "    \"\"\"\n",
        "    This class implements the Attentive Recurrent Comparators. This module has two main parts.\n",
        "\n",
        "    1.) controller: The RNN module that takes as input glimpses from a pair of images and emits a hidden state.\n",
        "\n",
        "    2.) glimpser: A Linear layer that takes the hidden state emitted by the controller and generates the glimpse\n",
        "                    parameters. These glimpse parameters are (h_center, w_center, delta). (h_center, w_center)\n",
        "                    represents the relative position of the center of the glimpse on the image. delta determines\n",
        "                    the zoom factor of the glimpse.\n",
        "\n",
        "    Args:\n",
        "        num_glimpses (int): How many glimpses must the ARC \"see\" before emitting the final hidden state.\n",
        "        glimpse_h (int): The height of the glimpse in pixels.\n",
        "        glimpse_w (int): The width of the glimpse in pixels.\n",
        "        controller_out (int): The size of the hidden state emitted by the controller.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_glimpses: int=8, glimpse_h: int=8, glimpse_w: int=8, controller_out: int=128) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_glimpses = num_glimpses\n",
        "        self.glimpse_h = glimpse_h\n",
        "        self.glimpse_w = glimpse_w\n",
        "        self.controller_out = controller_out\n",
        "\n",
        "        # main modules of ARC\n",
        "\n",
        "        self.controller = nn.LSTMCell(input_size=(glimpse_h * glimpse_w), hidden_size=self.controller_out)\n",
        "        self.glimpser = nn.Linear(in_features=self.controller_out, out_features=3)\n",
        "\n",
        "        # this will actually generate glimpses from images using the glimpse parameters.\n",
        "        self.glimpse_window = GlimpseWindow(glimpse_h=self.glimpse_h, glimpse_w=self.glimpse_w)\n",
        "\n",
        "    def forward(self, image_pairs: Variable) -> Variable:\n",
        "        \"\"\"\n",
        "        The method calls the internal _forward() method which returns hidden states for all time steps. This i\n",
        "\n",
        "        Args:\n",
        "            image_pairs (B, 2, h, w):  A batch of pairs of images\n",
        "\n",
        "        Returns:\n",
        "            (B, controller_out): A batch of final hidden states after each pair of image has been shown for num_glimpses\n",
        "            glimpses.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # return only the last hidden state\n",
        "        all_hidden = self._forward(image_pairs)  # (2*num_glimpses, B, controller_out)\n",
        "        last_hidden = all_hidden[-1, :, :]  # (B, controller_out)\n",
        "\n",
        "        return last_hidden\n",
        "\n",
        "    def _forward(self, image_pairs: Variable) -> Variable:\n",
        "        \"\"\"\n",
        "        The main forward method of ARC. But it returns hidden state from all time steps (all glimpses) as opposed to\n",
        "        just the last one. See the exposed forward() method.\n",
        "\n",
        "        Args:\n",
        "            image_pairs: (B, 2, h, w) A batch of pairs of images\n",
        "\n",
        "        Returns:\n",
        "            (2*num_glimpses, B, controller_out) Hidden states from ALL time steps.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # convert to images to float.\n",
        "        image_pairs = image_pairs.float()\n",
        "\n",
        "        # calculate the batch size\n",
        "        batch_size = image_pairs.size()[0]\n",
        "\n",
        "        # an array for collecting hidden states from each time step.\n",
        "        all_hidden = []\n",
        "\n",
        "        # initial hidden state of the LSTM.\n",
        "        Hx = Variable(torch.zeros(batch_size, self.controller_out))  # (B, controller_out)\n",
        "        Cx = Variable(torch.zeros(batch_size, self.controller_out))  # (B, controller_out)\n",
        "\n",
        "        if use_cuda:\n",
        "            Hx, Cx = Hx.cuda(), Cx.cuda()\n",
        "\n",
        "        # take `num_glimpses` glimpses for both images, alternatingly.\n",
        "        for turn in range(2*self.num_glimpses):\n",
        "            # select image to show, alternate between the first and second image in the pair\n",
        "            images_to_observe = image_pairs[:,  turn % 2]  # (B, h, w)\n",
        "\n",
        "            # choose a portion from image to glimpse using attention\n",
        "            glimpse_params = torch.tanh(self.glimpser(Hx))  # (B, 3)  a batch of glimpse params (x, y, delta)\n",
        "            glimpses = self.glimpse_window.get_glimpse(images_to_observe, glimpse_params)  # (B, glimpse_h, glimpse_w)\n",
        "            flattened_glimpses = glimpses.view(batch_size, -1)  # (B, glimpse_h * glimpse_w), one time-step\n",
        "\n",
        "            # feed the glimpses and the previous hidden state to the LSTM.\n",
        "            Hx, Cx = self.controller(flattened_glimpses, (Hx, Cx))  # (B, controller_out), (B, controller_out)\n",
        "\n",
        "            # append this hidden state to all states\n",
        "            all_hidden.append(Hx)\n",
        "\n",
        "        all_hidden = torch.stack(all_hidden)  # (2*num_glimpses, B, controller_out)\n",
        "\n",
        "        # return a batch of all hidden states.\n",
        "        return all_hidden\n",
        "\n",
        "\n",
        "class ArcBinaryClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    A binary classifier that uses ARC.\n",
        "    Given a pair of images, feeds them the ARC and uses the final hidden state of ARC to\n",
        "    classify the images as belonging to the same class or not.\n",
        "\n",
        "    Args:\n",
        "        num_glimpses (int): How many glimpses must the ARC \"see\" before emitting the final hidden state.\n",
        "        glimpse_h (int): The height of the glimpse in pixels.\n",
        "        glimpse_w (int): The width of the glimpse in pixels.\n",
        "        controller_out (int): The size of the hidden state emitted by the controller.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_glimpses: int=8, glimpse_h: int=8, glimpse_w: int=8, controller_out: int = 128):\n",
        "        super().__init__()\n",
        "        self.arc = ARC(\n",
        "            num_glimpses=num_glimpses,\n",
        "            glimpse_h=glimpse_h,\n",
        "            glimpse_w=glimpse_w,\n",
        "            controller_out=controller_out)\n",
        "\n",
        "        # two dense layers, which take the hidden state from the controller of ARC and\n",
        "        # classify the images as belonging to the same class or not.\n",
        "        self.dense1 = nn.Linear(controller_out, 64)\n",
        "        self.dense2 = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, image_pairs: Variable) -> Variable:\n",
        "        arc_out = self.arc(image_pairs)\n",
        "\n",
        "        d1 = F.elu(self.dense1(arc_out))\n",
        "        decision = torch.sigmoid(self.dense2(d1))\n",
        "\n",
        "        return decision\n",
        "\n",
        "    def save_to_file(self, file_path: str) -> None:\n",
        "        torch.save(self.state_dict(), file_path)"
      ],
      "metadata": {
        "id": "8ntlC8Dt5-tM"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "Doc5d6NK5hi8",
        "outputId": "bb674496-a125-40f7-c36f-68c30f3332f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Will start training 6_8_128_cpu with parameters:\n",
            "Namespace(batchSize=128, cuda=False, glimpseSize=8, imageSize=32, load=None, lr=0.0002, name='6_8_128_cpu', numGlimpses=6, numStates=128)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-4f1b874f0b12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-30-4f1b874f0b12>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-4f1b874f0b12>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# load the dataset in memory.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimageSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;31m# ready to train ...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-4f6dbf628d43>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, batch_size, image_size)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mBatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOmniglot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Omniglot_ARC'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'omniglot.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mOmniglot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0ma_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma_start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-4f6dbf628d43>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, batch_size, image_size)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mtranslation_px\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mboth\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my\u001b[0m \u001b[0mdirections\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \"\"\"\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mchars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# resize the images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    415\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Omniglot_ARC/omniglot.npy'"
          ]
        }
      ],
      "source": [
        "#Train\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--batchSize', type=int, default=128, help='input batch size')\n",
        "parser.add_argument('--imageSize', type=int, default=32, help='the height / width of the input image to ARC')\n",
        "parser.add_argument('--glimpseSize', type=int, default=8, help='the height / width of glimpse seen by ARC')\n",
        "parser.add_argument('--numStates', type=int, default=128, help='number of hidden states in ARC controller')\n",
        "parser.add_argument('--numGlimpses', type=int, default=6, help='the number glimpses of each image in pair seen by ARC')\n",
        "parser.add_argument('--lr', type=float, default=0.0002, help='learning rate, default=0.0002')\n",
        "parser.add_argument('--cuda', action='store_true', help='enables cuda')\n",
        "parser.add_argument('--name', default=None, help='Custom name for this configuration. Needed for saving'\n",
        "                                                 ' model checkpoints in a separate folder.')\n",
        "parser.add_argument('--load', default=None, help='the model to load from. Start fresh if not specified.')\n",
        "\n",
        "\n",
        "def get_pct_accuracy(pred: Variable, target) -> int:\n",
        "    hard_pred = (pred > 0.5).int()\n",
        "    correct = (hard_pred == target).sum().data[0]\n",
        "    accuracy = float(correct) / target.size()[0]\n",
        "    accuracy = int(accuracy * 100)\n",
        "    return accuracy\n",
        "\n",
        "def train():\n",
        "    args = parser.parse_args(args=[]) #modified from opt = parser.parse_args()\n",
        "    # opt was renamed to args\n",
        "\n",
        "    if args.cuda:\n",
        "        batcher.use_cuda = True\n",
        "        models.use_cuda = True\n",
        "\n",
        "    if args.name is None:\n",
        "        # if no name is given, we generate a name from the parameters.\n",
        "        # only those parameters are taken, which if changed break torch.load compatibility.\n",
        "        args.name = \"{}_{}_{}_{}\".format(args.numGlimpses, args.glimpseSize, args.numStates,\n",
        "                                        \"cuda\" if args.cuda else \"cpu\")\n",
        "\n",
        "    print(\"Will start training {} with parameters:\\n{}\\n\\n\".format(args.name, args))\n",
        "\n",
        "    # make directory for storing models.\n",
        "    models_path = os.path.join(\"saved_models\", args.name)\n",
        "    os.makedirs(models_path, exist_ok=True)\n",
        "\n",
        "    # initialise the model\n",
        "    discriminator = ArcBinaryClassifier(num_glimpses=args.numGlimpses,\n",
        "                                        glimpse_h=args.glimpseSize,\n",
        "                                        glimpse_w=args.glimpseSize,\n",
        "                                        controller_out=args.numStates)\n",
        "\n",
        "    if args.cuda:\n",
        "        discriminator.cuda()\n",
        "\n",
        "    # load from a previous checkpoint, if specified.\n",
        "    if args.load is not None:\n",
        "        discriminator.load_state_dict(torch.load(os.path.join(models_path, args.load)))\n",
        "\n",
        "    # set up the optimizer.\n",
        "    bce = torch.nn.BCELoss()\n",
        "    if args.cuda:\n",
        "        bce = bce.cuda()\n",
        "\n",
        "    optimizer = torch.optim.Adam(params=discriminator.parameters(), lr=args.lr)\n",
        "\n",
        "    # load the dataset in memory.\n",
        "    loader = Batcher(batch_size=args.batchSize, image_size=args.imageSize)\n",
        "\n",
        "    # ready to train ...\n",
        "    best_validation_loss = None\n",
        "    saving_threshold = 1.02\n",
        "    last_saved = datetime.utcnow()\n",
        "    save_every = timedelta(minutes=10)\n",
        "\n",
        "    i = -1\n",
        "    while True:\n",
        "        i += 1\n",
        "\n",
        "        X, Y = loader.fetch_batch(\"train\")\n",
        "        pred = discriminator(X)\n",
        "        loss = bce(pred, Y.float())\n",
        "\n",
        "        if i % 10 == 0:\n",
        "\n",
        "            # validate your model\n",
        "            X_val, Y_val = loader.fetch_batch(\"val\")\n",
        "            pred_val = discriminator(X_val)\n",
        "            loss_val = bce(pred_val, Y_val.float())\n",
        "\n",
        "            training_loss = loss.data[0]\n",
        "            validation_loss = loss_val.data[0]\n",
        "\n",
        "            print(\"Iteration: {} \\t Train: Acc={}%, Loss={} \\t\\t Validation: Acc={}%, Loss={}\".format(\n",
        "                i, get_pct_accuracy(pred, Y), training_loss, get_pct_accuracy(pred_val, Y_val), validation_loss\n",
        "            ))\n",
        "\n",
        "            if best_validation_loss is None:\n",
        "                best_validation_loss = validation_loss\n",
        "\n",
        "            if best_validation_loss > (saving_threshold * validation_loss):\n",
        "                print(\"Significantly improved validation loss from {} --> {}. Saving...\".format(\n",
        "                    best_validation_loss, validation_loss\n",
        "                ))\n",
        "                discriminator.save_to_file(os.path.join(models_path, str(validation_loss)))\n",
        "                best_validation_loss = validation_loss\n",
        "                last_saved = datetime.utcnow()\n",
        "\n",
        "            if last_saved + save_every < datetime.utcnow():\n",
        "                print(\"It's been too long since we last saved the model. Saving...\")\n",
        "                discriminator.save_to_file(os.path.join(models_path, str(validation_loss)))\n",
        "                last_saved = datetime.utcnow()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "def main() -> None:\n",
        "    train()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Image Augmenter\n",
        "\n",
        "\"\"\"\n",
        "taken and modified from: https://github.com/pranv/ARC/blob/master/image_augmenter.py which was in turn\n",
        "taken and modified from: https://github.com/aleju/ImageAugmenter\n",
        "\"\"\"\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Wrapper functions and classes around scikit-images AffineTransformation.\n",
        "Simplifies augmentation of images in machine learning.\n",
        "\n",
        "Example usage:\n",
        "        img_width = 32 # width of the images\n",
        "        img_height = 32 # height of the images\n",
        "        images = ... # e.g. load via scipy.misc.imload(filename)\n",
        "\n",
        "        # For each image: randomly flip it horizontally (50% chance),\n",
        "        # randomly rotate it between -20 and +20 degrees, randomly translate\n",
        "        # it on the x-axis between -5 and +5 pixel.\n",
        "        ia = ImageAugmenter(img_width, img_height, hlip=True, rotation_deg=20,\n",
        "                            translation_x_px=5)\n",
        "        augmented_images = ia.augment_batch(images)\n",
        "\"\"\"\n",
        "\n",
        "# The MIT License (MIT)\n",
        "#\n",
        "# Copyright (c) 2015 aleju\n",
        "#\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "# of this software and associated documentation files (the \"Software\"), to deal\n",
        "# in the Software without restriction, including without limitation the rights\n",
        "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "# copies of the Software, and to permit persons to whom the Software is\n",
        "# furnished to do so, subject to the following conditions:\n",
        "#\n",
        "# The above copyright notice and this permission notice shall be included in all\n",
        "# copies or substantial portions of the Software.\n",
        "#\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "# SOFTWARE.\n",
        "\n",
        "# --------------------------------\n",
        "# horizontal and vertical flipping/mirroring\n",
        "# --------------------------------\n",
        "        # This should be done before applying the affine matrices, as otherwise\n",
        "        # contents of image might already be rotated/translated out of the image.\n",
        "        # It is done with numpy instead of the affine matrices, because\n",
        "        # scikit-image doesn't offer a nice interface to add mirroring/flipping\n",
        "        # to affine transformations. The numpy operations are O(1), so they\n",
        "        # shouldn't have a noticeable effect on runtimes. They also won't suffer\n",
        "        # from interpolation problems."
      ],
      "metadata": {
        "id": "i4-Viypt8XYR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}