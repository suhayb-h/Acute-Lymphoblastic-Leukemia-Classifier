{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suhayb-h/Acute-Lymphoblastic-Leukemia-Classifier/blob/main/4_ARC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attentive Recurrent Comparator Progress\n",
        "Early papers on Attentive Recurrent Comparators (ARCs) yielded promising “superhuman” performance results based on classifying the Omniglot dataset. ARCs are rarely used in machine learning applications, seemingly due to higher computational demands and insignificant performance improvements. Despite this shortcoming, this project attempts to retrofit an ARC to differentiate between cancer cells and non-cancer cell. The PyTorch code in this notebook was modified from a data scientist by the name of Sanyam Agarwal. Agarwal translated the code to be used in PyTorch. The code he translated was originally created to be used within the Theano language in the paper \"Attentive Recurrent Comparators\" written by Shyam, Gupta and Dukkipati. \n",
        "\n",
        "PyTorch makes the retrofitting for the C-NMC dataset possible and feasible. This library was created as a modern machine learning language that could utilize GPU CUDA cores, which could offset computational demand. To date, there have been no published papers detailing the use of an ARC in a binary classification problem.\n",
        "\n",
        "So far, the model was reconfigured able to work on Google Co-Lab and successfully ran on the original OMNIGLOT database (a database of letters from different languages). The model is divided into 5 components:\n",
        "\n",
        "1. Downloading the dataset to a numpy array\n",
        "\n",
        "2. Augmenting images\n",
        "\n",
        "3. Construction of the model\n",
        "\n",
        "4. Batching the data\n",
        "\n",
        "5.\tTraining the model\n",
        "\n",
        "<hr color=red>\n"
      ],
      "metadata": {
        "id": "YLP-mS2mQFPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Relevent Libraries\n",
        "\n",
        "# Download - Many of these libraries could potentially be removed\n",
        "import os\n",
        "import urllib.request\n",
        "import numpy as np\n",
        "import zipfile\n",
        "from imageio import imread #changed from scipy.ndimage -> imageio\n",
        "import matplotlib.image as img\n",
        "import glob as glob\n",
        "from numpy import asarray\n",
        "from numpy import save\n",
        "\n",
        "#Batcher\n",
        "from numpy.random import choice\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from PIL import Image\n",
        "\n",
        "#Model\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "#Train\n",
        "import argparse\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "#vizualize\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl"
      ],
      "metadata": {
        "id": "pziLE6sG5yxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr color=red>\n",
        "\n",
        "##Downloading the dataset to a numpy array\n",
        "\n",
        "Since the C-NMC dataset is locally stored, this portion of the code was replaced. The original code appended all images into a single array arranged in order to be passed through the batcher code. In recognition of this finding, a single array was created with all the images appended non-randomly with all normal cells appended first and cancer cells appended afterwards:\n",
        "\n",
        "<hr color=red>"
      ],
      "metadata": {
        "id": "4qNdU-0_0GmA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ = []\n",
        "\n",
        "for i in glob.glob(\n",
        "    '/content/drive/Othercomputers/My MacBook Air/'\n",
        "    'C-NMC_Leukemia/training_data/hem/*.bmp'):\n",
        "    im=img.imread(i)\n",
        "    train_.append(im)\n",
        "\n",
        "for i in glob.glob(\n",
        "    '/content/drive/Othercomputers/My MacBook Air/'\n",
        "    'C-NMC_Leukemia/training_data/all/*.bmp'):\n",
        "    im=img.imread(i)\n",
        "    train_.append(im)\n",
        "\n",
        "train_array = np.array(train_)\n",
        "\n",
        "# save to npy file to save time in further training trials\n",
        "save('/content/drive/Othercomputers/My MacBook Air/'\n",
        "'C-NMC_Leukemia/training_data/train.npy', train_array)"
      ],
      "metadata": {
        "id": "pQwI1LtoW2jj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr color=red>\n",
        "\n",
        "##Augmenting images\n",
        "This portion of the code was eliminated when assessing the C-NMC dataset. Since the dataset is significantly larger than the OMNIGLOT dataset, the need to create new images through an augmentor code should be unneccesary.\n",
        "\n",
        "##Model Construction\n",
        "The ARC is still a relatively experimental model, and is not widely adopted. As such, current progress in model construction has required intricate mathematical coding and subsequent careful translation between languages. It was in the best interest of this project to leave this portion of this code completely untouched.\n",
        "\n",
        "<hr color=red>"
      ],
      "metadata": {
        "id": "LNOzjhoR0CYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Model\n",
        "use_cuda = False\n",
        "\n",
        "class GlimpseWindow:\n",
        "    \"\"\"\n",
        "    Generates glimpses from images using Cauchy kernels.\n",
        "    Args:\n",
        "        glimpse_h (int): The height of the glimpses to be generated.\n",
        "        glimpse_w (int): The width of the glimpses to be generated.\n",
        "    \"\"\"\n",
        "    def __init__(self, glimpse_h: int, glimpse_w: int):\n",
        "        self.glimpse_h = glimpse_h\n",
        "        self.glimpse_w = glimpse_w\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_filterbanks(delta_caps: Variable, \n",
        "                         center_caps: Variable, \n",
        "                         image_size: int, \n",
        "                         glimpse_size: int) -> Variable:\n",
        "        \"\"\"\n",
        "        Generates Cauchy Filter Banks along a dimension.\n",
        "        Args:\n",
        "            delta_caps (B,):  A batch of deltas [-1, 1]\n",
        "            center_caps (B,): A batch of [-1, 1] reals that dictate location of \n",
        "                              center of cauchy kernel glimpse\n",
        "            image_size (int): size of images along that dimension\n",
        "            glimpse_size (int): size of glimpses to be generated along that \n",
        "                                dimension\n",
        "        Returns:\n",
        "            (B, image_size, glimpse_size): A batch of filter banks\n",
        "        \"\"\"\n",
        "        # convert dimension sizes to float. lots of math ahead.\n",
        "        image_size = float(image_size)\n",
        "        glimpse_size = float(glimpse_size)\n",
        "\n",
        "        # scale centers and deltas to map to the actual size of given image.\n",
        "        centers = (image_size - 1) * (center_caps + 1) / 2.0  # (B)\n",
        "        deltas = \\\n",
        "        (float(image_size) / glimpse_size) * (1.0 - torch.abs(delta_caps))\n",
        "\n",
        "        # calculate gamma for cauchy kernel\n",
        "        gammas = torch.exp(1.0 - 2 * torch.abs(delta_caps))  # (B)\n",
        "\n",
        "        # coordinate of pixels on the glimpse\n",
        "        # glimpse_size\n",
        "        glimpse_pixels = \\\n",
        "        Variable(torch.arange(0, glimpse_size) - (glimpse_size - 1.0) / 2.0)\n",
        "        if use_cuda:\n",
        "            glimpse_pixels = glimpse_pixels.cuda()\n",
        "\n",
        "        # space out with delta\n",
        "        # (B, glimpse_size)\n",
        "        glimpse_pixels = deltas[:, None] * glimpse_pixels[None, :]\n",
        "        # center around the centers\n",
        "        glimpse_pixels = centers[:, None] + glimpse_pixels  # (B, glimpse_size)\n",
        "\n",
        "        # coordinates of pixels on the image\n",
        "        image_pixels = Variable(torch.arange(0, image_size))  # (image_size)\n",
        "        if use_cuda:\n",
        "            image_pixels = image_pixels.cuda()\n",
        "\n",
        "        # (B, glimpse_size, image_size)\n",
        "        fx = image_pixels - glimpse_pixels[:, :, None]\n",
        "        fx = fx / gammas[:, None, None]\n",
        "        fx = fx ** 2.0\n",
        "        fx = 1.0 + fx\n",
        "        fx = math.pi * gammas[:, None, None] * fx\n",
        "        fx = 1.0 / fx\n",
        "        fx = fx / (torch.sum(fx, dim=2) + 1e-4)[:, :, None]  \n",
        "        # ^^^ add small constant in the denominator to avoid division by 0.\n",
        "\n",
        "        return fx.transpose(1, 2)\n",
        "\n",
        "    def get_attention_mask(\n",
        "        self, glimpse_params: Variable, mask_h: int, mask_w: int) -> Variable:\n",
        "        \"\"\"\n",
        "        Visualization: generate a mask of which pixels got most \"attention\".\n",
        "        Args:\n",
        "            glimpse_params (B, hx):  A batch of glimpse parameters.\n",
        "            mask_h (int): Height of image for which the mask is being generated.\n",
        "            mask_w (int): Width of image for which the mask is being generated.\n",
        "        Returns:\n",
        "            (B, mask_h, mask_w):  Batch of masks with attended \n",
        "                                  pixels weighted more.\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size, _ = glimpse_params.size()\n",
        "\n",
        "        # (B, image_h, glimpse_h)\n",
        "        F_h = self._get_filterbanks(\n",
        "            delta_caps=glimpse_params[:, 2], center_caps=glimpse_params[:, 0],\n",
        "            image_size=mask_h, glimpse_size=self.glimpse_h)\n",
        "\n",
        "        # (B, image_w, glimpse_w)\n",
        "        F_w = self._get_filterbanks(\n",
        "            delta_caps=glimpse_params[:, 2], center_caps=glimpse_params[:, 1],\n",
        "            image_size=mask_w, glimpse_size=self.glimpse_w)\n",
        "\n",
        "        # (B, glimpse_h, glimpse_w)\n",
        "        glimpse_proxy = Variable(\n",
        "            torch.ones(batch_size, self.glimpse_h, self.glimpse_w))\n",
        "\n",
        "        # find the attention mask that lead to the glimpse.\n",
        "        mask = glimpse_proxy\n",
        "        mask = torch.bmm(F_h, mask)\n",
        "        mask = torch.bmm(mask, F_w.transpose(1, 2))\n",
        "\n",
        "        # scale to between 0 and 1.0\n",
        "        mask = mask - mask.min()\n",
        "        mask = mask / mask.max()\n",
        "        mask = mask.float()\n",
        "\n",
        "        return mask\n",
        "\n",
        "    def get_glimpse(\n",
        "        self, images: Variable, glimpse_params: Variable) -> Variable:\n",
        "        \"\"\"\n",
        "        Generate glimpses given images and glimpse parameters. This is the main \n",
        "        method of this class. The glimpse parameters are \n",
        "        (h_center, w_center, delta). (h_center, w_center) represents the \n",
        "        relative position of the center of the glimpse on the image. \n",
        "        delta determines the zoom factor of the glimpse.\n",
        "        Args:\n",
        "            images (B, h, w):  A batch of images\n",
        "            glimpse_params (B, 3):  A batch of glimpse parameters \n",
        "                                    (h_center, w_center, delta)\n",
        "        Returns:\n",
        "            (B, glimpse_h, glimpse_w): A batch of glimpses.\n",
        "        \"\"\"\n",
        "        batch_size, image_h, image_w = images.size()\n",
        "\n",
        "        # (B, image_h, glimpse_h)\n",
        "        F_h = self._get_filterbanks(delta_caps=glimpse_params[:, 2], \n",
        "                                    center_caps=glimpse_params[:, 0],\n",
        "                                    image_size=image_h, \n",
        "                                    glimpse_size=self.glimpse_h)\n",
        "\n",
        "        # (B, image_w, glimpse_w)\n",
        "        F_w = self._get_filterbanks(delta_caps=glimpse_params[:, 2], \n",
        "                                    center_caps=glimpse_params[:, 1],\n",
        "                                    image_size=image_w, \n",
        "                                    glimpse_size=self.glimpse_w)\n",
        "\n",
        "        # F_h.T * images * F_w\n",
        "        glimpses = images\n",
        "        glimpses = torch.bmm(F_h.transpose(1, 2), glimpses)\n",
        "        glimpses = torch.bmm(glimpses, F_w)\n",
        "\n",
        "        return glimpses  # (B, glimpse_h, glimpse_w)\n",
        "\n",
        "class ARC(nn.Module):\n",
        "    \"\"\"\n",
        "    This class implements the Attentive Recurrent Comparators in two main parts.\n",
        "    1.) controller: The RNN module that takes input glimpses from a pair of \n",
        "                    images and emits a hidden state.\n",
        "    2.) glimpser: A Linear layer that takes the hidden state emitted by the \n",
        "                  controller and generates the glimpse parameters. These glimpse \n",
        "                  parameters are (h_center, w_center, delta). \n",
        "                  (h_center, w_center) represents the relative position of the \n",
        "                  center of the glimpse on the image. delta determines the zoom \n",
        "                  factor of the glimpse.\n",
        "    Args:\n",
        "        num_glimpses (int): How many glimpses must the ARC \"see\" before emitting \n",
        "                            the final hidden state.\n",
        "        glimpse_h (int): The height of the glimpse in pixels.\n",
        "        glimpse_w (int): The width of the glimpse in pixels.\n",
        "        controller_out (int): The size of the hidden state emitted by the \n",
        "                              controller.\n",
        "    \"\"\"\n",
        "    def __init__(self, \n",
        "                 num_glimpses: int=8, \n",
        "                 glimpse_h: int=8, \n",
        "                 glimpse_w: int=8, \n",
        "                 controller_out: int=128) -> None:\n",
        "        super().__init__()\n",
        "        self.num_glimpses = num_glimpses\n",
        "        self.glimpse_h = glimpse_h\n",
        "        self.glimpse_w = glimpse_w\n",
        "        self.controller_out = controller_out\n",
        "\n",
        "        # main modules of ARC\n",
        "        self.controller = nn.LSTMCell(input_size=(glimpse_h * glimpse_w), \n",
        "                                      hidden_size=self.controller_out)\n",
        "        self.glimpser = nn.Linear(in_features=self.controller_out, \n",
        "                                  out_features=3)\n",
        "\n",
        "        # Generate glimpses from images using the glimpse parameters.\n",
        "        self.glimpse_window = GlimpseWindow(glimpse_h=self.glimpse_h, \n",
        "                                            glimpse_w=self.glimpse_w)\n",
        "\n",
        "    def forward(self, image_pairs: Variable) -> Variable:\n",
        "        \"\"\"\n",
        "        Calls the internal _forward() method and returns hidden states for all \n",
        "        time steps.\n",
        "        Args:\n",
        "            image_pairs (B, 2, h, w):  A batch of pairs of images\n",
        "        Returns:\n",
        "            (B, controller_out):  A batch of final hidden states after each pair \n",
        "                                  of image has been shown for num_glimpses\n",
        "            glimpses.\n",
        "        \"\"\"\n",
        "        # return only the last hidden state\n",
        "        all_hidden = self._forward(image_pairs)\n",
        "        # ^^^ (2*num_glimpses, B, controller_out)\n",
        "        last_hidden = all_hidden[-1, :, :]  # (B, controller_out)\n",
        "\n",
        "        return last_hidden\n",
        "\n",
        "    def _forward(self, image_pairs: Variable) -> Variable:\n",
        "        \"\"\"\n",
        "        The main forward method of ARC. But it returns hidden state from all \n",
        "        time steps (all glimpses) as opposed to just the last one. See the \n",
        "        exposed forward() method.\n",
        "        Args:\n",
        "            image_pairs: (B, 2, h, w) A batch of pairs of images\n",
        "        Returns:\n",
        "            (2*num_glimpses, B, controller_out) \n",
        "            Hidden states from ALL time steps.\n",
        "        \"\"\"\n",
        "        # convert to images to float.\n",
        "        image_pairs = image_pairs.float()\n",
        "\n",
        "        # calculate the batch size\n",
        "        batch_size = image_pairs.size()[0]\n",
        "\n",
        "        # an array for collecting hidden states from each time step.\n",
        "        all_hidden = []\n",
        "\n",
        "        # initial hidden state of the LSTM.\n",
        "        Hx = Variable(torch.zeros(batch_size, self.controller_out))  \n",
        "        # (B, controller_out)\n",
        "        Cx = Variable(torch.zeros(batch_size, self.controller_out))  \n",
        "        # (B, controller_out)\n",
        "\n",
        "        if use_cuda:\n",
        "            Hx, Cx = Hx.cuda(), Cx.cuda()\n",
        "\n",
        "        # take `num_glimpses` glimpses for both images, alternatingly.\n",
        "        for turn in range(2*self.num_glimpses):\n",
        "            # select image to show, \n",
        "            # alternate between the first and second image in pair\n",
        "            images_to_observe = image_pairs[:,  turn % 2]  # (B, h, w)\n",
        "\n",
        "            # choose a portion from image to glimpse using attention\n",
        "            glimpse_params = torch.tanh(self.glimpser(Hx))  \n",
        "            # ^^^ (B, 3)  a batch of glimpse params (x, y, delta)\n",
        "            glimpses = self.glimpse_window.get_glimpse(\n",
        "                images_to_observe, glimpse_params)  \n",
        "            # ^^^ (B, glimpse_h, glimpse_w)\n",
        "            flattened_glimpses = glimpses.view(batch_size, -1)  \n",
        "            # ^^^ (B, glimpse_h * glimpse_w), one time-step\n",
        "\n",
        "            # feed the glimpses and the previous hidden state to the LSTM.\n",
        "            Hx, Cx = self.controller(flattened_glimpses, (Hx, Cx))  \n",
        "            # (B, controller_out), (B, controller_out)\n",
        "\n",
        "            # append this hidden state to all states\n",
        "            all_hidden.append(Hx)\n",
        "\n",
        "        all_hidden = torch.stack(all_hidden)  \n",
        "        # (2*num_glimpses, B, controller_out)\n",
        "\n",
        "        # return a batch of all hidden states.\n",
        "        return all_hidden\n",
        "\n",
        "class ArcBinaryClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    A binary classifier that uses ARC.\n",
        "    Given a pair of images, feeds them to the ARC and uses the final hidden \n",
        "    state of ARC to classify the images as belonging to the same class or not.\n",
        "    Args:\n",
        "        num_glimpses (int): How many glimpses must the ARC \"see\" before emitting \n",
        "                            the final hidden state.\n",
        "        glimpse_h (int): The height of the glimpse in pixels.\n",
        "        glimpse_w (int): The width of the glimpse in pixels.\n",
        "        controller_out (int): The size of the hidden state emitted by the \n",
        "                              controller.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_glimpses: int=8, \n",
        "                 glimpse_h: int=8, \n",
        "                 glimpse_w: int=8, \n",
        "                 controller_out: int = 128):\n",
        "        super().__init__()\n",
        "        self.arc = ARC(\n",
        "            num_glimpses=num_glimpses,\n",
        "            glimpse_h=glimpse_h,\n",
        "            glimpse_w=glimpse_w,\n",
        "            controller_out=controller_out)\n",
        "\n",
        "        # Two dense layers. Takes hidden state from controller of ARC and\n",
        "        # classifies images as belonging to the same class or not.\n",
        "        self.dense1 = nn.Linear(controller_out, 64)\n",
        "        self.dense2 = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, image_pairs: Variable) -> Variable:\n",
        "        arc_out = self.arc(image_pairs)\n",
        "\n",
        "        d1 = F.elu(self.dense1(arc_out))\n",
        "        decision = torch.sigmoid(self.dense2(d1))\n",
        "\n",
        "        return decision\n",
        "\n",
        "    def save_to_file(self, file_path: str) -> None:\n",
        "        torch.save(self.state_dict(), file_path)"
      ],
      "metadata": {
        "id": "ESAiij2DD-ra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr color=red>\n",
        "\n",
        "##Batching the data\n",
        "Since the original code was meant to classify 50 different letter images, this portion of the code was significantly modified. Failure to produce meaningful C-NMC training seems to be related to the index splitting portion of the 'batcher' code. The next step might be to replace the batcher code with a binary classification related array batcher.\n",
        "\n",
        "<hr color=red>"
      ],
      "metadata": {
        "id": "s1PCjImd0vwi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Batcher: Original Source -> https://github.com/pranv/ARC\n",
        "use_cuda = False\n",
        "\n",
        "class Omniglot(object):\n",
        "    def __init__(self, path=os.path.join(\n",
        "        '/content/drive/Othercomputers/My MacBook Air/'\n",
        "        'C-NMC_Leukemia/training_data/', \n",
        "        'train.npy'), batch_size=128, image_size=224):\n",
        "        \"\"\"\n",
        "        batch_size: the output is (2 * batch size, 1, image_size, image_size)\n",
        "                    X[i] & X[i + batch_size] are the pair\n",
        "        image_size: size of the image\n",
        "        data_split: in number of alphabets, e.g. [30, 10] means out of 50 \n",
        "                    Omniglot characters, 30 is for training, 10 for validation \n",
        "                    and the remaining(10) for testing\n",
        "\n",
        "        within_alphabet:  for verfication task, when 2 characters are sampled to \n",
        "                          form a pair, this flag specifies if should they be \n",
        "                          from the same alphabet/language\n",
        "        ---------------------\n",
        "        Data Augmentation Parameters:\n",
        "            flip: here flipping both the images in a pair\n",
        "            scale: x would scale image by + or - x%\n",
        "            rotation_deg\n",
        "            shear_deg\n",
        "            translation_px: in both x and y directions\n",
        "        \"\"\"\n",
        "        chars = np.load(path)\n",
        "\n",
        "        # resize the images\n",
        "        resized_chars = np.zeros((10661, 20, image_size, image_size), \n",
        "                                 dtype='uint8')\n",
        "        for i in range(10661):\n",
        "            for j in range(20):\n",
        "                resized_chars[i, j] = np.resize(\n",
        "                    chars[i, j], (image_size, image_size)) \n",
        "                # ^^^ np added for compatability\n",
        "        chars = resized_chars\n",
        "\n",
        "        self.mean_pixel = chars.mean() / 255.0  \n",
        "        # used later for mean subtraction\n",
        "\n",
        "        # starting index of each alphabet in a list of chars\n",
        "        a_start = [0, 3389]\n",
        "\n",
        "        # size of each alphabet (num of chars)\n",
        "        a_size = [3389, 7272]\n",
        "\n",
        "        # each alphabet/language has different number of characters.\n",
        "        # in order to uniformly sample all characters, weighs the probability\n",
        "        # of sampling a alphabet by its size. p is probability\n",
        "        def size2p(size):\n",
        "            s = np.array(size).astype('float64')\n",
        "            return s / s.sum()\n",
        "\n",
        "        self.size2p = size2p\n",
        "        self.data = chars\n",
        "        self.a_start = a_start\n",
        "        self.a_size = a_size\n",
        "        self.image_size = image_size\n",
        "        self.batch_size = batch_size\n",
        "        flip = True\n",
        "        scale = 0.2\n",
        "        rotation_deg = 20\n",
        "        shear_deg = 10\n",
        "        translation_px = 5\n",
        "        #self.augmentor = ImageAugmenter(image_size, image_size,\n",
        "        #                                hflip=flip, vflip=flip,\n",
        "        #                                scale_to_percent=1.0 + scale, \n",
        "        #                                rotation_deg=rotation_deg, \n",
        "        #                                shear_deg=shear_deg,\n",
        "        #                                translation_x_px=translation_px, \n",
        "        #                                translation_y_px=translation_px)\n",
        "\n",
        "    def fetch_batch(self, part):\n",
        "        \"\"\"\n",
        "            This outputs batch_size number of pairs\n",
        "            Thus the actual number of images outputted is 2 * batch_size\n",
        "            Say A & B form the half of a pair\n",
        "            The Batch is divided into 4 parts:\n",
        "                Dissimilar A \t\tDissimilar B\n",
        "                Similar A \t\t\tSimilar B\n",
        "\n",
        "            Corresponding images in Similar A and Similar B form similar pair\n",
        "            similarly, Dissimilar A and Dissimilar B form the dissimilar pair\n",
        "\n",
        "            When flattened, the batch has 4 parts with indices:\n",
        "                Dissimilar A \t\t0 - batch_size / 2\n",
        "                Similar A    \t\tbatch_size / 2  - batch_size\n",
        "                Dissimilar B \t\tbatch_size  - 3 * batch_size / 2\n",
        "                Similar B \t\t\t3 * batch_size / 2 - batch_size\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "class Batcher(Omniglot):\n",
        "    def __init__(self, path=os.path.join(\n",
        "        '/content/drive/Othercomputers/My MacBook Air/'\n",
        "        'C-NMC_Leukemia/training_data', 'train.npy'), \n",
        "        batch_size=128, \n",
        "        image_size=32):\n",
        "        Omniglot.__init__(self, path, batch_size, image_size)\n",
        "\n",
        "        a_start = self.a_start\n",
        "        a_size = self.a_size\n",
        "\n",
        "        # slicing indices for splitting a_start & a_size\n",
        "        i = 1\n",
        "        j = 10662\n",
        "        starts = {}\n",
        "        starts['train'], starts['val'] = a_start[:i], a_start[i:j]\n",
        "        #starts['train'], starts['val'], starts['test'] = \\\n",
        "        #a_start[:i], a_start[i:j], a_start[j:]\n",
        "        sizes = {}\n",
        "        sizes['train'], sizes['val'] = a_size[:i], a_start[i:j]\n",
        "        #sizes['train'], sizes['val'], sizes['test'] = \\\n",
        "        #a_size[:i], a_size[i:j], a_size[j:]\n",
        "        size2p = self.size2p\n",
        "        p = {}\n",
        "        p['train'], p['val'] = size2p(sizes['train']), size2p(sizes['val'])\n",
        "        #p['train'], p['val'], p['test'] = \\\n",
        "        #size2p(sizes['train']), size2p(sizes['val']), size2p(sizes['test'])        \n",
        "        self.starts = starts\n",
        "        self.sizes = sizes\n",
        "        self.p = p\n",
        "\n",
        "    def fetch_batch(self, part, batch_size: int = None):\n",
        "\n",
        "        if batch_size is None:\n",
        "            batch_size = self.batch_size\n",
        "\n",
        "        X, Y = self._fetch_batch(part, batch_size)\n",
        "        X = Variable(torch.from_numpy(X)).view(2*batch_size, \n",
        "                                               self.image_size, \n",
        "                                               self.image_size)\n",
        "        X1 = X[:batch_size]  # (B, h, w)\n",
        "        X2 = X[batch_size:]  # (B, h, w)\n",
        "        X = torch.stack([X1, X2], dim=1)  # (B, 2, h, w)\n",
        "        Y = Variable(torch.from_numpy(Y))\n",
        "\n",
        "        if use_cuda:\n",
        "            X, Y = X.cuda(), Y.cuda()\n",
        "\n",
        "        return X, Y\n",
        "\n",
        "    def _fetch_batch(self, part, batch_size: int = None):\n",
        "        if batch_size is None:\n",
        "            batch_size = self.batch_size\n",
        "\n",
        "        data = self.data\n",
        "        starts = self.starts[part]\n",
        "        sizes = self.sizes[part]\n",
        "        p = self.p[part]\n",
        "        image_size = self.image_size\n",
        "        num_alphbts = len(starts)\n",
        "        X = np.zeros((2 * batch_size, image_size, image_size), dtype='uint8')\n",
        "        #for i in range(batch_size // 2):\n",
        "            # choose similar chars\n",
        "#            same_idx = choice(range(starts[0], starts[-1] + sizes[-1])) \n",
        "\n",
        "            # choose dissimilar chars within alphabet\n",
        "#            alphbt_idx = choice(num_alphbts, p=p)\n",
        "# #           char_offset = choice(sizes[alphbt_idx], 2, replace=False)\n",
        "#  #          diff_idx = starts[alphbt_idx] + char_offset\n",
        "#   #         X[i], X[i + batch_size] = data[diff_idx, choice(20, 2)]\n",
        "#    #        X[i + batch_size // 2], X[i + 3 * batch_size // 2] = \\\n",
        "#     #       data[same_idx, choice(20, 2, replace=False)]\n",
        "\n",
        "        y = np.zeros((batch_size, 1), dtype='int32')\n",
        "        y[:batch_size // 2] = 0\n",
        "        y[batch_size // 2:] = 1\n",
        "\n",
        "        if part == 'train':\n",
        "            #X = self.augmentor.augment_batch(X)\n",
        "        #else:\n",
        "        #Above two lines removed for rest of code to work without ImageAugmenter\n",
        "            X = X / 255.0\n",
        "\n",
        "        X = X - self.mean_pixel\n",
        "        X = X[:, np.newaxis]\n",
        "        X = X.astype(\"float32\")\n",
        "\n",
        "        return X, y\n"
      ],
      "metadata": {
        "id": "y4Rtd_2wEMzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr color=red>\n",
        "\n",
        "##Training the model\n",
        "The training code was initially created with no stop set in place, which requires the user to manually stop training. This should be relatively easy to implement. Furthermore,  GPU cuda cores were successfully utilized in the training portion of the code.\n",
        "\n",
        "<hr color=red>"
      ],
      "metadata": {
        "id": "RQBrUSDQ0zF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Train\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('-f') #neccessary null argument for colab compatibility\n",
        "parser.add_argument('--batchSize', type=int, default=128, \n",
        "                    help='input batch size')\n",
        "parser.add_argument('--imageSize', type=int, default=32, \n",
        "                    help='the height / width of the input image to ARC')\n",
        "parser.add_argument('--glimpseSize', type=int, default=8, \n",
        "                    help='the height / width of glimpse seen by ARC')\n",
        "parser.add_argument('--numStates', type=int, default=128, \n",
        "                    help='number of hidden states in ARC controller')\n",
        "parser.add_argument('--numGlimpses', type=int, default=6, \n",
        "                    help='number glimpses of each image in pair seen by ARC')\n",
        "parser.add_argument('--lr', type=float, default=0.0002, \n",
        "                    help='learning rate, default=0.0002')\n",
        "parser.add_argument('--cuda', action='store_true', \n",
        "                    help='enables cuda')\n",
        "parser.add_argument('--name', default=None, \n",
        "                    help='Custom name for this configuration. Needed for saving'\n",
        "                    ' model checkpoints in a separate folder.')\n",
        "parser.add_argument('--load', default=None, \n",
        "                    help='model to load from. Start fresh if not specified.')\n",
        "\n",
        "def get_pct_accuracy(pred: Variable, target) -> int:\n",
        "    hard_pred = (pred > 0.5).int()\n",
        "    correct = (hard_pred == target).sum().data#[0]\n",
        "    accuracy = float(correct) / target.size()[0]\n",
        "    accuracy = int(accuracy * 100)\n",
        "    return accuracy\n",
        "\n",
        "def train():\n",
        "    opt = parser.parse_args()\n",
        "\n",
        "    if opt.cuda:\n",
        "        batcher.use_cuda = True\n",
        "        models.use_cuda = True\n",
        "\n",
        "    if opt.name is None:\n",
        "        # if no name is given, we generate a name from the parameters.\n",
        "        # only those parameters are taken, which if changed break \n",
        "        # torch.load compatibility.\n",
        "        opt.name = \"{}_{}_{}_{}\".format(opt.numGlimpses, \n",
        "                                        opt.glimpseSize, \n",
        "                                        opt.numStates,\n",
        "                                        \"cuda\" if opt.cuda else \"cpu\")\n",
        "        \n",
        "    # make directory for storing models.\n",
        "    models_path = os.path.join(\"saved_models\", opt.name)\n",
        "    os.makedirs(models_path, exist_ok=True)\n",
        "\n",
        "    # initialise the model\n",
        "    discriminator = ArcBinaryClassifier(num_glimpses=opt.numGlimpses,\n",
        "                                        glimpse_h=opt.glimpseSize,\n",
        "                                        glimpse_w=opt.glimpseSize,\n",
        "                                        controller_out=opt.numStates)\n",
        "\n",
        "    if opt.cuda:\n",
        "        discriminator.cuda()\n",
        "\n",
        "    # load from a previous checkpoint, if specified.\n",
        "    if opt.load is not None:\n",
        "        discriminator.load_state_dict(\n",
        "            torch.load(os.path.join(models_path, opt.load)))\n",
        "\n",
        "    # set up the optimizer.\n",
        "    bce = torch.nn.BCELoss()\n",
        "    if opt.cuda:\n",
        "        bce = bce.cuda()\n",
        "\n",
        "    optimizer = torch.optim.SGD(params=discriminator.parameters(), lr=opt.lr) \n",
        "    # ^^^ Switched from Adam to SGD\n",
        "\n",
        "    # load the dataset in memory.\n",
        "    loader = Batcher(batch_size=opt.batchSize, image_size=opt.imageSize)\n",
        "\n",
        "    # ready to train ...\n",
        "    best_validation_loss = None\n",
        "    saving_threshold = 1.02\n",
        "    last_saved = datetime.utcnow()\n",
        "    save_every = timedelta(minutes=10)\n",
        "\n",
        "    i = -1\n",
        "    while True:\n",
        "        i += 1\n",
        "        X, Y = loader.fetch_batch(\"train\")\n",
        "        pred = discriminator(X)\n",
        "        loss = bce(pred, Y.float())\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            # validate your model\n",
        "            X_val, Y_val = loader.fetch_batch(\"val\")\n",
        "            pred_val = discriminator(X_val)\n",
        "            loss_val = bce(pred_val, Y_val.float())\n",
        "\n",
        "            training_loss = loss.data#[0]\n",
        "            validation_loss = loss_val.data#[0]\n",
        "\n",
        "            print(\n",
        "                \"Iter: {} \\t Train: Acc={}%, \"\n",
        "                \"Loss={} \\t\\t Val: Acc={}%, Loss={}\".format(\n",
        "                i, get_pct_accuracy(pred, Y), \n",
        "                training_loss, \n",
        "                get_pct_accuracy(pred_val, Y_val), \n",
        "                validation_loss\n",
        "            ))\n",
        "\n",
        "            if best_validation_loss is None:\n",
        "                best_validation_loss = validation_loss\n",
        "\n",
        "            if best_validation_loss > (saving_threshold * validation_loss):\n",
        "                print(\"Improved val loss from {} --> {}. Saving...\".format(\n",
        "                    best_validation_loss, validation_loss\n",
        "                ))\n",
        "                discriminator.save_to_file(\n",
        "                    os.path.join(models_path, str(validation_loss)))\n",
        "                best_validation_loss = validation_loss\n",
        "                last_saved = datetime.utcnow()\n",
        "\n",
        "            if last_saved + save_every < datetime.utcnow():\n",
        "                print(\"Too long since last saved model. Saving...\")\n",
        "                discriminator.save_to_file(\n",
        "                    os.path.join(models_path, str(validation_loss)))\n",
        "                last_saved = datetime.utcnow()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "def main() -> None:\n",
        "    train()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KPNmyUcoDodI",
        "outputId": "ce4fff86-e63a-4203-e8d3-149310b0cd80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 0 \t Train: Acc=50%, Loss=0.6938114762306213 \t\t Validation: Acc=50%, Loss=0.6938114762306213\n",
            "Iteration: 10 \t Train: Acc=50%, Loss=0.6938105225563049 \t\t Validation: Acc=50%, Loss=0.6938105225563049\n",
            "Iteration: 20 \t Train: Acc=50%, Loss=0.693809449672699 \t\t Validation: Acc=50%, Loss=0.693809449672699\n",
            "Iteration: 30 \t Train: Acc=50%, Loss=0.6938083171844482 \t\t Validation: Acc=50%, Loss=0.6938083171844482\n",
            "Iteration: 40 \t Train: Acc=50%, Loss=0.6938072443008423 \t\t Validation: Acc=50%, Loss=0.6938072443008423\n",
            "Iteration: 50 \t Train: Acc=50%, Loss=0.6938061714172363 \t\t Validation: Acc=50%, Loss=0.6938061714172363\n",
            "Iteration: 60 \t Train: Acc=50%, Loss=0.6938051581382751 \t\t Validation: Acc=50%, Loss=0.6938051581382751\n",
            "Iteration: 70 \t Train: Acc=50%, Loss=0.6938040256500244 \t\t Validation: Acc=50%, Loss=0.6938040256500244\n",
            "Iteration: 80 \t Train: Acc=50%, Loss=0.6938028931617737 \t\t Validation: Acc=50%, Loss=0.6938028931617737\n",
            "Iteration: 90 \t Train: Acc=50%, Loss=0.6938018798828125 \t\t Validation: Acc=50%, Loss=0.6938018798828125\n",
            "Iteration: 100 \t Train: Acc=50%, Loss=0.6938008666038513 \t\t Validation: Acc=50%, Loss=0.6938008666038513\n",
            "Iteration: 110 \t Train: Acc=50%, Loss=0.6937999129295349 \t\t Validation: Acc=50%, Loss=0.6937999129295349\n",
            "Iteration: 120 \t Train: Acc=50%, Loss=0.6937986612319946 \t\t Validation: Acc=50%, Loss=0.6937986612319946\n",
            "Iteration: 130 \t Train: Acc=50%, Loss=0.6937976479530334 \t\t Validation: Acc=50%, Loss=0.6937976479530334\n",
            "Iteration: 140 \t Train: Acc=50%, Loss=0.6937966346740723 \t\t Validation: Acc=50%, Loss=0.6937966346740723\n",
            "Iteration: 150 \t Train: Acc=50%, Loss=0.6937956213951111 \t\t Validation: Acc=50%, Loss=0.6937956213951111\n",
            "Iteration: 160 \t Train: Acc=50%, Loss=0.6937944889068604 \t\t Validation: Acc=50%, Loss=0.6937944889068604\n",
            "Iteration: 170 \t Train: Acc=50%, Loss=0.6937934160232544 \t\t Validation: Acc=50%, Loss=0.6937934160232544\n",
            "Iteration: 180 \t Train: Acc=50%, Loss=0.6937924027442932 \t\t Validation: Acc=50%, Loss=0.6937924027442932\n",
            "Iteration: 190 \t Train: Acc=50%, Loss=0.6937913298606873 \t\t Validation: Acc=50%, Loss=0.6937913298606873\n",
            "Iteration: 200 \t Train: Acc=50%, Loss=0.6937903761863708 \t\t Validation: Acc=50%, Loss=0.6937903761863708\n",
            "Iteration: 210 \t Train: Acc=50%, Loss=0.6937893629074097 \t\t Validation: Acc=50%, Loss=0.6937893629074097\n",
            "Iteration: 220 \t Train: Acc=50%, Loss=0.6937881708145142 \t\t Validation: Acc=50%, Loss=0.6937881708145142\n",
            "Iteration: 230 \t Train: Acc=50%, Loss=0.693787157535553 \t\t Validation: Acc=50%, Loss=0.693787157535553\n",
            "Iteration: 240 \t Train: Acc=50%, Loss=0.6937861442565918 \t\t Validation: Acc=50%, Loss=0.6937861442565918\n",
            "Iteration: 250 \t Train: Acc=50%, Loss=0.6937851309776306 \t\t Validation: Acc=50%, Loss=0.6937851309776306\n",
            "Iteration: 260 \t Train: Acc=50%, Loss=0.6937841773033142 \t\t Validation: Acc=50%, Loss=0.6937841773033142\n",
            "Iteration: 270 \t Train: Acc=50%, Loss=0.6937830448150635 \t\t Validation: Acc=50%, Loss=0.6937830448150635\n",
            "Iteration: 280 \t Train: Acc=50%, Loss=0.6937819719314575 \t\t Validation: Acc=50%, Loss=0.6937819719314575\n",
            "Iteration: 290 \t Train: Acc=50%, Loss=0.6937810182571411 \t\t Validation: Acc=50%, Loss=0.6937810182571411\n",
            "Iteration: 300 \t Train: Acc=50%, Loss=0.6937800049781799 \t\t Validation: Acc=50%, Loss=0.6937800049781799\n",
            "Iteration: 310 \t Train: Acc=50%, Loss=0.6937790513038635 \t\t Validation: Acc=50%, Loss=0.6937790513038635\n",
            "Iteration: 320 \t Train: Acc=50%, Loss=0.6937780380249023 \t\t Validation: Acc=50%, Loss=0.6937780380249023\n",
            "Iteration: 330 \t Train: Acc=50%, Loss=0.6937769651412964 \t\t Validation: Acc=50%, Loss=0.6937769651412964\n",
            "Iteration: 340 \t Train: Acc=50%, Loss=0.6937758922576904 \t\t Validation: Acc=50%, Loss=0.6937758922576904\n",
            "Iteration: 350 \t Train: Acc=50%, Loss=0.6937748193740845 \t\t Validation: Acc=50%, Loss=0.6937748193740845\n",
            "Iteration: 360 \t Train: Acc=50%, Loss=0.6937738060951233 \t\t Validation: Acc=50%, Loss=0.6937738060951233\n",
            "Iteration: 370 \t Train: Acc=50%, Loss=0.6937728524208069 \t\t Validation: Acc=50%, Loss=0.6937728524208069\n",
            "Iteration: 380 \t Train: Acc=50%, Loss=0.6937718987464905 \t\t Validation: Acc=50%, Loss=0.6937718987464905\n",
            "Iteration: 390 \t Train: Acc=50%, Loss=0.6937708854675293 \t\t Validation: Acc=50%, Loss=0.6937708854675293\n",
            "Iteration: 400 \t Train: Acc=50%, Loss=0.6937698721885681 \t\t Validation: Acc=50%, Loss=0.6937698721885681\n",
            "Iteration: 410 \t Train: Acc=50%, Loss=0.6937687397003174 \t\t Validation: Acc=50%, Loss=0.6937687397003174\n",
            "Iteration: 420 \t Train: Acc=50%, Loss=0.693767786026001 \t\t Validation: Acc=50%, Loss=0.693767786026001\n",
            "Iteration: 430 \t Train: Acc=50%, Loss=0.6937668323516846 \t\t Validation: Acc=50%, Loss=0.6937668323516846\n",
            "Iteration: 440 \t Train: Acc=50%, Loss=0.6937656998634338 \t\t Validation: Acc=50%, Loss=0.6937656998634338\n",
            "Iteration: 450 \t Train: Acc=50%, Loss=0.6937647461891174 \t\t Validation: Acc=50%, Loss=0.6937647461891174\n",
            "Iteration: 460 \t Train: Acc=50%, Loss=0.693763792514801 \t\t Validation: Acc=50%, Loss=0.693763792514801\n",
            "Iteration: 470 \t Train: Acc=50%, Loss=0.6937628388404846 \t\t Validation: Acc=50%, Loss=0.6937628388404846\n",
            "Iteration: 480 \t Train: Acc=50%, Loss=0.6937618255615234 \t\t Validation: Acc=50%, Loss=0.6937618255615234\n",
            "Iteration: 490 \t Train: Acc=50%, Loss=0.6937608122825623 \t\t Validation: Acc=50%, Loss=0.6937608122825623\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-2cd7e74316d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-2cd7e74316d8>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-2cd7e74316d8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr color=red>\n",
        "\n",
        "Progress currently stands at a model successfully configured to utilize the C-NMC dataset, but the accuracy score for both training and validation sets are stuck at 50% exactly. This could be due to indexing incompatibility issues within the batcher code of the original model. Loss values do seem to be variating every epoch, which is a promising sign. "
      ],
      "metadata": {
        "id": "Nh3swjBt02XO"
      }
    }
  ]
}